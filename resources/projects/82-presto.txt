341: Adam J. Shook, debug, IfStmt, LOG.debug("No locality groups to set");
364: Adam J. Shook, debug, MethodDeclaration, LOG.debug("Setting locality groups: {}", localityGroups);
662: Adam J. Shook, debug, TryStmt, LOG.debug("Getting tablet splits for table %s", tableName);
695: Adam J. Shook, debug, TryStmt, LOG.debug("Fetching tablet locations: %s", fetchTabletLocations);
709: Adam J. Shook, debug, TryStmt, LOG.debug("Number of splits for table %s is %d with %d ranges", tableName, tabletSplits.size(), splitRanges.size());
736: Adam J. Shook, debug, IfStmt, LOG.debug("Using session scan auths for user %s: %s", sessionScanUser, scanAuths);
748: Adam J. Shook, debug, IfStmt, LOG.debug("scan_auths table property set, using: %s", scanAuths);
752: Adam J. Shook, debug, MethodDeclaration, LOG.debug("scan_auths table property not set, using connector auths: %s", this.auths);
147: Adam J. Shook, error, CatchClause, LOG.error(e, "Error shutting down connector");
155: Adam J. Shook, info, TryStmt, LOG.info("Connection to instance %s at %s established, user %s", instance, zooKeepers, username);
73: Adam J. Shook, warn, CatchClause, LOG.warn("NamespaceExistsException suppressed when creating " + schema);
103: Adam J. Shook, debug, TryStmt, LOG.debug("Set locality groups for %s to %s", tableName, groups);
100: Adam J. Shook, debug, ConstructorDeclaration, LOG.debug("Created new cache size %d expiry %s", size, expireDuration);
133: Grzegorz Kokosi≈Ñski, debug, LambdaExpr, LOG.debug("Cardinality for column %s is %s", key.getName(), cardinality);
159: Adam J. Shook, info, IfStmt, LOG.info("Cardinality %s, is below threshold. Returning early while other tasks finish", smallestCardinality);
192: Adam J. Shook, debug, MethodDeclaration, LOG.debug("Getting cardinality for %s:%s", family, qualifier);
200: Adam J. Shook, debug, MethodDeclaration, LOG.debug("Column values contain %s exact ranges of %s", exactRanges.size(), colValues.size());
340: Adam J. Shook, debug, MethodDeclaration, LOG.debug("Loading a non-exact range from Accumulo: %s", key);
371: Adam J. Shook, debug, MethodDeclaration, LOG.debug("Loading %s exact ranges from Accumulo", size);
382: Adam J. Shook, debug, MethodDeclaration, LOG.debug("rangeToKey size is %s", rangeToKey.size());
142: Adam J. Shook, debug, IfStmt, LOG.debug("Secondary index is disabled");
146: Adam J. Shook, debug, MethodDeclaration, LOG.debug("Secondary index is enabled");
153: Adam J. Shook, debug, IfStmt, LOG.debug("Query contains no constraints on indexed columns, skipping secondary index");
175: Adam J. Shook, debug, IfStmt, LOG.debug("Use of index metrics is enabled");
159: Adam J. Shook, debug, IfStmt, LOG.debug("Use of index metrics is disabled");
169: Adam J. Shook, debug, IfStmt, LOG.debug("Query would return no results, returning empty list of splits");
166: Adam J. Shook, debug, IfStmt, LOG.debug("Number of splits for %s.%s is %d with %d ranges", schema, table, tabletSplits.size(), indexRanges.size());
191: Adam J. Shook, warn, IfStmt, LOG.warn("Query containts constraint on non-indexed column %s. Is it worth indexing?", columnConstraint.getName());
243: Adam J. Shook, debug, IfStmt, LOG.debug("Use of index would scan %s of %s rows, ratio %s. Threshold %2f, Using for index table? %s", numEntries, numRows, ratio, threshold, ratio < threshold);
250: Adam J. Shook, debug, IfStmt, LOG.debug("%d indexed columns, intersecting ranges", constraintRanges.size());
252: Adam J. Shook, debug, IfStmt, LOG.debug("Intersection results in %d ranges from secondary index", indexRanges.size());
264: Adam J. Shook, debug, IfStmt, LOG.debug("Query would return no results, returning empty list of splits");
272: Adam J. Shook, debug, MethodDeclaration, LOG.debug("Use of index would scan %d of %d rows, ratio %s. Threshold %2f, Using for table? %b", numEntries, numRows, ratio, threshold, ratio < threshold, table);
278: Adam J. Shook, debug, IfStmt, LOG.debug("Number of splits for %s.%s is %d with %d ranges", schema, table, tabletSplits.size(), indexRanges.size());
291: Adam J. Shook, debug, MethodDeclaration, LOG.debug("Smallest cardinality is %d, num rows is %d, ratio is %2f with threshold of %f", smallestCardinality, numRows, ratio, threshold);
313: Adam J. Shook, debug, MethodDeclaration, LOG.debug("Number of rows in table is %d", numRows);
344: Adam J. Shook, debug, LambdaExpr, LOG.debug("Retrieved %d ranges for index column %s", columnRanges.size(), constraintEntry.getKey().getName());
122: Adam J. Shook, debug, IfStmt, LOG.debug("Using session scanner auths for user %s: %s", sessionScanUser, scanAuths);
134: Adam J. Shook, debug, IfStmt, LOG.debug("scan_auths table property not set, using user auths: %s", auths);
129: Adam J. Shook, debug, IfStmt, LOG.debug("scan_auths table property set: %s", auths);
41: Adam J. Shook, warn, IfStmt, LogLog.warn(format("Cannot obtain JUL %s. Verify that this appender is used while an appropriate LogManager is active.", loggingEvent.getLoggerName()));
50: Adam J. Shook, Info, MethodDeclaration, LocationInfo location = loggingEvent.getLocationInformation();
103: Adam J. Shook, info, MethodDeclaration, LOG.info("Loading data from %s.%s...", sourceCatalog, sourceSchema);
108: Adam J. Shook, info, MethodDeclaration, LOG.info("Loading from %s.%s complete in %s", sourceCatalog, sourceSchema, nanosSince(startTime).toString(SECONDS));
147: Adam J. Shook, info, MethodDeclaration, LOG.info("Running import for %s", target, sql);
148: Adam J. Shook, info, MethodDeclaration, LOG.info("%s", sql);
151: Adam J. Shook, info, MethodDeclaration, LOG.info("Imported %s rows for %s in %s", rows, target, nanosSince(start));
175: Adam J. Shook, info, TryStmt, LOG.info("Connection to MAC instance %s at %s established, user %s password %s", accumulo.getInstanceName(), accumulo.getZooKeepers(), MAC_USER, MAC_PASSWORD);
193: Adam J. Shook, info, MethodDeclaration, LOG.info("MAC is enabled, starting MiniAccumuloCluster at %s", macDir);
204: Adam J. Shook, info, TryStmt, LOG.info("Shutting down MAC");
213: Adam J. Shook, info, TryStmt, LOG.info("Cleaning up MAC directory");
93: Christopher Berner, error, CatchClause, log.error(e, "Error shutting down connector");
382: yuananf, warn, CatchClause, log.warn(e, "Failed to cleanup temporary table: %s", temporaryTable);
461: David Phillips, debug, TryStmt, log.debug("Execute: %s", query);
126: David Phillips, error, CatchClause, log.error(e, "Error shutting down connector");
91: Yang Yang, debug, TryStmt, log.debug("Executing: %s", statement.toString());
126: Dain Sundstrom, info, MethodDeclaration, LOGGER.info("=== Pre-running all benchmarks for JVM warmup ===");
131: Dain Sundstrom, info, MethodDeclaration, LOGGER.info("=== Actually running benchmarks for metrics ===");
67: Dain Sundstrom, debug, MethodDeclaration, initializeLogging(driverOptions.debug);
70: Dain Sundstrom, info, MethodDeclaration, log.info("======== SERVER STARTED ========");
71: Dain Sundstrom, info, MethodDeclaration, log.info("\n====\n%s\n====", queryRunner.getCoordinator().getBaseUrl());
102: David Phillips, error, CatchClause, log.error(e, "Error shutting down connector");
62: David Phillips, debug, MethodDeclaration, log.debug("%s.%s #partitions: %d", cassandraTableHandle.getSchemaName(), cassandraTableHandle.getTableName(), allPartitions.size());
64: Dain Sundstrom, debug, MethodDeclaration, log.debug("Creating record set: %s", cql);
482: Andrii Rosa, warn, IfStmt, log.warn(e.getCustomMessage(10, true, true));
483: Andrii Rosa, warn, IfStmt, log.warn("Reconnecting in %dms", delay);
80: mweindel, debug, TryStmt, log.debug("%s already bound to %s", name, objectInstance);
58: Andrii Rosa, warn, IfStmt, log.warn("Cluster has been closed internally");
71: Andrii Rosa, info, MethodDeclaration, log.info("Starting cassandra...");
155: Andrii Rosa, info, MethodDeclaration, log.info("Cassandra version: %s", version);
167: Andrii Rosa, info, IfStmt, log.info("Size estimates for the table %s.%s have been refreshed successfully: %s", keyspace, table, sizeEstimates);
170: Andrii Rosa, info, WhileStmt, log.info("Size estimates haven't been refreshed as expected. Retrying ...");
125: David Phillips, debug, CatchClause, log.debug(e, "error printing status");
84: David Phillips, error, CatchClause, log.error(e, "Error shutting down connector");
45: Masha Basmanova, info, MethodDeclaration, log.info("======== SERVER STARTED ========");
46: Masha Basmanova, info, MethodDeclaration, log.info("\n====\n%s\n====", queryRunner.getCoordinator().getBaseUrl());
198: David Phillips, error, CatchClause, log.error(e, "Error shutting down connector");
31: James Sun, info, MethodDeclaration, log.info("File created: query: %s, schema: %s, table: %s, partition: '%s', format: %s, size: %s, path: %s", writeCompletedEvent.getQueryId(), writeCompletedEvent.getSchemaName(), writeCompletedEvent.getTableName(), writeCompletedEvent.getPartitionName(), writeCompletedEvent.getStorageFormat(), writeCompletedEvent.getBytes(), writeCompletedEvent.getPath());
133: Haozhun Jin, warn, IfStmt, log.warn("Hive writes are disabled. " + "To write data to Hive, your JVM timezone must match the Hive storage timezone. " + "Add -Duser.timezone=%s to your JVM arguments", timeZone.getID());
237: Lukasz Osipiuk, warn, CatchClause, log.warn("exception '%s' while rollback on %s", e, writer);
277: Haozhun Jin, warn, IfStmt, log.warn("Split buffering for %s.%s in query %s exceeded memory limit (%s). %s splits are buffered.", databaseName, tableName, queryId, succinctBytes(maxOutstandingSplitsBytes), getBufferedInternalSplitCount());
83: Dain Sundstrom, debug, TryStmt, log.debug("%s already bound to %s", name, objectInstance);
149: David Phillips, debug, CatchClause, log.debug("Failed on executing %s with attempt %d, will retry. Exception: %s", callableName, attempt, e.getMessage());
1423: Haozhun Jin, warn, MethodDeclaration, log.warn(format, args);
1431: Haozhun Jin, warn, MethodDeclaration, log.warn(t, format, args);
393: Rentao Wu, warn, CatchClause, log.warn(e, "Failed to delete path: " + path.toString());
727: Nezih Yigitbasi, debug, TryStmt, log.debug("Using AWS credential provider %s for URI %s", providerClass, uri);
1005: Zhenxiao Luo, debug, ConstructorDeclaration, log.debug("OutputStream for key '%s' using file: %s", key, tempFile);
1023: Zhenxiao Luo, warn, IfStmt, log.warn("Could not delete temporary file: %s", tempFile);
1034: Nezih Yigitbasi, debug, TryStmt, log.debug("Starting upload for host: %s, key: %s, file: %s, size: %s", host, key, tempFile, tempFile.length());
1064: Nezih Yigitbasi, debug, TryStmt, log.debug("Completed upload for host: %s, key: %s", host, key);
1089: Nezih Yigitbasi, debug, IfStmt, log.debug("Upload progress event (%s/%s): %s", host, key, eventType);
1095: Nezih Yigitbasi, debug, IfStmt, log.debug("Upload percentage (%s/%s): %.0f%%", host, key, transferred);
50: Haozhun Jin, warn, CatchClause, log.warn(t, "ResumableTask completed exceptionally");
159: Haozhun Jin, info, MethodDeclaration, log.info("Loading data from %s.%s...", sourceCatalog, sourceSchema);
164: Haozhun Jin, info, MethodDeclaration, log.info("Loading from %s.%s complete in %s", sourceCatalog, sourceSchema, nanosSince(startTime).toString(SECONDS));
170: Haozhun Jin, info, MethodDeclaration, log.info("Running import for %s", table.getObjectName());
193: Haozhun Jin, info, MethodDeclaration, log.info("Imported %s rows for %s in %s", rows, table.getObjectName(), nanosSince(start).convertToMostSuccinctTimeUnit());
204: Haozhun Jin, info, MethodDeclaration, log.info("======== SERVER STARTED ========");
205: Haozhun Jin, info, MethodDeclaration, log.info("\n====\n%s\n====", queryRunner.getCoordinator().getBaseUrl());
82: Piotr Nowojski, error, CatchClause, log.error(e, "Error shutting down connector");
99: Piotr Nowojski, error, CatchClause, log.error(exception, "This should never happen, JmxPeriodicSampler will not be scheduled again.");
128: Nezih Yigitbasi, error, CatchClause, log.error(exception, "Error reading jmx records");
82: Piotr Nowojski, debug, TryStmt, log.debug("%s already bound to %s", name, objectInstance);
89: David Phillips, error, CatchClause, log.error(e, "Error shutting down connector");
179: Henning Schmiedehausen, debug, IfStmt, log.debug("Found a total of %d messages with %d bytes (%d messages expected). Last Offset: %d (%d, %d)", totalMessages, totalBytes, split.getEnd() - split.getStart(), cursorOffset, split.getStart(), split.getEnd());
301: Henning Schmiedehausen, debug, IfStmt, log.debug("Fetching %d bytes from offset %d (%d - %d). %d messages read so far", KAFKA_READ_BUFFER_SIZE, cursorOffset, split.getStart(), split.getEnd(), totalMessages);
314: Henning Schmiedehausen, warn, IfStmt, log.warn("Fetch response has error: %d", errorCode);
72: Henning Schmiedehausen, warn, CatchClause, log.warn(e, "While closing consumer %s:", entry.getKey());
85: David Phillips, info, MethodDeclaration, log.info("Creating new Consumer for %s", host);
89: David Phillips, debug, ForeachStmt, log.debug("Adding Partition %s/%s", metadata.topic(), part.partitionId());
93: David Phillips, warn, IfStmt, log.warn("No leader for partition %s/%s found!", metadata.topic(), part.partitionId());
137: Henning Schmiedehausen, warn, IfStmt, log.warn("Offset response has error: %d", errorCode);
69: David Phillips, debug, MethodDeclaration, log.debug("Loading kafka table definitions from %s", tableDescriptionDir.getAbsolutePath());
114: Henning Schmiedehausen, warn, CatchClause, log.warn(e, "Error: ");
76: Henning Schmiedehausen, debug, IfStmt, log.debug("Kafka table %s.%s: %s", schemaName, table.getTableName(), table);
83: Henning Schmiedehausen, debug, TryStmt, log.debug("Loaded Table definitions: %s", tableDefinitions.keySet());
102: Henning Schmiedehausen, debug, IfStmt, log.debug("Created dummy Table definition for %s", tableName);
97: Henning Schmiedehausen, debug, IfStmt, log.debug("Found Table definition for %s: %s", tableName, kafkaTable);
124: Henning Schmiedehausen, debug, IfStmt, log.debug("Considering files: %s", asList(files));
81: Dain Sundstrom, info, TryStmt, log.info("Loading data...");
86: Dain Sundstrom, info, TryStmt, log.info("Loading complete in %s", nanosSince(startTime).toString(SECONDS));
99: Dain Sundstrom, info, MethodDeclaration, log.info("Running import for %s", table.getTableName());
101: Dain Sundstrom, info, MethodDeclaration, log.info("Imported %s in %s", 0, table.getTableName(), nanosSince(start).convertToMostSuccinctTimeUnit());
139: Haozhun Jin, info, MethodDeclaration, log.info("======== SERVER STARTED ========");
140: Haozhun Jin, info, MethodDeclaration, log.info("\n====\n%s\n====", queryRunner.getCoordinator().getBaseUrl());
86: Nileema Shingte, error, CatchClause, log.error(e, "Error shutting down connector");
155: David Phillips, error, CatchClause, log.error(t, "Error shutting down connector: %s", entry.getKey());
304: Dain Sundstrom, error, CatchClause, log.error(t, "Error shutting down connector: %s", connectorId);
212: Dain Sundstrom, debug, CatchClause, log.debug(e, "Error creating explain plan");
362: Dain Sundstrom, error, CatchClause, log.error(e, "Error logging query timeline");
350: Raghav Sethi, info, TryStmt, log.info("TIMELINE: Query %s :: Transaction:[%s] :: elapsed %sms :: planning %sms :: scheduling %sms :: running %sms :: finishing %sms :: begin %s :: end %s", queryInfo.getQueryId(), queryInfo.getSession().getTransactionId().map(TransactionId::toString).orElse(""), max(elapsed, 0), max(planning, 0), max(scheduling, 0), max(running, 0), max(finishing, 0), queryStartTime, queryEndTime);
416: Raghav Sethi, error, CatchClause, log.error(e, "Error processing split completion event for task %s", taskId);
77: Raghav Sethi, info, MethodDeclaration, log.info("-- Loading event listener --");
85: Raghav Sethi, info, MethodDeclaration, log.info("-- Loaded event listener %s --", name);
125: Lukasz Osipiuk, error, CatchClause, log.error(e, "Error requesting system memory revoking");
160: Piotr Findeisen, error, CatchClause, log.error(e, "Error when acting on memory pool reservation");
155: Piotr Findeisen, debug, IfStmt, log.debug("Scheduling check for %s", memoryPool);
179: Piotr Findeisen, error, CatchClause, log.error(e, "Error requesting memory revoking");
267: Lukasz Osipiuk, debug, IfStmt, log.debug("memoryPool=%s: requested revoking %s; remaining %s", memoryPool.getId(), revokedBytes, remainingBytesToRevoke.get());
155: Dain Sundstrom, error, MethodDeclaration, log.error("BUG! %s for %s leaked with %s partitioned splits.  Cleaning up so server can continue to function.", getClass().getName(), taskId, leakedSplits);
215: Nezih Yigitbasi, debug, LambdaExpr, log.debug("Query %s is %s", queryId, newState);
706: Dain Sundstrom, debug, IfStmt, log.debug(throwable, "Failure after query %s finished", queryId);
702: Martin Traverso, debug, IfStmt, log.debug(throwable, "Query %s failed", queryId);
736: Nezih Yigitbasi, Error, CatchClause, log.error("Error cleaning up query: %s", t);
430: David Phillips, warn, CatchClause, log.warn(t, "Error closing split source");
220: Nezih Yigitbasi, error, CatchClause, log.error(e, "Error cancelling abandoned queries");
227: Nezih Yigitbasi, error, CatchClause, log.error(e, "Error enforcing memory limits");
234: Nezih Yigitbasi, error, CatchClause, log.error(e, "Error enforcing query timeout limits");
241: Nezih Yigitbasi, error, CatchClause, log.error(e, "Error enforcing query CPU time limits");
248: Nezih Yigitbasi, error, CatchClause, log.error(e, "Error removing expired queries");
255: Nezih Yigitbasi, error, CatchClause, log.error(e, "Error pruning expired queries");
270: Nileema Shingte, info, ForeachStmt, log.info("Server shutting down. Query %s has been cancelled", queryExecution.getQueryId());
526: Dain Sundstrom, debug, MethodDeclaration, log.debug("Cancel query %s", queryId);
537: Dain Sundstrom, debug, MethodDeclaration, log.debug("Cancel stage %s", stageId);
653: Dain Sundstrom, debug, WhileStmt, log.debug("Remove query %s", queryId);
674: Nezih Yigitbasi, error, CatchClause, log.error(e, "Exception failing abandoned query %s", queryExecution.getQueryId());
669: Nezih Yigitbasi, info, IfStmt, log.info("Failing abandoned query %s", queryExecution.getQueryId());
148: Dain Sundstrom, warn, CatchClause, log.warn(e, "Error running task cleanup callback %s", SqlTask.this.taskId);
390: Dain Sundstrom, debug, MethodDeclaration, log.debug("Aborting task %s output %s", taskId, bufferId);
202: Christopher Berner, warn, IfStmt, log.warn("Switching coordinator affinity from " + coordinatorId + " to " + assignments.getCoordinatorId());
219: Dain Sundstrom, warn, CatchClause, log.warn(e, "Error removing old tasks");
225: Dain Sundstrom, warn, CatchClause, log.warn(e, "Error canceling abandoned tasks");
234: Dain Sundstrom, warn, CatchClause, log.warn(e, "Error updating stats");
414: Nileema Shingte, warn, CatchClause, log.warn(e, "Error while inspecting age of complete task %s", taskId);
437: Dain Sundstrom, warn, CatchClause, log.warn(e, "Error while inspecting age of task %s", sqlTask.getTaskId());
432: Nileema Shingte, info, IfStmt, log.info("Failing abandoned task %s", taskStatus.getTaskId());
101: Dain Sundstrom, debug, ConstructorDeclaration, stageState.addStateChangeListener(state -> log.debug("Stage %s is %s", stageId, state));
180: Dain Sundstrom, debug, IfStmt, log.debug(throwable, "Failure after stage %s finished", stageId);
177: Dain Sundstrom, error, IfStmt, log.error(throwable, "Stage %s failed", stageId);
218: Dain Sundstrom, error, CatchClause, log.error(e, "Error setting future state for %s", name);
225: Dain Sundstrom, error, CatchClause, log.error(e, "Error notifying state change listener for %s", name);
52: Dain Sundstrom, debug, MethodDeclaration, log.debug("Task %s is %s", TaskStateMachine.this.taskId, newState);
118: Raghav Sethi, error, CatchClause, log.error(e, "Error closing split for task %s", taskHandle.getTaskId());
237: Raghav Sethi, debug, MethodDeclaration, log.debug("Task scheduled " + taskId);
285: Raghav Sethi, debug, MethodDeclaration, log.debug("Task finished or failed " + taskHandle.getTaskId());
460: James Sun, warn, ForeachStmt, log.warn(exception, "Split thread %s has been running longer than %s", currentMaxActiveSplit, duration);
526: Raghav Sethi, error, IfStmt, log.error(t, "Error processing %s", split.getInfo());
523: Raghav Sethi, Error, IfStmt, log.error("Error processing %s: %s: %s", split.getInfo(), e.getErrorCode().getName(), e.getMessage());
500: Dain Sundstrom, debug, IfStmt, log.debug("%s is finished", split.getInfo());
153: Christopher Berner, info, MethodDeclaration, log.info("-- Loading resource group configuration manager --");
161: Christopher Berner, info, MethodDeclaration, log.info("-- Loaded resource group configuration manager %s --", name);
206: Christopher Berner, error, CatchClause, log.error(e, "Exception while generation cpu quota for %s", group);
212: Christopher Berner, error, CatchClause, log.error(e, "Exception while processing queued queries for %s", group);
250: Christopher Berner, error, CatchClause, log.error(e, "Error %s resource group %s", export ? "exporting" : "unexporting", group.getId());
214: David Phillips, warn, CatchClause, log.warn(t, "Error closing split source");
85: David Phillips, warn, CatchClause, log.warn(e, "Unable to determine location of %s. Will attempt again in %s", host, NEGATIVE_CACHE_DURATION);
123: Christopher Berner, debug, IfStmt, log.debug("No nodes available to schedule %s. Available nodes %s", split, nodeMap.getNodesByHost().keys());
131: Christopher Berner, debug, IfStmt, log.debug("No nodes available to schedule %s. Available nodes %s", split, nodeMap.getNodesByHost().keys());
143: Martin Traverso, warn, CatchClause, log.warn(e, "Error updating services");
331: Martin Traverso, warn, CatchClause, log.warn(e, "Error pinging service %s (%s)", service.getId(), uri);
386: Martin Traverso, warn, CatchClause, log.warn(e, "Error scheduling request for %s", uri);
156: Raghav Sethi, error, CatchClause, log.error(e, "Error exporting memory pool %s", poolId);
274: Haozhun Jin, info, MethodDeclaration, log.info(nodeDescription.toString());
441: Christopher Berner, error, CatchClause, log.error(e, "Failed to unexport pool %s", pool.getId());
86: Christopher Berner, warn, IfStmt, log.warn("Memory info update request to %s has not returned in %s", memoryInfoUri, sinceUpdate.toString(SECONDS));
111: Christopher Berner, Error, IfStmt, log.warn("Error fetching memory info from %s returned status %d: %s", memoryInfoUri, result.getStatusCode(), result.getStatusMessage());
121: David Phillips, Error, MethodDeclaration, log.warn("Error fetching memory info from %s: %s", memoryInfoUri, t.getMessage());
227: Dain Sundstrom, info, ForeachStmt, log.info("Previously active node is missing: %s (last seen at %s)", missingNode.getNodeIdentifier(), missingNode.getHostAndPort());
76: Nezih Yigitbasi, warn, IfStmt, log.warn("Node state update request to %s has not returned in %s", stateInfoUri, sinceUpdate.toString(SECONDS));
99: Nezih Yigitbasi, Error, IfStmt, log.warn("Error fetching node state from %s returned status %d: %s", stateInfoUri, result.getStatusCode(), result.getStatusMessage());
108: Nezih Yigitbasi, Error, MethodDeclaration, log.warn("Error fetching node state from %s: %s", stateInfoUri, t.getMessage());
86: David Phillips, info, IfStmt, log.info("Skipping disabled catalog %s", catalogName);
90: David Phillips, info, MethodDeclaration, log.info("-- Loading catalog %s --", file);
97: David Phillips, info, MethodDeclaration, log.info("-- Added catalog %s using connector %s --", catalogName, connectorName);
508: Dain Sundstrom, error, IfStmt, log.error("Driver still has memory reserved after freeing all operator memory.");
511: Dain Sundstrom, error, IfStmt, log.error("Driver still has system memory reserved after freeing all operator memory.");
514: Dain Sundstrom, error, IfStmt, log.error("Driver still has revocable memory reserved after freeing all operator memory. Freeing it.");
616: Dain Sundstrom, error, IfStmt, log.error(newException, message, args);
349: James Sun, debug, MethodDeclaration, log.debug(exception, "Acknowledge request failed: %s", uri);
357: David Phillips, debug, IfStmt, log.debug("Unexpected acknowledge response code: %s", response.getStatusCode());
400: Dain Sundstrom, debug, MethodDeclaration, log.debug("Request to %s failed %s", uri, t);
445: Nileema Shingte, error, MethodDeclaration, log.error("Request to delete %s failed %s", location, t);
432: Dain Sundstrom, error, CatchClause, log.error(e, "Lookup source compile failed for types=%s error=%s", types, e);
498: Lukasz Osipiuk, error, CatchClause, log.error(e, "Lookup source compile failed for types=%s error=%s", types, e);
103: Eric Hwang, Error, IfStmt, hll = HyperLogLog.newInstance(standardErrorToBuckets(maxStandardError));
131: Dain Sundstrom, info, MethodDeclaration, log.info("-- Loading system access control --");
139: Dain Sundstrom, info, MethodDeclaration, log.info("-- Loaded system access control %s --", name);
75: Nezih Yigitbasi, info, MethodDeclaration, log.info("Shutdown requested");
101: Nezih Yigitbasi, info, WhileStmt, log.info("Waiting for all tasks to finish");
107: Nezih Yigitbasi, warn, CatchClause, log.warn("Interrupted while waiting for all tasks to finish");
127: Nezih Yigitbasi, warn, CatchClause, log.warn(e, "Timed out waiting for the life cycle to stop");
130: Nezih Yigitbasi, warn, CatchClause, log.warn(e, "Interrupted while waiting for the life cycle to stop");
134: Nezih Yigitbasi, warn, CatchClause, log.warn(e, "Problem stopping the life cycle");
153: Dain Sundstrom, info, MethodDeclaration, log.info("-- Loading plugin %s --", plugin);
158: Dain Sundstrom, info, MethodDeclaration, log.info("-- Finished loading plugin %s --", plugin);
167: David Phillips, warn, IfStmt, log.warn("No service providers of type %s", Plugin.class.getName());
171: David Phillips, info, ForeachStmt, log.info("Installing %s", plugin.getClass().getName());
179: Haozhun Jin, info, ForeachStmt, log.info("Registering block encoding %s", blockEncoding.getName());
184: Christopher Berner, info, ForeachStmt, log.info("Registering type %s", type.getTypeSignature());
189: Christopher Berner, info, ForeachStmt, log.info("Registering parametric type %s", parametricType.getName());
194: David Phillips, info, ForeachStmt, log.info("Registering connector %s", connectorFactory.getName());
199: David Phillips, info, ForeachStmt, log.info("Registering functions from %s", functionClass.getName());
204: Rongrong Zhong, info, ForeachStmt, log.info("Registering session property configuration manager %s", sessionConfigFactory.getName());
209: Christopher Berner, info, ForeachStmt, log.info("Registering resource group configuration manager %s", configurationManagerFactory.getName());
214: Dain Sundstrom, info, ForeachStmt, log.info("Registering system access control %s", accessControlFactory.getName());
219: David Phillips, info, ForeachStmt, log.info("Registering password authenticator %s", authenticatorFactory.getName());
224: Raghav Sethi, info, ForeachStmt, log.info("Registering event listener %s", eventListenerFactory.getName());
260: Dain Sundstrom, debug, MethodDeclaration, log.debug("Classpath for %s:", dir.getName());
263: Dain Sundstrom, debug, ForeachStmt, log.debug("    %s", file);
280: David Phillips, debug, MethodDeclaration, log.debug("Classpath for %s:", name);
287: David Phillips, debug, ForeachStmt, log.debug("    %s", file);
139: Martin Traverso, error, CatchClause, log.error(e);
136: Dain Sundstrom, info, TryStmt, log.info("======== SERVER STARTED ========");
93: Rongrong Zhong, info, MethodDeclaration, log.info("-- Loading session property configuration manager --");
100: Rongrong Zhong, info, MethodDeclaration, log.info("-- Loaded session property configuration manager %s --", configManagerName);
53: Dain Sundstrom, warn, MethodDeclaration, log.warn(throwable, "Request failed for %s", request.getRequestURI());
64: Aleksei Statkevich, warn, CatchClause, log.warn(e, "Error removing old queries");
585: Aleksei Statkevich, warn, IfStmt, log.warn("Query %s in state %s has no failure info", queryInfo.getQueryId(), state);
595: Aleksei Statkevich, warn, IfStmt, log.warn("Failed query %s has no error code", queryInfo.getQueryId());
134: Nileema Shingte, error, IfStmt, log.error("Can not reschedule update because an update is already running");
694: Nileema Shingte, debug, IfStmt, log.debug(cause, "Remote task %s failed with %s", taskStatus.getSelf(), cause);
118: Nileema Shingte, warn, IfStmt, log.warn(reason, "Error " + jobDescription + " %s: %s", taskId, taskUri);
115: Nileema Shingte, Error, IfStmt, log.warn("Error " + jobDescription + " %s: %s: %s", taskId, reason.getMessage(), taskUri);
149: Nileema Shingte, error, IfStmt, log.error(t, format, args);
146: Nileema Shingte, error, IfStmt, log.error(format + ": %s", ObjectArrays.concat(args, t));
166: zhenyuy, debug, CatchClause, LOG.debug(e, "Authentication failed for token %s", token);
162: zhenyuy, debug, TryStmt, LOG.debug("Failed to establish GSS context for token %s", token);
70: David Phillips, info, MethodDeclaration, log.info("-- Loading password authenticator --");
78: David Phillips, info, MethodDeclaration, log.info("-- Loaded password authenticator %s --", name);
136: Lukasz Osipiuk, warn, CatchClause, log.warn(e, "Error cleaning spill files");
131: Lukasz Osipiuk, warn, CatchClause, log.warn("Could not cleanup old spill file: " + spillFile);
127: Lukasz Osipiuk, info, TryStmt, log.info("Deleting old spill file: " + spillFile);
96: Dain Sundstrom, error, CatchClause, log.error(e, "Error compiling comparator for channels %s with order %s", sortChannels, sortChannels);
99: David Phillips, warn, CatchClause, log.warn(t, "Error closing split source");
131: Eric Hwang, error, CatchClause, log.error(t, "Unexpected exception while cleaning up expired transactions");
143: Eric Hwang, info, IfStmt, log.info("Removing expired transaction: %s", entry.getKey());
387: Dain Sundstrom, Info, MethodDeclaration, catalogsByConnectorId.put(catalog.getInformationSchemaId(), catalog);
401: Dain Sundstrom, Info, IfStmt, ConnectorTransactionMetadata informationSchema = createConnectorTransactionMetadata(catalog.getInformationSchemaId(), catalog);
410: Dain Sundstrom, Info, IfStmt, this.catalogMetadata.put(catalog.getInformationSchemaId(), catalogMetadata);
468: Dain Sundstrom, error, LambdaExpr, log.error(throwable, "Read-only connector should not throw exception on commit");
479: Dain Sundstrom, error, LambdaExpr, addExceptionCallback(future, throwable -> log.error(throwable, "Read-only connector should not throw exception on commit"));
517: David Phillips, error, CatchClause, log.error(e, "Connector threw exception on abort");
157: Karol Sobczak, debug, MethodDeclaration, log.debug("Fallback to NFA, pattern: %s, DFA states limit: %d, DFA retries: %d", re2jPattern.pattern(), dfaStatesLimit, dfaRetries);
62: David Phillips, debug, MethodDeclaration, log.debug("Defining class: %s", classDefinition.getName());
93: Dain Sundstrom, error, CatchClause, log.error(e, "Finalizer cleanup failed");
192: Dain Sundstrom, info, MethodDeclaration, log.info("FINISHED %s in %s verified %s expressions", method.getName(), Duration.nanosSince(start), futures.size());
77: Piotr Nowojski, info, MethodDeclaration, log.info("======== SERVER STARTED ========");
78: Piotr Nowojski, info, MethodDeclaration, log.info("\n====\n%s\n====", queryRunner.getCoordinator().getBaseUrl());
83: Dongmin Yu), debug, CatchClause, log.debug(e, "Table(%s) not found", tableName);
487: Min, debug, IfStmt, log.debug("Unable to guess field type from %s : %s", value == null ? "null" : value.getClass().getName(), value);
287: Dain Sundstrom, warn, IfStmt, log.warn("ORC file %s was written by a newer Hive version %s. This file may not be readable by this version of Hive (%s.%s).", orcDataSource, Joiner.on('.').join(version), CURRENT_MAJOR_VERSION, CURRENT_MINOR_VERSION);
115: David Phillips, debug, CatchClause, log.debug("Authentication failed for user [%s]: %s", user, e.getMessage());
119: David Phillips, debug, CatchClause, log.debug(e, "Authentication error for user [%s]", user);
111: David Phillips, debug, TryStmt, log.debug("Authentication successful for user [%s]", user);
162: David Phillips, debug, CatchClause, log.debug("Authentication error for user [%s]: %s", user, e.getMessage());
168: David Phillips, debug, IfStmt, log.debug(message);
123: Grzegorz Kokosi≈Ñski, warn, IfStmt, LOGGER.warn("connection().setSchema() is not supported in JDBC 4");
157: Brian Rickman, warn, IfStmt, LOGGER.warn("shouldSetTimezone() only applies to PrestoJdbcDriver");
182: Brian Rickman, warn, IfStmt, LOGGER.warn("shouldSetLocale() only applies to PrestoJdbcDriver");
270: Brian Rickman, warn, IfStmt, LOGGER.warn("testSqlEscapeFunctions() only applies to TeradataJdbcDriver");
105: Brian Rickman, warn, IfStmt, LOGGER.warn("preparedSelectApi() only applies to TeradataJdbcDriver");
130: Brian Rickman, warn, IfStmt, LOGGER.warn("preparedSelectSql() only applies to TeradataJdbcDriver");
168: Brian Rickman, warn, IfStmt, LOGGER.warn("preparedInsertVarbinaryApi() only applies to TeradataJdbcDriver");
273: Brian Rickman, warn, IfStmt, LOGGER.warn("preparedInsertApi() only applies to TeradataJdbcDriver");
379: Brian Rickman, warn, IfStmt, LOGGER.warn("preparedInsertSql() only applies to TeradataJdbcDriver");
420: Brian Rickman, warn, IfStmt, LOGGER.warn("preparedInsertVarbinarySql() only applies to TeradataJdbcDriver");
91: Brian Rickman, info, CatchClause, LOGGER.info("'%s' is not a number", value, e);
213: David Phillips, error, CatchClause, log.error(e, "Error shutting down connector");
243: David Phillips, warn, CatchClause, log.warn(t, "Failed to unblock maintenance for table ID %s, will retry", tableId);
736: Jiexi Lin, info, MethodDeclaration, log.info("Committing insert into tableId %s (queryId: %s, shards: %s, columns: %s)", handle.getTableId(), session.getQueryId(), shards.size(), columns.size());
799: Nileema Shingte, info, MethodDeclaration, log.info("Finishing delete for tableId %s (removed: %s, rewritten: %s)", tableId, oldShardUuids.size() - newShards.size(), newShards.size());
128: David Phillips, error, IfStmt, log.error("Backup is corrupt after write. Quarantining local file: %s", quarantineBase);
130: David Phillips, warn, IfStmt, log.warn("Quarantine of corrupt backup shard failed: %s", uuid);
137: David Phillips, warn, IfStmt, log.warn("Failed to delete staging file: %s", restored);
45: David Phillips, debug, MethodDeclaration, log.debug("Creating shard backup: %s", uuid);
52: David Phillips, debug, MethodDeclaration, log.debug("Restoring shard backup: %s", uuid);
59: David Phillips, debug, MethodDeclaration, log.debug("Deleting shard backup: %s", uuid);
88: David Phillips, error, CatchClause, log.error(t, "Error clearing online nodes");
243: David Phillips, warn, CatchClause, log.warn(e, "Failed to drop index table %s", shardIndexTable(tableId));
274: Jiexi Lin, warn, CatchClause, log.warn(e, "Failed to alter table on attempt %s, will retry. SQL: %s", attempts, sql);
372: David Phillips, warn, CatchClause, log.warn(e, "Failed to commit shards on attempt %d, will retry.", attempt);
712: David Phillips, info, IfStmt, log.info("Reassigned bucket %s for distribution ID %s from %s to %s", bucket, distributionId, oldNodeId, nodeId);
55: David Phillips, warn, CatchClause, log.warn(e, "Failed to insert created shard on attempt %s, will retry", attempt);
39: David Phillips, warn, CatchClause, log.warn("Failed to connect to database. Will retry again in %s. Exception: %s", delay, e.getMessage());
242: David Phillips, error, CatchClause, log.error(t, "Error cleaning transactions");
273: Raghav Sethi, error, CatchClause, log.error(t, "Error cleaning backup shards");
296: Jianhong Li, error, CatchClause, log.error(t, "Error cleaning local shards");
307: Raghav Sethi, error, CatchClause, log.error(t, "Error cleaning local shards");
385: Jianhong Li, info, MethodDeclaration, log.info("Cleaned %s local shards immediately", deletions.size());
427: David Phillips, info, MethodDeclaration, log.info("Cleaned %s local shards", deletions.size());
458: David Phillips, error, IfStmt, log.error(e, "Error cleaning backup shard: %s", uuid);
92: Nileema Shingte, debug, TryStmt, log.debug("Running query: %s", statement);
167: Raghav Sethi, error, CatchClause, log.error(t, "Error balancing buckets");
175: Raghav Sethi, info, MethodDeclaration, log.info("Bucket balancer started. Computing assignments...");
178: Raghav Sethi, info, MethodDeclaration, log.info("Moving buckets...");
181: Raghav Sethi, info, MethodDeclaration, log.info("Bucket balancing finished. Moved %s buckets.", moves);
213: Raghav Sethi, info, ForeachStmt, log.info("Distribution %s: Current bucket skew: min %s, max %s. Target bucket skew: min %s, max %s", distribution.getId(), currentMin, currentMax, targetMin, targetMax);
271: Raghav Sethi, info, ForeachStmt, log.info("Distribution %s: Moved bucket %s from %s to %s", reassignment.getDistributionId(), reassignment.getBucketNumber(), source, reassignment.getNodeIdentifier());
165: Nezih Yigitbasi, warn, CatchClause, log.warn(e, "Failed to delete file: %s", file.getAbsolutePath());
94: Nileema Shingte, debug, TryStmt, log.debug("Rewrote file %s in %s (input rows: %s, output rows: %s)", input.getName(), nanosSince(start), inputRowCount, inputRowCount - deleteRowCount);
161: David Phillips, error, CatchClause, log.error(t, "Error ejecting shards");
222: David Phillips, warn, IfStmt, log.warn("No backup for shard: %s", shardUuid);
233: David Phillips, info, WhileStmt, log.info("Moving shard %s to node %s (shard: %s, node: %s, average: %s, target: %s)", shardUuid, target, shardSize, nodeSize, averageSize, targetSize);
248: David Phillips, warn, IfStmt, log.warn("Failed to delete shard file: %s", file);
176: Raghav Sethi, error, CatchClause, log.error(t, "Error creating shard recovery tasks");
172: Raghav Sethi, warn, MethodCallExpr, log.warn(t, "Error recovering shard: %s", shard.getShardUuid());
170: Raghav Sethi, warn, ForeachStmt, Futures.addCallback(shardQueue.submit(new MissingShard(shard.getShardUuid(), shard.getCompressedSize(), shard.getXxhash64(), false)), failureCallback(t -> log.warn(t, "Error recovering shard: %s", shard.getShardUuid())));
220: David Phillips, error, IfStmt, log.error("Local file is corrupt. Quarantining local file: %s", quarantine);
222: David Phillips, warn, IfStmt, log.warn("Quarantine of corrupt local file failed: %s", shardUuid);
232: Nileema Shingte, info, MethodDeclaration, log.info("Copying shard %s from backup...", shardUuid);
249: Nileema Shingte, info, MethodDeclaration, log.info("Copied shard %s from backup in %s (%s at %s/s)", shardUuid, duration, size, rate);
276: David Phillips, error, IfStmt, log.error("Local file is corrupt after recovery. Quarantining local file: %s", quarantine);
278: David Phillips, warn, IfStmt, log.warn("Quarantine of corrupt recovered file failed: %s", shardUuid);
84: Nileema Shingte, info, MethodDeclaration, log.info("Compacted shards %s into %s", shardUuids, newShards.stream().map(ShardInfo::getShardUuid).collect(toList()));
164: Nileema Shingte, error, CatchClause, log.error(t, "Error discovering shards to compact");
171: Nileema Shingte, info, MethodDeclaration, log.info("Discovering shards that need compaction...");
182: Nileema Shingte, info, ForeachStmt, log.info("Created %s organization set(s) for table ID %s", organizationSets.size(), tableId);
220: Nileema Shingte, warn, IfStmt, log.warn("Temporal column type of table ID %s set incorrectly to %s", tableId, type);
156: Nileema Shingte, error, CatchClause, log.error(t, "Error running shard organizer");
149: Nileema Shingte, info, TryStmt, log.info("Running shard organizer...");
203: Nileema Shingte, info, MethodDeclaration, log.info("Created %s organization set(s) from %s shards for table ID %s", organizationSets.size(), filteredShards.size(), tableId);
87: Nileema Shingte, warn, IfStmt, log.warn(throwable, "Error running organization job");
82: Nileema Shingte, error, CatchClause, log.error(e, "Task failed");
82: David Phillips, debug, TryStmt, log.debug("%s already bound to %s", name, objectInstance);
105: David Phillips, info, MethodDeclaration, log.info("Loading data from %s.%s...", catalog, schema);
110: David Phillips, info, MethodDeclaration, log.info("Loading from %s.%s complete in %s", catalog, schema, nanosSince(startTime));
121: David Phillips, info, MethodDeclaration, log.info("Running import for %s", target);
124: David Phillips, info, MethodDeclaration, log.info("Imported %s rows for %s in %s", rows, target, nanosSince(start));
151: David Phillips, info, MethodDeclaration, log.info("======== SERVER STARTED ========");
152: David Phillips, info, MethodDeclaration, log.info("\n====\n%s\n====", queryRunner.getCoordinator().getBaseUrl());
77: Henning Schmiedehausen, debug, ConstructorDeclaration, log.debug("Field decoders found: %s", this.fieldDecoders);
63: Alexander Vasilevskiy, warn, CatchClause, log.warn(e, "While destroying JedisPool %s:", entry.getKey());
81: David Phillips, info, MethodDeclaration, log.info("Creating new JedisPool for %s", host);
79: Alexander Vasilevskiy, debug, ConstructorDeclaration, log.debug("Loading redis table definitions from %s", redisConnectorConfig.getTableDescriptionDir().getAbsolutePath());
146: Alexander Vasilevskiy, debug, IfStmt, log.debug("Read a total of %d values with %d bytes.", totalValues, totalBytes);
309: Alexander Vasilevskiy, debug, SwitchEntryStmt, log.debug("Scanning new Redis keys from cursor %s . %d values read so far", cursor, totalValues);
321: Alexander Vasilevskiy, debug, SwitchStmt, log.debug("Redis type of key %s is unsupported", split.getKeyDataFormat());
342: Alexander Vasilevskiy, warn, IfStmt, log.warn("Redis data modified while query was running, string value at key %s deleted", keyString);
349: Alexander Vasilevskiy, warn, IfStmt, log.warn("Redis data modified while query was running, hash value at key %s deleted", keyString);
354: Alexander Vasilevskiy, debug, SwitchStmt, log.debug("Redis type for key %s is unsupported", keyString);
102: Alexander Vasilevskiy, warn, CatchClause, log.warn(e, "Error: ");
65: Alexander Vasilevskiy, debug, IfStmt, log.debug("Redis table %s.%s: %s", schemaName, table.getTableName(), table);
72: Alexander Vasilevskiy, debug, TryStmt, log.debug("Loaded table definitions: %s", tableDefinitions.keySet());
91: Alexander Vasilevskiy, debug, IfStmt, log.debug("Created dummy Table definition for %s", tableName);
86: Alexander Vasilevskiy, debug, IfStmt, log.debug("Found Table definition for %s: %s", tableName, redisTable);
112: Alexander Vasilevskiy, debug, IfStmt, log.debug("Considering files: %s", asList(files));
76: Alexander Vasilevskiy, info, TryStmt, log.info("Loading data...");
81: Alexander Vasilevskiy, info, TryStmt, log.info("Loading complete in %s", nanosSince(startTime).toString(SECONDS));
94: Alexander Vasilevskiy, info, MethodDeclaration, log.info("Running import for %s", table.getTableName());
101: Alexander Vasilevskiy, info, MethodDeclaration, log.info("Imported %s in %s", table.getTableName(), nanosSince(start).convertToMostSuccinctTimeUnit());
244: Elon Azoulay, error, CatchClause, log.error(e, "Error loading configuration from db");
234: Raghav Sethi, info, IfStmt, log.info("Loaded %s selectors and %s resource groups from database", this.selectors.get().size(), this.resourceGroupSpecs.size());
227: Raghav Sethi, info, ForeachStmt, log.info("Resource group spec deleted %s", deleted);
230: Raghav Sethi, info, ForeachStmt, log.info("Resource group spec %s changed to %s", changed, resourceGroupSpecs.get(changed));
75: Rongrong Zhong, warn, IfStmt, log.warn(e, "Failed to fetch exact match resource group selectors");
58: Rongrong Zhong, info, IfStmt, log.info("Successfully fetched exact match selectors after %s", nanosSince(start));
69: Rongrong Zhong, warn, CatchClause, log.warn("Failed to decode resource group from DB: %s", resourceGroupId);
120: Dain Sundstrom, info, TryStmt, log.info("Created TestingDiscoveryServer in %s", nanosSince(start).convertToMostSuccinctTimeUnit());
156: Dain Sundstrom, info, ConstructorDeclaration, log.info("Announced servers in %s", nanosSince(start).convertToMostSuccinctTimeUnit());
162: Dain Sundstrom, info, ConstructorDeclaration, log.info("Added functions in %s", nanosSince(start).convertToMostSuccinctTimeUnit());
194: Dain Sundstrom, info, MethodDeclaration, log.info("Created TestingPrestoServer in %s", nanosSince(start).convertToMostSuccinctTimeUnit());
275: Dain Sundstrom, info, MethodDeclaration, log.info("Installed plugin %s in %s", plugin.getClass().getSimpleName(), nanosSince(start).convertToMostSuccinctTimeUnit());
292: David Phillips, info, MethodDeclaration, log.info("Created catalog %s (%s) in %s", catalogName, connectorId, nanosSince(start));
306: David Phillips, info, MethodDeclaration, log.info("Announced catalog %s (%s) in %s", catalogName, connectorId, nanosSince(start));
58: David Phillips, info, MethodDeclaration, log.info("FINISHED in presto: %s", nanosSince(start));
101: Dain Sundstrom, info, MethodDeclaration, log.info("FINISHED in presto: %s, h2: %s, total: %s", actualTime, nanosSince(expectedStart), nanosSince(start));
256: Dain Sundstrom, info, MethodDeclaration, log.info("Loading data from %s.%s...", sourceCatalog, sourceSchema);
261: Dain Sundstrom, info, MethodDeclaration, log.info("Loading from %s.%s complete in %s", sourceCatalog, sourceSchema, nanosSince(startTime).toString(SECONDS));
273: David Phillips, info, MethodDeclaration, log.info("Running import for %s", table.getObjectName());
276: David Phillips, info, MethodDeclaration, log.info("Imported %s rows for %s in %s", rows, table.getObjectName(), nanosSince(start).convertToMostSuccinctTimeUnit());
135: Aleksei Statkevich, info, IfStmt, log.info("Query %s: %s", statusInfo.getId(), statusInfo.getInfoUri());
33: Dain Sundstrom, info, MethodDeclaration, log.info("======== SERVER STARTED ========");
34: Dain Sundstrom, info, MethodDeclaration, log.info("\n====\n%s\n====", queryRunner.getCoordinator().getBaseUrl());
106: Aleksei Statkevich, error, CatchClause, log.error(ie, "Interrupted while shutting down connector");
109: Aleksei Statkevich, error, CatchClause, log.error(e, "Error shutting down connector");
82: Aleksei Statkevich, debug, TryStmt, log.debug("%s already bound to %s", name, objectInstance);
90: Aleksei Statkevich, info, MethodDeclaration, log.info("======== SERVER STARTED ========");
91: Aleksei Statkevich, info, MethodDeclaration, log.info("\n====\n%s\n====", queryRunner.getCoordinator().getBaseUrl());
52: David Phillips, error, CatchClause, log.error(t);
49: David Phillips, info, TryStmt, log.info("======== SERVER STARTED ========");
75: Rebecca Schlussel, info, MethodDeclaration, log.info("======== SERVER STARTED ========");
76: Rebecca Schlussel, info, MethodDeclaration, log.info("\n====\n%s\n====", queryRunner.getCoordinator().getBaseUrl());
81: Christopher Berner, info, MethodDeclaration, log.info("Total Queries:     %d", totalQueries);
83: Christopher Berner, info, MethodDeclaration, log.info("Whitelisted Queries: %s", Joiner.on(',').join(whitelist));
91: Christopher Berner, debug, IfStmt, log.debug("Query %s is not whitelisted", query.getName());
95: Christopher Berner, debug, IfStmt, log.debug("Query %s is blacklisted", query.getName());
118: Christopher Berner, info, MethodDeclaration, log.info("Allowed Queries:     %d", queriesSubmitted);
119: Christopher Berner, info, MethodDeclaration, log.info("Skipped Queries:     %d", (totalQueries - queriesSubmitted));
120: Christopher Berner, info, MethodDeclaration, log.info("---------------------");
137: Christopher Berner, warn, IfStmt, log.warn("%s", validator.getSkippedMessage());
157: Christopher Berner, info, IfStmt, log.info("Progress: %s valid, %s failed, %s skipped, %.2f%% done", valid, failed, skipped, progress);
162: Christopher Berner, info, MethodDeclaration, log.info("Results: %s / %s (%s skipped)", valid, failed, skipped);
163: Dain Sundstrom, info, MethodDeclaration, log.info("");
172: Dain Sundstrom, info, IfStmt, log.info("");
268: Wenlei Xie, warn, CatchClause, LOG.warn(e, "Failed to rewrite %s for shadowing. Skipping.", pair.getName());
