56: Ryan Pridgeon, warn, IfStmt, log.warn("Removing server {} from {} as DNS resolution failed for {}", url, CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, host);
76: Steven Wu, error, CatchClause, log.error("Failed to close " + name, t);
108: Colin P. Mccabe, debug, IfStmt, log.debug("Disabling exponential reconnect backoff because " + RECONNECT_BACKOFF_MS_CONFIG + " is set, but " + RECONNECT_BACKOFF_MAX_MS_CONFIG + " is not.");
199: Colin P. Mccabe, debug, IfStmt, log.debug("Built full fetch {} for node {} with {}.", nextMetadata, node, partitionsToLogString(next.keySet()));
252: Colin P. Mccabe, debug, MethodDeclaration, log.debug("Built incremental fetch {} for node {}. Added {}, altered {}, removed {} " + "out of {}", nextMetadata, node, partitionsToLogString(added), partitionsToLogString(altered), partitionsToLogString(removed), partitionsToLogString(sessionPartitions.keySet()));
423: Colin P. Mccabe, debug, IfStmt, log.debug("Node {} sent an incremental fetch response for session {}{}", node, response.sessionId(), responseDataToLogString(response));
417: Colin P. Mccabe, debug, IfStmt, log.debug("Node {} sent an incremental fetch response closing session {}{}", node, nextMetadata.sessionId(), responseDataToLogString(response));
412: Colin P. Mccabe, info, IfStmt, log.info("Node {} sent an invalid incremental fetch response with {}", node, problem);
404: Colin P. Mccabe, debug, IfStmt, log.debug("Node {} sent a full fetch response that created a new incremental " + "fetch session {}{}", node, response.sessionId(), responseDataToLogString(response));
398: Colin P. Mccabe, debug, IfStmt, log.debug("Node {} sent a full fetch response{}", node, responseDataToLogString(response));
394: Colin P. Mccabe, info, IfStmt, log.info("Node {} sent an invalid full fetch response with {}", node, problem);
383: Colin P. Mccabe, info, IfStmt, log.info("Node {} was unable to process the fetch request with {}: {}.", node, nextMetadata, response.error());
440: Colin P. Mccabe, Error, MethodDeclaration, log.info("Error sending fetch request {} to node {}: {}.", nextMetadata, node, t.toString());
242: Rajini Sivaram, debug, IfStmt, log.debug("Removing unused topic {} from the metadata list, expiryMs {} now {}", entry.getKey(), expireMs, now);
265: Satish Duggana, info, BlockStmt, log.info("Cluster ID: {}", newClusterId);
270: Jay Kreps, debug, MethodDeclaration, log.debug("Updated cluster metadata version {} to {}", this.version, this.cluster);
268: Ismael Juma, debug, IfStmt, log.debug("Manually disconnected from {}. Removed requests: {}.", nodeId, Utils.join(requestTypes, ", "));
389: Ismael Juma, trace, BlockStmt, log.trace("No version information found when sending {} with correlation id {} to node {}. " + "Assuming version {}.", clientRequest.apiKey(), clientRequest.correlationId(), nodeId, version);
419: Ismael Juma, debug, IfStmt, log.debug("Using older server API v{} to send {} {} with correlation id {} to node {}", header.apiVersion(), clientRequest.apiKey(), request, clientRequest.correlationId(), nodeId);
416: Ismael Juma, trace, IfStmt, log.trace("Sending {} {} with correlation id {} to node {}", clientRequest.apiKey(), request, clientRequest.correlationId(), nodeId);
462: Ismael Juma, error, CatchClause, log.error("Unexpected error during I/O", e);
484: Jason Gustafson, error, CatchClause, log.error("Uncaught error in request completion:", e);
564: Jason Gustafson, trace, IfStmt, log.trace("Removing node {} from least loaded node selection: is-blacked-out: {}, in-flight-requests: {}", node, this.connectionStates.isBlackedOut(node.idString(), now), currInflight);
557: Jason Gustafson, trace, IfStmt, log.trace("Found least loaded node {} connected with no in-flight requests", node);
572: Jason Gustafson, trace, BlockStmt, log.trace("Least loaded node selection failed to find an available node");
570: Jason Gustafson, trace, BlockStmt, log.trace("Found least loaded node {}", found);
609: Jason Gustafson, error, SwitchStmt, log.error("Connection to node {} failed authentication due to: {}", nodeId, exception.getMessage());
613: Rajini Sivaram, warn, SwitchStmt, log.warn("Connection to node {} terminated during authentication. This may indicate " + "that authentication failed due to invalid credentials.", nodeId);
617: Rajini Sivaram, warn, SwitchStmt, log.warn("Connection to node {} could not be established. Broker may not be available.", nodeId);
623: Ismael Juma, trace, ForeachStmt, log.trace("Cancelled request {} {} with correlation id {} due to node {} being disconnected", request.header.apiKey(), request.request, request.header.correlationId(), nodeId);
644: Mayuresh Gharat, debug, ForeachStmt, log.debug("Disconnecting from node {} due to request timeout.", nodeId);
688: Ismael Juma, trace, IfStmt, log.trace("Completed receive from node {} for {} with correlation id {}, received {}", req.destination, req.header.apiKey(), req.header.correlationId(), responseStruct);
706: Ismael Juma, warn, IfStmt, log.warn("Received error {} from node {} when making an ApiVersionsRequest with correlation id {}. Disconnecting.", apiVersionsResponse.error(), node, req.header.correlationId());
719: Colin P. Mccabe, debug, IfStmt, log.debug("Recorded API versions for node {}: {}", node, nodeVersionInfo);
732: Jay Kreps, debug, ForeachStmt, log.debug("Node {} disconnected.", node);
755: Ismael Juma, debug, IfStmt, log.debug("Completed connection to node {}. Ready.", node);
752: Ismael Juma, debug, IfStmt, log.debug("Completed connection to node {}. Fetching API versions.", node);
766: Ashish Singh, debug, IfStmt, log.debug("Initiating API versions fetch from node {}.", node);
801: Ismael Juma, Error, CatchClause, log.debug("Error connecting to node {}", node, e);
790: Ismael Juma, debug, TryStmt, log.debug("Initiating connection to node {}", node);
844: Yuto Kawamura, debug, IfStmt, log.debug("Give up sending metadata request since no node is available");
862: Ismael Juma, warn, BlockStmt, log.warn("Bootstrap broker {} disconnected", node);
882: Ashish Singh, Error, BlockStmt, log.warn("Error while fetching metadata with correlation id {} : {}", requestHeader.correlationId(), errors);
889: Ashish Singh, trace, IfStmt, log.trace("Ignoring empty metadata response with correlation id {}.", requestHeader.correlationId());
927: Ismael Juma, debug, IfStmt, log.debug("Sending metadata request {} to node {}", metadataRequest, node);
943: Ismael Juma, debug, IfStmt, log.debug("Initialize connection to node {} for sending metadata request", node);
409: Kamal C, debug, ConstructorDeclaration, log.debug("Kafka admin client initialized");
429: Kamal C, debug, IfStmt, log.debug("Moving hard shutdown time forward.");
427: Kamal C, debug, IfStmt, log.debug("Initiating close operation.");
436: Kamal C, debug, IfStmt, log.debug("Hard shutdown time is already earlier than requested.");
443: Kamal C, debug, IfStmt, log.debug("Waiting for the I/O thread to exit. Hard shutdown in {} ms.", deltaMs);
453: Kamal C, debug, CatchClause, log.debug("Interrupted while joining I/O thread", e);
451: Kamal C, debug, TryStmt, log.debug("Kafka admin client closed.");
528: Colin P. Mccabe, debug, IfStmt, log.debug("{} aborted at {} after {} attempt(s)", this, now, tries, new Exception(prettyPrintException(throwable)));
539: Colin P. Mccabe, trace, IfStmt, log.trace("{} attempting protocol downgrade.", this);
547: Colin P. Mccabe, debug, IfStmt, log.debug("{} timed out at {} after {} attempt(s)", this, now, tries, new Exception(prettyPrintException(throwable)));
556: Colin P. Mccabe, debug, IfStmt, log.debug("{} failed with non-retriable exception after {} attempt(s)", this, tries, new Exception(prettyPrintException(throwable)));
565: Colin P. Mccabe, debug, IfStmt, log.debug("{} failed after {} attempt(s)", this, tries, new Exception(prettyPrintException(throwable)));
572: Kamal C, debug, IfStmt, log.debug("{} failed: {}. Beginning retry #{}", this, prettyPrintException(throwable), tries);
715: Kamal C, trace, IfStmt, log.trace("Metadata is not ready yet. No cluster nodes found.");
719: Kamal C, trace, IfStmt, log.trace("Metadata is not ready yet. No controller found.");
723: Kamal C, trace, IfStmt, log.trace("Metadata is now ready.");
737: Kamal C, debug, BlockStmt, log.debug("Timed out {} new calls.", numTimedOut);
753: Kamal C, debug, BlockStmt, log.debug("Timed out {} call(s) with assigned nodes.", numTimedOut);
794: Kamal C, trace, MethodDeclaration, log.trace("Assigned {} to {}", call, node);
823: Kamal C, trace, IfStmt, log.trace("Client is not ready to send to {}. Must delay {} ms", node, nodeTimeout);
837: Kamal C, trace, ForStmt, log.trace("Sending {} to {}. correlationId={}", requestBuilder, node, clientRequest.correlationId());
869: Kamal C, debug, IfStmt, log.debug("Closing connection to {} to time out {}", nodeId, call);
867: Kamal C, warn, IfStmt, log.warn("Aborted call {} is still in callsInFlight.", call);
880: Kamal C, debug, BlockStmt, log.debug("Timed out {} call(s) in flight.", numTimedOut);
945: Colin P. Mccabe, error, IfStmt, log.error("Internal server error on {}: ignoring call {} in correlationIdToCall " + "that did not exist in callsInFlight", response.destination(), call);
966: Kamal C, trace, BlockStmt, log.trace("{} handleResponse failed with {}", call, prettyPrintException(t));
962: Kamal C, trace, BlockStmt, log.trace("{} got response {}", call, response.responseBody().toString(response.requestHeader().apiVersion()));
976: Kamal C, trace, IfStmt, log.trace("All work has been completed, and the I/O thread is now exiting.");
980: Kamal C, info, IfStmt, log.info("Forcing a hard I/O thread shutdown. Requests in progress will be aborted.");
983: Kamal C, debug, MethodDeclaration, log.debug("Hard shutdown in {} ms.", curHardShutdownTimeMs - now);
1010: Kamal C, trace, MethodDeclaration, log.trace("Thread starting");
1038: Kamal C, trace, WhileStmt, log.trace("Entering KafkaClient#poll(timeout={})", pollTimeout);
1040: Kamal C, trace, WhileStmt, log.trace("KafkaClient#poll retrieved {} response(s)", responses.size());
1057: Kamal C, debug, IfStmt, log.debug("Timed out {} remaining operations.", numTimedOut);
1061: Kamal C, debug, MethodDeclaration, log.debug("Exiting AdminClientRunnable thread.");
1076: Kamal C, debug, IfStmt, log.debug("Queueing {} with a timeout {} ms from now.", call, call.deadlineMs - now);
1088: Kamal C, debug, IfStmt, log.debug("The AdminClient thread has exited. Timing out {}.", call);
1103: Kamal C, debug, IfStmt, log.debug("The AdminClient is not accepting new calls. Timing out {}.", call);
1156: Colin P. Mccabe, warn, IfStmt, log.warn("Server response mentioned unknown topic {}", entry.getKey());
1219: Colin P. Mccabe, warn, IfStmt, log.warn("Server response mentioned unknown topic {}", entry.getKey());
1842: Dong Lin, Info, MethodDeclaration, final Map<Integer, KafkaFutureImpl<Map<String, DescribeLogDirsResponse.LogDirInfo>>> futures = new HashMap<>(brokers.size());
1845: Dong Lin, Info, ForeachStmt, futures.put(brokerId, new KafkaFutureImpl<Map<String, DescribeLogDirsResponse.LogDirInfo>>());
1862: Dong Lin, Info, MethodDeclaration, KafkaFutureImpl<Map<String, DescribeLogDirsResponse.LogDirInfo>> future = futures.get(brokerId);
1882: Dong Lin, Info, MethodDeclaration, final Map<TopicPartitionReplica, KafkaFutureImpl<DescribeReplicaLogDirsResult.ReplicaLogDirInfo>> futures = new HashMap<>(replicas.size());
1885: Dong Lin, Info, ForeachStmt, futures.put(replica, new KafkaFutureImpl<DescribeReplicaLogDirsResult.ReplicaLogDirInfo>());
1918: Dong Lin, Info, ForeachStmt, DescribeLogDirsResponse.LogDirInfo logDirInfo = responseEntry.getValue();
1924: Dong Lin, error, BlockStmt, handleFailure(new IllegalStateException("The error " + logDirInfo.error + " for log directory " + logDir + " in the response from broker " + brokerId + " is illegal"));
1929: Dong Lin, Info, ForeachStmt, DescribeLogDirsResponse.ReplicaInfo replicaInfo = replicaInfoEntry.getValue();
2527: Guozhang Wang, warn, IfStmt, log.warn("Skipping return offset for {} due to error {}.", topicPartition, error);
662: Jason Gustafson, debug, TryStmt, log.debug("Initializing the Kafka consumer");
787: Jason Gustafson, debug, TryStmt, log.debug("Kafka consumer initialized");
913: Guozhang Wang, debug, IfStmt, log.debug("Subscribed to topic(s): {}", Utils.join(topics, ", "));
975: Ashish Singh, debug, TryStmt, log.debug("Subscribed to pattern: {}", pattern);
1013: Guozhang Wang, debug, TryStmt, log.debug("Unsubscribed all topics or patterns and assigned partitions");
1062: Rekha Joshi, debug, IfStmt, log.debug("Subscribed to partition(s): {}", Utils.join(partitions, ", "));
1322: Jason Gustafson, debug, TryStmt, log.debug("Committing offsets: {}", offsets);
1344: Jason Gustafson, debug, TryStmt, log.debug("Seeking to offset {} for partition {}", offset, partition);
1366: Jason Gustafson, debug, ForeachStmt, log.debug("Seeking to beginning of partition {}", tp);
1392: Jason Gustafson, debug, ForeachStmt, log.debug("Seeking to end of partition {}", tp);
1543: Jason Gustafson, debug, TryStmt, log.debug("Pausing partitions {}", partitions);
1563: Jason Gustafson, debug, TryStmt, log.debug("Resuming partitions {}", partitions);
1742: Jason Gustafson, trace, MethodDeclaration, log.trace("Closing the Kafka consumer");
1749: Rajini Sivaram, error, CatchClause, log.error("Failed to close coordinator", t);
1758: Jason Gustafson, debug, MethodDeclaration, log.debug("Kafka consumer has been closed");
337: Vahid Hashemian, error, BlockStmt, log.error(topicPartition + " is assigned to more than one consumer.");
359: Vahid Hashemian, debug, IfStmt, log.debug(topicPartition + " can be moved from consumer " + otherConsumer + " to consumer " + consumer + " for a more balanced assignment.");
502: Vahid Hashemian, error, BlockStmt, log.error("The consumer " + consumer + " is assigned more partitions than the maximum possible.");
601: Vahid Hashemian, error, BlockStmt, log.error("Expected more than one potential consumer for partition '" + partition + "'");
606: Vahid Hashemian, error, BlockStmt, log.error("Expected partition '" + partition + "' to be assigned to a consumer");
882: Vahid Hashemian, error, IfStmt, log.error("A cycle of length " + (path.size() - 1) + " was found: " + path.toString());
899: Vahid Hashemian, error, IfStmt, log.error("Stickiness is violated for topic " + topicMovements.getKey() + "\nPartition movements for this topic occurred among the following consumer pairs:" + "\n" + topicMovements.getValue().toString());
227: Jason Gustafson, debug, IfStmt, log.debug("Coordinator discovery failed, refreshing metadata");
251: Jason Gustafson, debug, IfStmt, log.debug("No broker available to send FindCoordinator request");
343: Colin P. Mccabe, warn, CatchClause, log.warn("Interrupted while waiting for consumer heartbeat thread to close");
409: Jason Gustafson, info, SynchronizedStmt, log.info("Successfully joined group with generation {}", generation.generationId);
442: Jason Gustafson, info, MethodDeclaration, log.info("(Re-)joining group");
450: Colin P. Mccabe, debug, MethodDeclaration, log.debug("Sending JoinGroup ({}) to coordinator {}", requestBuilder, this.coordinator);
497: Jason Gustafson, error, IfStmt, log.error("Attempt to join group failed due to fatal error: {}", error.message());
491: Jason Gustafson, debug, IfStmt, log.debug("Attempt to join group failed due to obsolete coordinator information: {}", error.message());
485: Jason Gustafson, debug, IfStmt, log.debug("Attempt to join group failed due to unknown member id.");
479: Jason Gustafson, debug, IfStmt, log.debug("Attempt to join group rejected since coordinator {} is loading the group.", coordinator());
460: Jason Gustafson, debug, IfStmt, log.debug("Received successful JoinGroup response: {}", joinResponse);
513: Jason Gustafson, debug, MethodDeclaration, log.debug("Sending follower SyncGroup to coordinator {}: {}", this.coordinator, requestBuilder);
525: Jason Gustafson, debug, TryStmt, log.debug("Sending leader SyncGroup to coordinator {}: {}", this.coordinator, requestBuilder);
562: Jason Gustafson, debug, IfStmt, log.debug("SyncGroup failed: {}", error.message());
557: Jason Gustafson, debug, IfStmt, log.debug("SyncGroup failed: {}", error.message());
553: Jason Gustafson, debug, IfStmt, log.debug("SyncGroup failed because the group began another rebalance");
579: Jason Gustafson, debug, MethodDeclaration, log.debug("Sending FindCoordinator request to broker {}", node);
590: Jason Gustafson, debug, MethodDeclaration, log.debug("Received FindCoordinator response {}", resp);
613: Jason Gustafson, debug, IfStmt, log.debug("Group coordinator lookup failed: {}", error.message());
605: Jason Gustafson, info, SynchronizedStmt, log.info("Discovered group coordinator {}", coordinator);
657: Jason Gustafson, info, IfStmt, log.info("Group coordinator {} is unavailable or invalid, will attempt rediscovery", this.coordinator);
721: Jason Gustafson, warn, BlockStmt, log.warn("Close timed out with {} pending requests to coordinator, terminating client connections", client.pendingRequestCount(coordinator));
753: Jason Gustafson, debug, IfStmt, log.debug("LeaveGroup request failed with error: {}", error.message());
750: Jason Gustafson, debug, IfStmt, log.debug("LeaveGroup request returned successfully");
761: Jason Gustafson, debug, MethodDeclaration, log.debug("Sending Heartbeat request to coordinator {}", coordinator);
791: Guozhang Wang, info, IfStmt, log.info("Attempt to heartbeat failed for since member id {} is not valid.", generation.memberId);
787: Guozhang Wang, info, IfStmt, log.info("Attempt to heartbeat failed since generation {} is not current", generation.generationId);
783: Guozhang Wang, info, IfStmt, log.info("Attempt to heartbeat failed since group is rebalancing");
778: Guozhang Wang, info, IfStmt, log.info("Attempt to heartbeat failed since coordinator {} is either not started or not valid.", coordinator());
774: Jason Gustafson, debug, IfStmt, log.debug("Received successful Heartbeat response");
898: Jason Gustafson, debug, SynchronizedStmt, log.debug("Enabling heartbeat thread");
907: Jason Gustafson, debug, SynchronizedStmt, log.debug("Disabling heartbeat thread");
1001: Vahid Hashemian, error, CatchClause, log.error("An authentication error occurred in the heartbeat thread", e);
1004: Jason Gustafson, error, CatchClause, log.error("A group authorization error occurred in the heartbeat thread", e);
1008: Jason Gustafson, error, CatchClause, log.error("Unexpected interrupt received in heartbeat thread", e);
1011: Jason Gustafson, error, CatchClause, log.error("Heartbeat thread failed due to unexpected error", e);
1017: Jason Gustafson, debug, TryStmt, log.debug("Heartbeat thread has closed");
930: Jason Gustafson, debug, TryStmt, log.debug("Heartbeat thread started");
65: Jason Gustafson, debug, BlockStmt, log.debug("Skipping assignment for topic {} since no metadata is available", topic);
256: Jason Gustafson, info, MethodDeclaration, log.info("Setting newly assigned partitions {}", subscriptions.assignedPartitions());
263: Jason Gustafson, error, CatchClause, log.error("User provided listener {} failed on partition assignment", listener.getClass().getName(), e);
356: Jason Gustafson, debug, MethodDeclaration, log.debug("Performing assignment using strategy {} with subscriptions {}", assignor.name(), subscriptions);
376: Jason Gustafson, warn, IfStmt, log.warn("The following subscribed topics are not assigned to any members: {} ", notAssignedTopics);
382: Jason Gustafson, info, IfStmt, log.info("The following not-subscribed topics are assigned, and their metadata will be " + "fetched from the brokers: {}", newlyAddedTopics);
393: Jason Gustafson, debug, MethodDeclaration, log.debug("Finished assignment for group: {}", assignment);
411: Jason Gustafson, info, MethodDeclaration, log.info("Revoking previously assigned partitions {}", subscriptions.assignedPartitions());
418: Jason Gustafson, error, CatchClause, log.error("User provided listener {} failed on partition revocation", listener.getClass().getName(), e);
450: Jason Gustafson, debug, ForeachStmt, log.debug("Setting offset for partition {} to the committed offset {}", tp, offset);
631: Jason Gustafson, debug, MethodDeclaration, log.debug("Sending asynchronous auto-commit of offsets {}", allConsumedOffsets);
645: Jason Gustafson, debug, IfStmt, log.debug("Completed asynchronous auto-commit of offsets {}", offsets);
642: Jason Gustafson, warn, IfStmt, log.warn("Asynchronous auto-commit of offsets {} failed: {}", offsets, exception.getMessage());
638: Jason Gustafson, debug, IfStmt, log.debug("Asynchronous auto-commit of offsets {} failed due to retriable error: {}", offsets, exception);
659: Jason Gustafson, debug, CatchClause, log.debug("Auto-commit of offsets {} was interrupted before completion", allConsumedOffsets);
664: Jason Gustafson, warn, CatchClause, log.warn("Synchronous auto-commit of offsets {} failed: {}", allConsumedOffsets, e.getMessage());
655: Jason Gustafson, debug, TryStmt, log.debug("Sending synchronous auto-commit of offsets {}", allConsumedOffsets);
657: Jason Gustafson, debug, BlockStmt, log.debug("Auto-commit of offsets {} timed out before completion", allConsumedOffsets);
673: Jason Gustafson, error, BlockStmt, log.error("Offset commit with offsets {} failed", offsets, exception);
720: Jason Gustafson, trace, MethodDeclaration, log.trace("Sending OffsetCommit request with {} to coordinator {}", offsets, coordinator);
748: Jason Gustafson, error, IfStmt, log.error("Offset commit failed on partition {} at offset {}: {}", tp, offset, error.message());
746: Jason Gustafson, debug, IfStmt, log.debug("Committed offset {} for partition {}", offset, tp);
786: Jason Gustafson, error, IfStmt, log.error("Not authorized to commit to topics {}", unauthorizedTopics);
806: Jason Gustafson, debug, MethodDeclaration, log.debug("Fetching committed offsets for partitions: {}", partitions);
821: Jason Gustafson, debug, IfStmt, log.debug("Offset fetch failed: {}", error.message());
856: Jason Gustafson, debug, IfStmt, log.debug("Found no committed offset for partition {}", tp);
844: Jason Gustafson, debug, IfStmt, log.debug("Failed to fetch offset for partition {}: {}", tp, error.message());
64: Anna Povzner, Error, CatchClause, log.warn("Error executing interceptor onConsume callback", e);
85: Anna Povzner, Error, CatchClause, log.warn("Error executing interceptor onCommit callback", e);
99: Anna Povzner, error, CatchClause, log.error("Failed to close consumer interceptor ", e);
486: Jason Gustafson, debug, IfStmt, log.debug("Raising WakeupException in response to user wakeup");
568: Jason Gustafson, debug, IfStmt, log.debug("Cancelled request with header {} due to node {} being disconnected", response.requestHeader(), response.destination());
200: Colin P. Mccabe, debug, IfStmt, log.debug("Sending {} {} to broker {}", isolationLevel, data.toString(), fetchTarget);
209: Colin P. Mccabe, error, IfStmt, log.error("Unable to find FetchSessionHandler for node {}. Ignoring fetch response.", fetchTarget.id());
225: Jason Gustafson, debug, ForeachStmt, log.debug("Fetch {} at offset {} for partition {} returned fetch data {}", isolationLevel, fetchOffset, partition, fetchData);
531: Jason Gustafson, trace, IfStmt, log.trace("Returning fetched records at offset {} for assigned partition {} and update " + "position to {}", position, partitionRecords.partition, nextOffset);
518: Jason Gustafson, debug, IfStmt, log.debug("Not returning fetched records for partition {} since it is no longer assigned", partitionRecords.partition);
567: Jason Gustafson, info, IfStmt, log.info("Resetting offset for partition {} to offset {}.", partition, offsetData.offset);
565: Jason Gustafson, debug, IfStmt, log.debug("Skipping reset of partition {} since an alternative reset has been requested", partition);
563: Jason Gustafson, debug, IfStmt, log.debug("Skipping reset of partition {} since reset is no longer needed", partition);
561: Jason Gustafson, debug, IfStmt, log.debug("Skipping reset of partition {} since it is no longer assigned", partition);
606: Jason Gustafson, error, BlockStmt, log.error("Discarding error in ListOffsetResponse because another error is pending", e);
674: Jason Gustafson, debug, IfStmt, log.debug("Leader for partition {} is unavailable for fetching offset", tp);
671: Jason Gustafson, debug, IfStmt, log.debug("Leader for partition {} is unknown for fetching offset", tp);
712: Jason Gustafson, debug, MethodDeclaration, log.debug("Sending ListOffsetRequest {} to broker {}", builder, node);
718: Jiangjie Qin, trace, MethodDeclaration, log.trace("Received ListOffsetResponse {} from broker {}", lor, node);
791: Colin P. Mccabe, warn, IfStmt, log.warn("Attempt to fetch offsets for partition {} failed due to: {}", topicPartition, error.message());
786: Jason Gustafson, warn, IfStmt, log.warn("Received unknown topic or partition error in ListOffset request for partition {}", topicPartition);
782: Jiangjie Qin, debug, IfStmt, log.debug("Attempt to fetch offsets for partition {} failed due to obsolete leadership information, retrying.", topicPartition);
768: Colin P. Mccabe, debug, IfStmt, log.debug("Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}", topicPartition, partitionData.offset, partitionData.timestamp);
760: Colin P. Mccabe, debug, IfStmt, log.debug("Handling v0 ListOffsetResponse response for {}. Fetched offset {}", topicPartition, offset);
866: Jason Gustafson, debug, IfStmt, log.debug("Added {} fetch request for partition {} at offset {} to node {}", isolationLevel, partition, position, node);
848: Jason Gustafson, trace, IfStmt, log.trace("Skipping fetch for partition {} because there is an in-flight request to {}", partition, node);
960: Jason Gustafson, warn, IfStmt, log.warn("Unknown error fetching data for topic-partition {}", tp);
957: Jason Gustafson, warn, IfStmt, log.warn("Not authorized to read from topic {}.", tp.topic());
951: Jason Gustafson, info, IfStmt, log.info("Fetch offset {} is out of range for partition {}, resetting offset", fetchOffset, tp);
948: Jason Gustafson, debug, IfStmt, log.debug("Discarding stale fetch response for partition {} since the fetched offset {}" + "does not match the current offset {}", tp, fetchOffset, subscriptions.position(tp));
944: Jason Gustafson, warn, IfStmt, log.warn("Received unknown topic or partition error in fetch for partition {}", tp);
941: Jason Gustafson, Error, IfStmt, log.debug("Error in fetch for partition {}: {}", tp, error.exceptionName());
897: Jason Gustafson, debug, IfStmt, log.debug("Discarding stale fetch response for partition {} since its offset {} does not match " + "the expected offset {}", tp, fetchOffset, position);
902: Jason Gustafson, trace, IfStmt, log.trace("Preparing to read {} bytes of data for partition {} with offset {}", partition.records.sizeInBytes(), tp, position);
926: Jason Gustafson, trace, IfStmt, log.trace("Updating high watermark for partition {} to {}", tp, partition.highWatermark);
931: huxihx, trace, IfStmt, log.trace("Updating log start offset for partition {} to {}", tp, partition.logStartOffset);
936: Jason Gustafson, trace, IfStmt, log.trace("Updating last stable offset for partition {} to {}", tp, partition.lastStableOffset);
1120: Jason Gustafson, debug, IfStmt, log.debug("Skipping aborted record batch from partition {} with producerId {} and " + "offsets {} to {}", partition, producerId, currentBatch.baseOffset(), currentBatch.lastOffset());
347: huxihx, trace, TryStmt, log.trace("Starting the Kafka producer");
452: huxihx, debug, TryStmt, log.debug("Kafka producer started");
489: Apurva Mehta, info, BlockStmt, log.info("Instantiated an idempotent producer.");
487: Apurva Mehta, info, BlockStmt, log.info("Instantiated a transactional producer.");
529: Apurva Mehta, info, IfStmt, log.info("Overriding the default {} to all since idempotence is enabled.", ProducerConfig.ACKS_CONFIG);
860: Jay Kreps, debug, CatchClause, log.debug("Exception occurred during message send:", e);
842: Jay Kreps, trace, TryStmt, log.trace("Sending record {} with callback {} to topic {} partition {}", record, callback, record.topic(), partition);
852: Jay Kreps, trace, IfStmt, log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);
917: Jason Gustafson, trace, DoStmt, log.trace("Requesting metadata update for topic {}.", topic);
997: Jay Kreps, trace, MethodDeclaration, log.trace("Flushing accumulated records in producer.");
1073: Jiangjie Qin, info, MethodDeclaration, log.info("Closing the Kafka producer with timeoutMillis = {} ms.", timeUnit.toMillis(timeout));
1090: Jiangjie Qin, error, CatchClause, log.error("Interrupted while joining ioThread", t);
1079: Jiangjie Qin, warn, IfStmt, log.warn("Overriding close timeout {} ms to 0 ms in order to prevent useless blocking due to self-join. " + "This means you have incorrectly invoked close with a non-zero timeout from the producer call-back.", timeout);
1097: Jiangjie Qin, info, IfStmt, log.info("Proceeding to force close the producer since pending requests could not be completed " + "within timeout {} ms.", timeout);
1116: huxihx, debug, MethodDeclaration, log.debug("Kafka producer has been closed");
52: Jeremy Custenborder, Error, IfStmt, log.error("Error when sending message to topic {} with key: {}, value: {} with error:", topic, keyString, valueString, e);
156: Jason Gustafson, trace, MethodDeclaration, log.trace("Aborting batch for partition {}", topicPartition, exception);
174: Jason Gustafson, trace, IfStmt, log.trace("Failed to produce messages to {}.", topicPartition, exception);
171: Jason Gustafson, trace, IfStmt, log.trace("Successfully produced messages to {} with base offset {}.", topicPartition, baseOffset);
180: Jason Gustafson, debug, IfStmt, log.debug("ProduceResponse returned for {} after batch had already been aborted.", topicPartition);
207: Ismael Juma, Error, CatchClause, log.error("Error executing user-provided callback on message for topic-partition '{}'", topicPartition, e);
68: Anna Povzner, Error, BlockStmt, log.warn("Error executing interceptor onSend callback", e);
66: Anna Povzner, Error, BlockStmt, log.warn("Error executing interceptor onSend callback for topic: {}, partition: {}", record.topic(), record.partition(), e);
91: Anna Povzner, Error, CatchClause, log.warn("Error executing interceptor onAcknowledgement callback", e);
121: Anna Povzner, Error, CatchClause, log.warn("Error executing interceptor onAcknowledgement callback", e);
135: Anna Povzner, error, CatchClause, log.error("Failed to close producer interceptor ", e);
207: Jiangjie Qin, trace, TryStmt, log.trace("Allocating a new {} byte message buffer for topic {} partition {}", size, tp.topic(), tp.partition());
384: Apurva Mehta, debug, IfStmt, log.debug("Reordered incoming batch with sequence {} for partition {}. It was placed in the queue at " + "position {}", batch.baseSequence(), batch.topicPartition, orderedBatches.size());
566: Apurva Mehta, debug, IfStmt, log.debug("Assigned producerId {} and producerEpoch {} to batch with base sequence " + "{} being sent to partition {}", producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, batch.baseSequence(), tp);
158: Jay Kreps, debug, MethodDeclaration, log.debug("Starting Kafka producer I/O thread.");
165: Jay Kreps, error, CatchClause, log.error("Uncaught error in kafka producer I/O thread: ", e);
169: Jay Kreps, debug, MethodDeclaration, log.debug("Beginning shutdown of Kafka producer I/O thread, sending remaining records.");
178: Jay Kreps, error, CatchClause, log.error("Uncaught error in kafka producer I/O thread: ", e);
190: Steven Wu, error, CatchClause, log.error("Failed to close network client", e);
193: Jay Kreps, debug, MethodDeclaration, log.debug("Shutdown of Kafka producer I/O thread has completed.");
233: Rajini Sivaram, trace, CatchClause, log.trace("Authentication exception while processing transactional request: {}", e);
256: Guozhang Wang, debug, IfStmt, log.debug("Requesting metadata update due to unknown leader topics from the batched records: {}", result.unknownLeaderTopics);
288: Apurva Mehta, trace, BlockStmt, log.trace("Expired {} batches in accumulator", expiredBatches.size());
305: Jay Kreps, trace, IfStmt, log.trace("Nodes with data ready to send: {}", result.readyNodes);
366: Jason Gustafson, debug, CatchClause, log.debug("Disconnect from {} while trying to send request {}. Going " + "to back off and retry", targetNode, requestBuilder);
360: Jason Gustafson, debug, IfStmt, log.debug("Sending transactional request {} to node {}", requestBuilder, targetNode);
385: Jason Gustafson, error, IfStmt, log.error("Aborting producer batches due to fatal error", exception);
451: Jason Gustafson, debug, CatchClause, log.debug("Broker {} disconnected while awaiting InitProducerId response", e);
444: Jason Gustafson, debug, IfStmt, log.debug("Could not find an available broker to send InitProducerIdRequest to. " + "We will back off and try again.");
438: Jason Gustafson, debug, IfStmt, log.debug("Retriable error from InitProducerId response", error.message());
453: Jason Gustafson, trace, WhileStmt, log.trace("Retry InitProducerIdRequest in {}ms.", retryBackoffMs);
476: Jason Gustafson, trace, IfStmt, log.trace("Received produce response from node {} with correlation id {}", response.destination(), correlationId);
471: Colin P. Mccabe, warn, IfStmt, log.warn("Cancelled request {} due to a version mismatch with node {}", response, response.destination(), response.versionMismatch());
466: Jason Gustafson, trace, IfStmt, log.trace("Cancelled request with header {} due to node {} being disconnected", requestHeader, response.destination());
524: Apurva Mehta, warn, IfStmt, log.warn("Got error produce response with correlation id {} on topic-partition {}, retrying ({} attempts left). Error: {}", correlationId, batch.topicPartition, this.retries - batch.attempts() - 1, error);
567: Guozhang Wang, warn, IfStmt, log.warn("Received invalid metadata error in produce request on partition {} due to {}. Going " + "to request metadata update now", batch.topicPartition, error.exception().toString());
564: Apurva Mehta, warn, IfStmt, log.warn("Received unknown topic or partition error in produce request on partition {}. The " + "topic/partition may not exist or the user may not have Describe access to it", batch.topicPartition);
591: Apurva Mehta, debug, IfStmt, log.debug("ProducerId: {}; Set last ack'd sequence number for topic-partition {} to {}", batch.producerId(), batch.topicPartition, transactionManager.lastAckedSequence(batch.topicPartition));
611: Apurva Mehta, error, IfStmt, log.error("The broker returned {} for topic-partition " + "{} at offset {}. This indicates data loss on the broker, and should be investigated.", exception, batch.topicPartition, baseOffset);
706: Colin P. Mccabe, trace, MethodDeclaration, log.trace("Sent produce request to {}: {}", nodeId, requestBuilder);
255: Jason Gustafson, debug, MethodDeclaration, log.debug("Begin adding offsets {} for consumer group {} to transaction", offsets, consumerGroupId);
269: Jason Gustafson, debug, MethodDeclaration, log.debug("Begin adding new partition {} to transaction", topicPartition);
328: Jason Gustafson, debug, IfStmt, log.debug("Skipping transition to abortable error state since the transaction is already being " + "aborted. Underlying exception: ", exception);
372: Jason Gustafson, info, MethodDeclaration, log.info("ProducerId set to {} with epoch {}", producerIdAndEpoch.producerId, producerIdAndEpoch.epoch);
499: tedyu, trace, IfStmt, log.trace("Partition {} keeps lastOffset at {}", batch.topicPartition, lastOffset);
513: Apurva Mehta, debug, MethodDeclaration, log.debug("producerId: {}, send to partition {} failed fatally. Reducing future sequence numbers by {}", batch.producerId(), batch.topicPartition, batch.recordCount);
530: Apurva Mehta, info, ForeachStmt, log.info("Resetting sequence number of batch with current sequence {} for partition {} to {}", inFlightBatch.baseSequence(), batch.topicPartition, newSequence);
538: Apurva Mehta, info, ForeachStmt, log.info("Resetting sequence number of batch with current sequence {} for partition {} to {}", inFlightBatch.baseSequence(), inFlightBatch.topicPartition, sequence);
562: Apurva Mehta, debug, MethodDeclaration, log.debug("Marking partition {} unresolved", topicPartition);
583: Apurva Mehta, info, IfStmt, log.info("No inflight batches remaining for {}, last ack'd sequence for partition is {}, next sequence is {}. " + "Going to reset producer state.", topicPartition, lastAckedSequence(topicPartition), sequenceNumber(topicPartition));
617: Jason Gustafson, trace, IfStmt, log.trace("Not sending transactional request {} because we are in an error state", nextRequestHandler.requestBuilder());
625: Jason Gustafson, debug, IfStmt, log.debug("Not sending EndTxn for completed transaction since no partitions " + "or offsets were successfully added");
633: Jason Gustafson, trace, BlockStmt, log.trace("Request {} dequeued for sending", nextRequestHandler.requestBuilder());
772: Jason Gustafson, debug, BlockStmt, log.debug("Transition from state {} to {}", currentState, target);
770: Jason Gustafson, debug, BlockStmt, log.debug("Transition from state {} to error state {}", currentState, target, lastError);
800: Jason Gustafson, debug, MethodDeclaration, log.debug("Enqueuing transactional request {}", requestHandler.requestBuilder());
904: Jason Gustafson, trace, IfStmt, log.trace("Received transactional response {} for request {}", response.responseBody(), requestBuilder());
897: Jason Gustafson, debug, IfStmt, log.debug("Disconnected from {}. Will retry.", response.destination());
1048: Jason Gustafson, error, IfStmt, log.error("Could not add partition {} due to unexpected error {}", topicPartition, error);
1044: Jason Gustafson, debug, IfStmt, log.debug("Did not attempt to add partition {} to transaction because other partitions in the " + "batch had errors.", topicPartition);
1067: Jason Gustafson, debug, IfStmt, log.debug("Successfully added partitions {} to transaction", partitions);
1220: Jason Gustafson, debug, IfStmt, log.debug("Successfully added partition for consumer group {} to transaction", builder.consumerGroupId());
1282: Jason Gustafson, debug, IfStmt, log.debug("Successfully added offsets {} from consumer group {} to transaction.", builder.offsets(), builder.consumerGroupId());
279: Jay Kreps, info, MethodDeclaration, log.info(b.toString());
287: Mickael Maison, warn, BlockStmt, log.warn("The configuration '{}' was supplied but isn't a known config.", key);
61: radai-rosenblatt, trace, MethodDeclaration, log.trace("allocated buffer of size {} and identity {}", sizeBytes, ref.hashCode);
76: radai-rosenblatt, trace, MethodDeclaration, log.trace("released buffer of size {} and identity {}", metadata.sizeBytes, ref.hashCode);
108: radai-rosenblatt, debug, CatchClause, log.debug("interrupted", e);
106: radai-rosenblatt, error, TryStmt, log.error("Reclaimed buffer of size {} and identity {} that was not properly release()ed. This is a bug.", metadata.sizeBytes, ref.hashCode);
112: radai-rosenblatt, info, MethodDeclaration, log.info("GC listener shutting down");
78: radai-rosenblatt, trace, IfStmt, log.trace("refused to allocate buffer of size {}", sizeBytes);
114: radai-rosenblatt, trace, MethodDeclaration, log.trace("allocated buffer of size {} ", justAllocated.capacity());
119: radai-rosenblatt, trace, MethodDeclaration, log.trace("released buffer of size {}", justReleased.capacity());
205: Erik Kringen, Error, CatchClause, log.warn("Error getting JMX attribute '{}'", name, e);
414: Aditya Auradkar, debug, IfStmt, log.debug("Added sensor with name {}", name);
447: Aditya Auradkar, debug, IfStmt, log.debug("Removed sensor with name {}", name);
531: Mickael Maison, Error, CatchClause, log.error("Error when removing metric from " + reporter.getClass().getName(), e);
564: Mickael Maison, Error, CatchClause, log.error("Error when registering metric on " + reporter.getClass().getName(), e);
600: Aditya Auradkar, debug, IfStmt, log.debug("Removing expired sensor {}", sensorEntry.getKey());
651: Mickael Maison, Error, CatchClause, log.error("Error when closing " + reporter.getClass().getName(), e);
98: Ismael Juma, error, BlockStmt, log.error("mismatch in sending bytes over socket; expected: " + size + " actual: " + totalWritten);
100: Ismael Juma, trace, MethodDeclaration, log.trace("Bytes written as part of multi-send call: {}, total bytes written so far: {}, expected bytes to write: {}", totalWrittenPerCall, totalWritten, size);
142: radai-rosenblatt, trace, BlockStmt, log.trace("Broker low on memory - could not allocate buffer of size {} for source {}", requestedBufferSize, source);
50: Sriharsha Chintalapani, warn, CatchClause, log.warn("Failed to create channel due to ", e);
201: Sriharsha Chintalapani, info, CatchClause, log.info("Failed to create channel due to ", e);
219: Jason Gustafson, debug, IfStmt, log.debug("Immediately connected to node {}", id);
318: Gwen Shapira, error, CatchClause, log.error("Exception closing nioSelector:", e);
344: Rajini Sivaram, error, IfStmt, log.error("Unexpected exception during send, closing connection {} and rethrowing exception {}", connectionId, e);
397: radai-rosenblatt, trace, IfStmt, log.trace("Broker no longer low on memory - unmuting incoming sockets");
531: Ismael Juma, warn, IfStmt, log.warn("Unexpected error from {}; closing connection", desc, e);
529: Rajini Sivaram, debug, IfStmt, log.debug("Connection with {} disconnected due to authentication exception", desc, e);
527: Ismael Juma, debug, BlockStmt, log.debug("Connection with {} disconnected", desc, e);
474: Manikumar Reddy O, debug, IfStmt, log.debug("Created socket with SO_RCVBUF = {}, SO_SNDBUF = {}, SO_TIMEOUT = {} to node {}", socketChannel.socket().getReceiveBufferSize(), socketChannel.socket().getSendBufferSize(), socketChannel.socket().getSoTimeout(), channel.id());
639: Rajini Sivaram, trace, BlockStmt, log.trace("About to close the idle connection from {} due to being idle for {} millis", connectionId, (currentTimeNanos - expiredConnection.getValue()) / 1000 / 1000);
732: parafiend, debug, IfStmt, log.debug("Tracking closing connection {} to process outstanding requests", channel.id());
748: Sriharsha Chintalapani, error, CatchClause, log.error("Exception closing connection to node {}:", channel.id(), e);
96: Sriharsha Chintalapani, info, CatchClause, log.info("Failed to create channel due to ", e);
177: Guozhang Wang, warn, CatchClause, log.warn("Failed to send SSL Close message", ie);
294: Rajini Sivaram, trace, SwitchStmt, log.trace("SSLHandshake NEED_TASK channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}", channelId, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());
299: Rajini Sivaram, trace, SwitchStmt, log.trace("SSLHandshake NEED_WRAP channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}", channelId, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());
316: Rajini Sivaram, trace, SwitchStmt, log.trace("SSLHandshake NEED_WRAP channelId {}, handshakeResult {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}", channelId, handshakeResult, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());
325: Rajini Sivaram, trace, SwitchStmt, log.trace("SSLHandshake NEED_UNWRAP channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}", channelId, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());
347: Rajini Sivaram, trace, SwitchStmt, log.trace("SSLHandshake NEED_UNWRAP channelId {}, handshakeResult {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}", channelId, handshakeResult, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());
412: Rajini Sivaram, debug, IfStmt, log.debug("SSL handshake completed successfully with peerHost '{}' peerPort {} peerPrincipal '{}' cipherSuite '{}'", session.getPeerHost(), session.getPeerPort(), peerPrincipal(), session.getCipherSuite());
416: Sriharsha Chintalapani, trace, IfStmt, log.trace("SSLHandshake FINISHED channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {} ", channelId, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());
430: Rajini Sivaram, trace, MethodDeclaration, log.trace("SSLHandshake handshakeWrap {}", channelId);
456: Rajini Sivaram, trace, MethodDeclaration, log.trace("SSLHandshake handshakeUnwrap {}", channelId);
474: Rajini Sivaram, trace, DoStmt, log.trace("SSLHandshake handshakeUnwrap: handshakeStatus {} status {}", handshakeStatus, result.getStatus());
520: Ismael Juma, trace, IfStmt, log.trace("Renegotiation requested, but it is not supported, channelId {}, " + "appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}", channelId, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());
714: Jaikiran Pai, debug, CatchClause, log.debug("SSL peer is not authenticated, returning ANONYMOUS instead");
807: Ismael Juma, debug, CatchClause, log.debug("SSLEngine.closeInBound() raised an exception.", e);
719: David Jacot, warn, IfStmt, log.warn("Unexpected error code: {}.", code);
84: Jason Gustafson, debug, BlockStmt, log.debug("Received unknown control record key version {}. Parsing as version {}", version, CURRENT_CONTROL_RECORD_KEY_VERSION);
117: Jason Gustafson, debug, BlockStmt, log.debug("Received end transaction marker value version {}. Parsing as version {}", version, CURRENT_END_TXN_MARKER_VERSION);
209: Jason Gustafson, warn, BlockStmt, log.warn("Record batch from {} with last offset {} exceeded max record batch size {} after cleaning " + "(new size is {}). Consumers with version earlier than 0.10.1.0 may need to " + "increase their fetch sizes.", partition, batch.lastOffset(), maxRecordBatchSize, filteredBatchSize);
93: Jason Gustafson, ERROR, ForeachStmt, Errors error = Errors.forCode(logDirStruct.get(ERROR_CODE));
132: Jason Gustafson, ERROR, ForeachStmt, logDirStruct.set(ERROR_CODE, logDirInfo.error.code());
135: Dong Lin, Info, ForeachStmt, Map<String, Map<Integer, ReplicaInfo>> replicaInfosByTopic = CollectionUtils.groupDataByTopic(logDirInfo.replicaInfos);
169: Rajini Sivaram, error, BlockStmt, updateErrorCounts(errorCounts, logDirInfo.error);
69: Rajini Sivaram, warn, BlockStmt, LOG.warn("Server config {} should be prefixed with SASL mechanism name, ignoring config", SaslConfigs.SASL_JAAS_CONFIG);
109: Ismael Juma, debug, IfStmt, LOG.debug("System property '" + JaasUtils.JAVA_LOGIN_CONFIG_PARAM + "' is not set, using default JAAS " + "configuration.");
106: Ismael Juma, debug, IfStmt, LOG.debug("System property '" + JaasUtils.JAVA_LOGIN_CONFIG_PARAM + "' and Kafka SASL property '" + SaslConfigs.SASL_JAAS_CONFIG + "' are not set, using default JAAS configuration.");
48: Rajini Sivaram, error, IfStmt, LOG.error("JAAS configuration is present, but system property " + ZK_SASL_CLIENT + " is set to false, which disables " + "SASL in the ZooKeeper client");
61: Rajini Sivaram, info, MethodDeclaration, log.info("Successfully logged in.");
133: Ismael Juma, trace, MethodDeclaration, LOGGER.trace("{} acquired", this);
154: Ismael Juma, trace, SynchronizedStmt, LOGGER.trace("{} released", this);
149: Sriharsha Chintalapani, debug, MethodDeclaration, LOG.debug("Creating SaslClient: client={};service={};serviceHostname={};mechs={}", clientPrincipalName, servicePrincipal, host, Arrays.toString(mechs));
255: Rajini Sivaram, debug, IfStmt, LOG.debug("Set SASL client state to {}", saslState);
398: Rajini Sivaram, debug, CatchClause, LOG.debug("Invalid SASL mechanism response, server may be expecting only GSSAPI tokens");
189: Rajini Sivaram, debug, MethodDeclaration, LOG.debug("Creating SaslServer for {} with mechanism {}", kerberosName, saslMechanism);
208: Sriharsha Chintalapani, warn, CatchClause, LOG.warn("Cannot add private credential to subject; clients authentication may fail", ex);
316: Rajini Sivaram, debug, IfStmt, LOG.debug("Set SASL server state to {}", saslState);
432: Rajini Sivaram, debug, IfStmt, LOG.debug("Received client packet of length {} starting with bytes 0x{}, process as GSSAPI packet", requestBytes.length, tokenBuilder);
435: Rajini Sivaram, debug, IfStmt, LOG.debug("First client packet is not a SASL mechanism request, using default mechanism GSSAPI");
410: Jason Gustafson, debug, TryStmt, LOG.debug("Handling Kafka request {}", apiKey);
459: Rajini Sivaram, debug, IfStmt, LOG.debug("SASL mechanism '{}' requested by client is not supported", clientMechanism);
455: Rajini Sivaram, debug, IfStmt, LOG.debug("Using SASL mechanism '{}' provided by client", clientMechanism);
63: Sriharsha Chintalapani, trace, MethodDeclaration, LOG.trace("Client supplied realm: {} ", rc.getDefaultText());
70: Jason Gustafson, info, MethodDeclaration, LOG.info("Successfully authenticated client: authenticationID={}; authorizationID={}.", authenticationID, authorizationID);
126: Edoardo Comar, debug, IfStmt, log.debug("[Principal={}]: It is not a Kerberos ticket", principal);
131: Edoardo Comar, debug, MethodDeclaration, log.debug("[Principal={}]: It is a Kerberos ticket", principal);
139: Edoardo Comar, info, MethodDeclaration, log.info("[Principal={}]: TGT refresh thread started.", principal);
154: Sriharsha Chintalapani, warn, IfStmt, log.warn("The TGT cannot be renewed beyond the next expiry date: {}." + "This process will not be able to authenticate new SASL connections after that " + "time (for example, it will not be able to authenticate a new connection with a Kafka " + "Broker).  Ask your system administrator to either increase the " + "'renew until' time by doing : 'modprinc -maxrenewlife {} ' within " + "kadmin, or instead, to generate a keytab for {}. Because the TGT's " + "expiry cannot be further extended by refreshing, exiting refresh thread now.", expiryDate, principal, principal);
177: Edoardo Comar, warn, IfStmt, log.warn("[Principal={}]: TGT refresh thread time adjusted from {} to {} since the former is sooner " + "than the minimum refresh interval ({} seconds) from now.", principal, until, newUntil, minTimeBeforeRelogin / 1000);
170: Edoardo Comar, info, IfStmt, log.info("[Principal={}]: Refreshing now because expiry is before next scheduled refresh time.", principal);
185: Edoardo Comar, error, IfStmt, log.error("[Principal={}]: Next refresh: {} is later than expiry {}. This may indicate a clock skew problem." + "Check that this host and the KDC hosts' clocks are in sync. Exiting refresh thread.", principal, nextRefreshDate, expiryDate);
148: Edoardo Comar, warn, IfStmt, log.warn("[Principal={}]: No TGT found: will try again at {}", principal, nextRefreshDate);
201: Edoardo Comar, error, IfStmt, log.error("[Principal={}]: NextRefresh: {} is in the past: exiting refresh thread. Check" + " clock sync between this host and KDC - (KDC's clock is likely ahead of this host)." + " Manual intervention will be required for this client to successfully authenticate." + " Exiting refresh thread.", principal, nextRefreshDate);
193: Edoardo Comar, info, IfStmt, log.info("[Principal={}]: TGT refresh sleeping until: {}", principal, until);
197: Edoardo Comar, warn, CatchClause, log.warn("[Principal={}]: TGT renewal thread has been interrupted and will exit.", principal);
226: Satish Duggana, warn, IfStmt, log.warn("[Principal={}]: Could not renew TGT due to problem running shell command: '{} {}'. " + "Exiting refresh thread.", principal, kinitCmd, kinitArgs, e);
222: Edoardo Comar, error, CatchClause, log.error("[Principal={}]: Interrupted while renewing TGT, exiting Login thread", principal);
212: Edoardo Comar, debug, TryStmt, log.debug("[Principal={}]: Running ticket cache refresh command: {} {}", principal, kinitCmd, kinitArgs);
255: Edoardo Comar, error, CatchClause, log.error("[Principal={}]: Failed to refresh TGT: refresh thread exiting now.", principal, le);
250: Edoardo Comar, error, IfStmt, log.error("[Principal={}]: Could not refresh TGT.", principal, le);
246: Edoardo Comar, error, CatchClause, log.error("[Principal={}]: Interrupted during login retry after LoginException:", principal, le);
272: Edoardo Comar, warn, CatchClause, log.warn("[Principal={}]: Error while waiting for Login thread to shutdown.", principal, e);
310: Edoardo Comar, info, MethodDeclaration, log.info("[Principal={}]: TGT valid starting at: {}", principal, tgt.getStartTime());
311: Edoardo Comar, info, MethodDeclaration, log.info("[Principal={}]: TGT expires: {}", principal, tgt.getEndTime());
327: Ismael Juma, debug, IfStmt, log.debug("Found TGT with client principal '{}' and server principal '{}'.", ticket.getClient().getName(), ticket.getServer().getName());
338: Edoardo Comar, warn, IfStmt, log.warn("[Principal={}]: Not attempting to re-login since the last re-login was attempted less than {} seconds before.", principal, minTimeBeforeRelogin / 1000);
360: Colin P. Mccabe, info, SynchronizedStmt, log.info("Initiating logout for {}", principal);
370: Sriharsha Chintalapani, info, SynchronizedStmt, log.info("Initiating re-login for {}", principal);
107: Rajini Sivaram, debug, CatchClause, log.debug("Extensions callback is not supported by client callback handler {}, no extensions will be added", callbackHandler);
185: Rajini Sivaram, debug, MethodDeclaration, log.debug("Setting SASL/{} client state to {}", mechanism, state);
102: Rajini Sivaram, debug, IfStmt, log.debug("Unsupported extensions will be ignored, supported {}, provided {}", SUPPORTED_EXTENSIONS, scramExtensions.extensionNames());
215: Rajini Sivaram, debug, MethodDeclaration, log.debug("Setting SASL/{} server state to {}", mechanism, state);
44: Manikumar Reddy, Error, CatchClause, log.warn("Error while loading kafka-version.properties :" + e.getMessage());
66: Manikumar Reddy, Error, CatchClause, log.warn("Error registering AppInfo mbean", e);
79: Manikumar Reddy, Error, CatchClause, log.warn("Error unregistering AppInfo mbean", e);
109: Manikumar Reddy, info, ConstructorDeclaration, log.info("Kafka version : " + AppInfoParser.getVersion());
110: Manikumar Reddy, info, ConstructorDeclaration, log.info("Kafka commitId : " + AppInfoParser.getCommitId());
51: Kamal C, error, MethodDeclaration, log.error("Uncaught exception in thread '{}':", name, e);
138: Narendra kumar, TRACE, IfStmt, writeLog(null, LocationAwareLogger.TRACE_INT, message, null, null);
145: Narendra kumar, TRACE, IfStmt, writeLog(null, LocationAwareLogger.TRACE_INT, format, new Object[] { arg }, null);
152: Narendra kumar, TRACE, IfStmt, writeLog(null, LocationAwareLogger.TRACE_INT, format, new Object[] { arg1, arg2 }, null);
159: Narendra kumar, TRACE, IfStmt, writeLog(null, LocationAwareLogger.TRACE_INT, format, args, null);
166: Narendra kumar, TRACE, IfStmt, writeLog(null, LocationAwareLogger.TRACE_INT, msg, null, t);
173: Narendra kumar, TRACE, IfStmt, writeLog(marker, LocationAwareLogger.TRACE_INT, msg, null, null);
180: Narendra kumar, TRACE, IfStmt, writeLog(marker, LocationAwareLogger.TRACE_INT, format, new Object[] { arg }, null);
187: Narendra kumar, TRACE, IfStmt, writeLog(marker, LocationAwareLogger.TRACE_INT, format, new Object[] { arg1, arg2 }, null);
194: Narendra kumar, TRACE, IfStmt, writeLog(marker, LocationAwareLogger.TRACE_INT, format, argArray, null);
201: Narendra kumar, TRACE, IfStmt, writeLog(marker, LocationAwareLogger.TRACE_INT, msg, null, t);
208: Narendra kumar, DEBUG, IfStmt, writeLog(null, LocationAwareLogger.DEBUG_INT, message, null, null);
215: Narendra kumar, DEBUG, IfStmt, writeLog(null, LocationAwareLogger.DEBUG_INT, format, new Object[] { arg }, null);
222: Narendra kumar, DEBUG, IfStmt, writeLog(null, LocationAwareLogger.DEBUG_INT, format, new Object[] { arg1, arg2 }, null);
229: Narendra kumar, DEBUG, IfStmt, writeLog(null, LocationAwareLogger.DEBUG_INT, format, args, null);
236: Narendra kumar, DEBUG, IfStmt, writeLog(null, LocationAwareLogger.DEBUG_INT, msg, null, t);
243: Narendra kumar, DEBUG, IfStmt, writeLog(marker, LocationAwareLogger.DEBUG_INT, msg, null, null);
250: Narendra kumar, DEBUG, IfStmt, writeLog(marker, LocationAwareLogger.DEBUG_INT, format, new Object[] { arg }, null);
257: Narendra kumar, DEBUG, IfStmt, writeLog(marker, LocationAwareLogger.DEBUG_INT, format, new Object[] { arg1, arg2 }, null);
264: Narendra kumar, DEBUG, IfStmt, writeLog(marker, LocationAwareLogger.DEBUG_INT, format, arguments, null);
271: Narendra kumar, DEBUG, IfStmt, writeLog(marker, LocationAwareLogger.DEBUG_INT, msg, null, t);
277: Narendra kumar, WARN, MethodDeclaration, writeLog(null, LocationAwareLogger.WARN_INT, message, null, null);
282: Narendra kumar, WARN, MethodDeclaration, writeLog(null, LocationAwareLogger.WARN_INT, format, new Object[] { arg }, null);
287: Narendra kumar, WARN, MethodDeclaration, writeLog(null, LocationAwareLogger.WARN_INT, message, new Object[] { arg1, arg2 }, null);
292: Narendra kumar, WARN, MethodDeclaration, writeLog(null, LocationAwareLogger.WARN_INT, format, args, null);
297: Narendra kumar, WARN, MethodDeclaration, writeLog(null, LocationAwareLogger.WARN_INT, msg, null, t);
302: Narendra kumar, WARN, MethodDeclaration, writeLog(marker, LocationAwareLogger.WARN_INT, msg, null, null);
307: Narendra kumar, WARN, MethodDeclaration, writeLog(marker, LocationAwareLogger.WARN_INT, format, new Object[] { arg }, null);
312: Narendra kumar, WARN, MethodDeclaration, writeLog(marker, LocationAwareLogger.WARN_INT, format, new Object[] { arg1, arg2 }, null);
317: Narendra kumar, WARN, MethodDeclaration, writeLog(marker, LocationAwareLogger.WARN_INT, format, arguments, null);
322: Narendra kumar, WARN, MethodDeclaration, writeLog(marker, LocationAwareLogger.WARN_INT, msg, null, t);
327: Narendra kumar, ERROR, MethodDeclaration, writeLog(null, LocationAwareLogger.ERROR_INT, message, null, null);
332: Narendra kumar, ERROR, MethodDeclaration, writeLog(null, LocationAwareLogger.ERROR_INT, format, new Object[] { arg }, null);
337: Narendra kumar, ERROR, MethodDeclaration, writeLog(null, LocationAwareLogger.ERROR_INT, format, new Object[] { arg1, arg2 }, null);
342: Narendra kumar, ERROR, MethodDeclaration, writeLog(null, LocationAwareLogger.ERROR_INT, format, args, null);
347: Narendra kumar, ERROR, MethodDeclaration, writeLog(null, LocationAwareLogger.ERROR_INT, msg, null, t);
352: Narendra kumar, ERROR, MethodDeclaration, writeLog(marker, LocationAwareLogger.ERROR_INT, msg, null, null);
357: Narendra kumar, ERROR, MethodDeclaration, writeLog(marker, LocationAwareLogger.ERROR_INT, format, new Object[] { arg }, null);
362: Narendra kumar, ERROR, MethodDeclaration, writeLog(marker, LocationAwareLogger.ERROR_INT, format, new Object[] { arg1, arg2 }, null);
367: Narendra kumar, ERROR, MethodDeclaration, writeLog(marker, LocationAwareLogger.ERROR_INT, format, arguments, null);
372: Narendra kumar, ERROR, MethodDeclaration, writeLog(marker, LocationAwareLogger.ERROR_INT, msg, null, t);
377: Narendra kumar, INFO, MethodDeclaration, writeLog(null, LocationAwareLogger.INFO_INT, msg, null, null);
382: Narendra kumar, INFO, MethodDeclaration, writeLog(null, LocationAwareLogger.INFO_INT, format, new Object[] { arg }, null);
387: Narendra kumar, INFO, MethodDeclaration, writeLog(null, LocationAwareLogger.INFO_INT, format, new Object[] { arg1, arg2 }, null);
392: Narendra kumar, INFO, MethodDeclaration, writeLog(null, LocationAwareLogger.INFO_INT, format, args, null);
397: Narendra kumar, INFO, MethodDeclaration, writeLog(null, LocationAwareLogger.INFO_INT, msg, null, t);
402: Narendra kumar, INFO, MethodDeclaration, writeLog(marker, LocationAwareLogger.INFO_INT, msg, null, null);
407: Narendra kumar, INFO, MethodDeclaration, writeLog(marker, LocationAwareLogger.INFO_INT, format, new Object[] { arg }, null);
412: Narendra kumar, INFO, MethodDeclaration, writeLog(marker, LocationAwareLogger.INFO_INT, format, new Object[] { arg1, arg2 }, null);
417: Narendra kumar, INFO, MethodDeclaration, writeLog(marker, LocationAwareLogger.INFO_INT, format, arguments, null);
422: Narendra kumar, INFO, MethodDeclaration, writeLog(marker, LocationAwareLogger.INFO_INT, msg, null, t);
504: Narendra kumar, trace, IfStmt, logger.trace(addPrefix(message));
511: Narendra kumar, trace, IfStmt, logger.trace(addPrefix(message), arg);
518: Narendra kumar, trace, IfStmt, logger.trace(addPrefix(message), arg1, arg2);
525: Narendra kumar, trace, IfStmt, logger.trace(addPrefix(message), args);
532: Narendra kumar, trace, IfStmt, logger.trace(addPrefix(msg), t);
539: Narendra kumar, trace, IfStmt, logger.trace(marker, addPrefix(msg));
546: Narendra kumar, trace, IfStmt, logger.trace(marker, addPrefix(format), arg);
553: Narendra kumar, trace, IfStmt, logger.trace(marker, addPrefix(format), arg1, arg2);
560: Narendra kumar, trace, IfStmt, logger.trace(marker, addPrefix(format), argArray);
567: Narendra kumar, trace, IfStmt, logger.trace(marker, addPrefix(msg), t);
574: Narendra kumar, debug, IfStmt, logger.debug(addPrefix(message));
581: Narendra kumar, debug, IfStmt, logger.debug(addPrefix(message), arg);
588: Narendra kumar, debug, IfStmt, logger.debug(addPrefix(message), arg1, arg2);
595: Narendra kumar, debug, IfStmt, logger.debug(addPrefix(message), args);
602: Narendra kumar, debug, IfStmt, logger.debug(addPrefix(msg), t);
609: Narendra kumar, debug, IfStmt, logger.debug(marker, addPrefix(msg));
616: Narendra kumar, debug, IfStmt, logger.debug(marker, addPrefix(format), arg);
623: Narendra kumar, debug, IfStmt, logger.debug(marker, addPrefix(format), arg1, arg2);
630: Narendra kumar, debug, IfStmt, logger.debug(marker, addPrefix(format), arguments);
637: Narendra kumar, debug, IfStmt, logger.debug(marker, addPrefix(msg), t);
643: Narendra kumar, warn, MethodDeclaration, logger.warn(addPrefix(message));
648: Narendra kumar, warn, MethodDeclaration, logger.warn(addPrefix(message), arg);
653: Narendra kumar, warn, MethodDeclaration, logger.warn(addPrefix(message), arg1, arg2);
658: Narendra kumar, warn, MethodDeclaration, logger.warn(addPrefix(message), args);
663: Narendra kumar, warn, MethodDeclaration, logger.warn(addPrefix(msg), t);
668: Narendra kumar, warn, MethodDeclaration, logger.warn(marker, addPrefix(msg));
673: Narendra kumar, warn, MethodDeclaration, logger.warn(marker, addPrefix(format), arg);
678: Narendra kumar, warn, MethodDeclaration, logger.warn(marker, addPrefix(format), arg1, arg2);
683: Narendra kumar, warn, MethodDeclaration, logger.warn(marker, addPrefix(format), arguments);
688: Narendra kumar, warn, MethodDeclaration, logger.warn(marker, addPrefix(msg), t);
693: Narendra kumar, error, MethodDeclaration, logger.error(addPrefix(message));
698: Narendra kumar, error, MethodDeclaration, logger.error(addPrefix(message), arg);
703: Narendra kumar, error, MethodDeclaration, logger.error(addPrefix(message), arg1, arg2);
708: Narendra kumar, error, MethodDeclaration, logger.error(addPrefix(message), args);
713: Narendra kumar, error, MethodDeclaration, logger.error(addPrefix(msg), t);
718: Narendra kumar, error, MethodDeclaration, logger.error(marker, addPrefix(msg));
723: Narendra kumar, error, MethodDeclaration, logger.error(marker, addPrefix(format), arg);
728: Narendra kumar, error, MethodDeclaration, logger.error(marker, addPrefix(format), arg1, arg2);
733: Narendra kumar, error, MethodDeclaration, logger.error(marker, addPrefix(format), arguments);
738: Narendra kumar, error, MethodDeclaration, logger.error(marker, addPrefix(msg), t);
743: Narendra kumar, info, MethodDeclaration, logger.info(addPrefix(message));
748: Narendra kumar, info, MethodDeclaration, logger.info(addPrefix(message), arg);
753: Narendra kumar, info, MethodDeclaration, logger.info(addPrefix(message), arg1, arg2);
758: Narendra kumar, info, MethodDeclaration, logger.info(addPrefix(message), args);
763: Narendra kumar, info, MethodDeclaration, logger.info(addPrefix(msg), t);
768: Narendra kumar, info, MethodDeclaration, logger.info(marker, addPrefix(msg));
773: Narendra kumar, info, MethodDeclaration, logger.info(marker, addPrefix(format), arg);
778: Narendra kumar, info, MethodDeclaration, logger.info(marker, addPrefix(format), arg1, arg2);
783: Narendra kumar, info, MethodDeclaration, logger.info(marker, addPrefix(format), arguments);
788: Narendra kumar, info, MethodDeclaration, logger.info(marker, addPrefix(msg), t);
110: Sriharsha Chintalapani, Error, CatchClause, LOG.warn("Error reading the error stream", ioe);
142: Sriharsha Chintalapani, Error, CatchClause, LOG.warn("Error while closing the input stream", ioe);
150: Sriharsha Chintalapani, Error, CatchClause, LOG.warn("Error while closing the error stream", ioe);
124: Sriharsha Chintalapani, warn, CatchClause, LOG.warn("Interrupted while reading the error stream", ie);
763: radai-rosenblatt, debug, TryStmt, log.debug("Non-atomic move of {} to {} succeeded after atomic move failed due to {}", source, target, outer.getMessage());
803: Jason Gustafson, warn, CatchClause, log.warn("Failed to close {} with type {}", name, closeable.getClass().getName(), t);
479: Colin P. Mccabe, info, TryStmt, log.info("Verified the error result of AdminClient#listTopics");
926: Colin P. Mccabe, debug, IfStmt, log.debug("callHasExpired({}) = {}", call, ret);
922: Colin P. Mccabe, debug, IfStmt, log.debug("Injecting timeout for {}.", call);
105: Colin P. Mccabe, trace, TryStmt, log.trace("Invoking {} at {}", callable, now);
117: Colin P. Mccabe, trace, MethodDeclaration, log.trace("Scheduling {} for {} ms from now.", callable, delayMs);
186: Colin P. Mccabe, Error, CatchClause, log.error("Error deleting {}", file.getAbsolutePath(), e);
811: Randall Hauch, debug, CatchClause, LOG.debug("Unable to parse the value as a map; reverting to string", e);
68: Randall Hauch, warn, CatchClause, LOG.warn("Failed to deserialize value for header '{}' on topic '{}', so using byte array", headerKey, topic, t);
76: Ewen Cheslack-Postava, trace, ForeachStmt, log.trace("Writing line to {}: {}", logFilename(), record.value());
83: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Flushing output stream for {}", logFilename());
108: dan norwood, warn, CatchClause, log.warn("Couldn't find file {} for FileStreamSourceTask, sleeping to wait for it to be created", logFilename());
88: Ewen Cheslack-Postava, debug, IfStmt, log.debug("Found previous offset, trying to skip to file offset {}", lastRecordedOffset);
95: Ewen Cheslack-Postava, Error, CatchClause, log.error("Error while trying to seek to previous offset in file: ", e);
99: Ewen Cheslack-Postava, debug, IfStmt, log.debug("Skipped to offset {}", lastRecordedOffset);
106: Ewen Cheslack-Postava, debug, TryStmt, log.debug("Opened {} for reading", logFilename());
132: Ewen Cheslack-Postava, trace, WhileStmt, log.trace("Read {} bytes from {}", nread, logFilename());
146: Ewen Cheslack-Postava, trace, IfStmt, log.trace("Read a line from {}", logFilename());
205: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Stopping");
213: Ewen Cheslack-Postava, error, CatchClause, log.error("Failed to close FileStreamSourceTask stream: ", e);
210: Ewen Cheslack-Postava, trace, IfStmt, log.trace("Closed input stream");
57: Ewen Cheslack-Postava, info, IfStmt, log.info("Usage: ConnectDistributed worker.properties");
112: Randall Hauch, error, CatchClause, log.error("Stopping due to error", t);
63: Randall Hauch, info, TryStmt, log.info("Kafka Connect distributed worker initializing ...");
72: Randall Hauch, info, TryStmt, log.info("Scanning for plugin classes. This might take a moment ...");
78: Randall Hauch, debug, TryStmt, log.debug("Kafka cluster ID: {}", kafkaClusterId);
99: Randall Hauch, info, TryStmt, log.info("Kafka Connect distributed worker initialization took {}ms", time.hiResClockMs() - initStart);
103: Randall Hauch, error, CatchClause, log.error("Failed to start Connect", e);
61: Ewen Cheslack-Postava, info, IfStmt, log.info("Usage: ConnectStandalone worker.properties connector1.properties [connector2.properties ...]");
122: Randall Hauch, error, CatchClause, log.error("Stopping due to error", t);
67: Randall Hauch, info, TryStmt, log.info("Kafka Connect standalone worker initializing ...");
76: Randall Hauch, info, TryStmt, log.info("Scanning for plugin classes. This might take a moment ...");
82: Randall Hauch, debug, TryStmt, log.debug("Kafka cluster ID: {}", kafkaClusterId);
92: Randall Hauch, info, TryStmt, log.info("Kafka Connect standalone worker initialization took {}ms", time.hiResClockMs() - initStart);
113: Randall Hauch, error, CatchClause, log.error("Stopping after connector error", t);
104: Randall Hauch, info, BlockStmt, log.info("Created connector {}", info.result().name());
102: Randall Hauch, error, BlockStmt, log.error("Failed to create job for {}", connectorPropsFile);
41: Ewen Cheslack-Postava, debug, ConstructorDeclaration, log.debug("Kafka Connect instance created");
49: Jason Gustafson, info, TryStmt, log.info("Kafka Connect starting");
55: Jason Gustafson, info, TryStmt, log.info("Kafka Connect started");
65: Jason Gustafson, info, IfStmt, log.info("Kafka Connect stopping");
70: Jason Gustafson, info, IfStmt, log.info("Kafka Connect stopped");
81: Ewen Cheslack-Postava, error, CatchClause, log.error("Interrupted waiting for Kafka Connect to shutdown");
92: Ewen Cheslack-Postava, error, CatchClause, log.error("Interrupted in shutdown hook while waiting for Kafka Connect startup to finish");
78: Randall Hauch, debug, ConstructorDeclaration, LOG.debug("Registering Connect metrics with JMX for worker '{}'", workerId);
150: Randall Hauch, debug, MethodDeclaration, LOG.debug("Unregistering Connect metrics with JMX for worker '{}'", workerId);
70: Ewen Cheslack-Postava, error, IfStmt, log.error("Graceful shutdown of offset commitOffsets thread timed out.");
99: Konstantine Karantasis, trace, CatchClause, log.trace("Offset commit thread was cancelled by another thread while removing connector task with id: {}", id);
106: Ewen Cheslack-Postava, debug, MethodDeclaration, log.debug("{} Committing offsets", workerTask);
111: Ewen Cheslack-Postava, error, TryStmt, log.error("{} Failed to commit offsets", workerTask);
139: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Worker starting");
144: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Worker started");
151: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Worker stopping");
157: Shikhar Bhushan, warn, IfStmt, log.warn("Shutting down connectors {} uncleanly; herder should have shut down connectors before the Worker is stopped", connectors.keySet());
162: Shikhar Bhushan, warn, IfStmt, log.warn("Shutting down tasks {} uncleanly; herder should have shut down tasks before the Worker is stopped", tasks.keySet());
172: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Worker stopped");
211: Shikhar Bhushan, error, CatchClause, log.error("Failed to start connector {}", connName, t);
202: Shikhar Bhushan, info, TryStmt, log.info("Creating connector {} of type {}", connName, connClass);
205: Shikhar Bhushan, info, TryStmt, log.info("Instantiated connector {} with version {} of type {}", connName, connector.version(), connector.getClass());
224: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Finished creating connector {}", connName);
257: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Reconfiguring connector tasks for {}", connName);
305: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Stopping connector {}", connName);
309: Shikhar Bhushan, warn, IfStmt, log.warn("Ignoring stop request for unowned connector {}", connName);
321: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Stopped connector {}", connName);
362: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Creating task {}", id);
421: Shikhar Bhushan, error, CatchClause, log.error("Failed to start task {}", id, t);
377: Shikhar Bhushan, info, TryStmt, log.info("Instantiated task {} with version {} of type {}", id, task.version(), taskClass.getName());
402: Randall Hauch, info, IfStmt, log.info("Set up the key converter {} for task {} using the connector config", keyConverter.getClass(), id);
400: Randall Hauch, info, IfStmt, log.info("Set up the key converter {} for task {} using the worker config", keyConverter.getClass(), id);
408: Randall Hauch, info, IfStmt, log.info("Set up the value converter {} for task {} using the connector config", valueConverter.getClass(), id);
406: Randall Hauch, info, IfStmt, log.info("Set up the value converter {} for task {} using the worker config", valueConverter.getClass(), id);
414: Randall Hauch, info, IfStmt, log.info("Set up the header converter {} for task {} using the connector config", headerConverter.getClass(), id);
412: Randall Hauch, info, IfStmt, log.info("Set up the header converter {} for task {} using the worker config", headerConverter.getClass(), id);
466: Ewen Cheslack-Postava, error, IfStmt, log.error("Tasks must be a subclass of either SourceTask or SinkTask", task);
474: Konstantine Karantasis, warn, IfStmt, log.warn("Ignoring stop request for unowned task {}", taskId);
478: Konstantine Karantasis, info, MethodDeclaration, log.info("Stopping task {}", task.id());
502: Konstantine Karantasis, warn, IfStmt, log.warn("Ignoring await stop request for non-present task {}", taskId);
507: Konstantine Karantasis, error, IfStmt, log.error("Graceful stop of task {} failed.", task.id());
580: Jason Gustafson, info, MethodDeclaration, log.info("Setting connector {} state to {}", connName, state);
98: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Error initializing connector", this, t);
79: Ewen Cheslack-Postava, debug, TryStmt, log.debug("{} Initializing connector {} with config {}", this, connName, config);
92: Ewen Cheslack-Postava, error, MethodDeclaration, log.error("{} Connector raised an error", this, e);
119: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Error while starting connector", this, t);
163: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Error while shutting down connector", this, t);
176: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Error while shutting down connector", this, t);
186: Ewen Cheslack-Postava, warn, IfStmt, log.warn("{} Cannot transition connector to {} since it has failed", this, targetState);
190: Ewen Cheslack-Postava, debug, MethodDeclaration, log.debug("{} Transition connector to {}", this, targetState);
71: Konstantine Karantasis, info, MethodDeclaration, log.info(b.toString());
135: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Task failed initialization and will not be started.", this, t);
207: Randall Hauch, trace, CatchClause, log.trace("{} Consumer woken up", this);
198: Ewen Cheslack-Postava, warn, IfStmt, log.warn("{} Commit of offsets timed out", this);
245: Randall Hauch, debug, IfStmt, log.debug("{} Finished offset commit successfully in {} ms for sequence number {}: {}", this, durationMillis, seqno, committedOffsets);
248: Randall Hauch, debug, IfStmt, log.debug("{} Setting last committed offsets to {}", this, committedOffsets);
240: Randall Hauch, error, IfStmt, log.error("{} Commit of offsets threw an unexpected exception for sequence number {}: {}", this, seqno, committedOffsets, error);
234: Randall Hauch, debug, IfStmt, log.debug("{} Received out of order commit callback for sequence number {}, but most recent sequence number is {}", this, seqno, commitSeqno);
277: Jeff Klukas, debug, IfStmt, log.debug("{} Initializing and starting task for topics regex {}", this, topicsRegexStr);
272: Jeff Klukas, debug, IfStmt, log.debug("{} Initializing and starting task for topics {}", this, topics);
282: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("{} Sink task finished initialization and start", this);
296: Randall Hauch, trace, MethodDeclaration, log.trace("{} Polling consumer with timeout {} ms", this, timeoutMs);
299: Randall Hauch, trace, MethodDeclaration, log.trace("{} Polling returned {} messages", this, msgs.count());
311: Randall Hauch, info, MethodDeclaration, log.info("{} Committing offsets synchronously using sequence number {}: {}", this, seqno, offsets);
325: Randall Hauch, info, MethodDeclaration, log.info("{} Committing offsets asynchronously using sequence number {}: {}", this, seqno, offsets);
365: Shikhar Bhushan, error, IfStmt, log.error("{} Offset commit failed, rewinding to last committed offsets", this, t);
367: Ewen Cheslack-Postava, debug, ForeachStmt, log.debug("{} Rewinding topic partition {} to offset {}", this, entry.getKey(), entry.getValue().offset());
362: Ewen Cheslack-Postava, warn, IfStmt, log.warn("{} Offset commit failed during close", this);
376: Randall Hauch, trace, IfStmt, log.trace("{} Closing the task before committing the offsets: {}", this, currentOffsets);
358: Randall Hauch, trace, TryStmt, log.trace("{} Calling task.preCommit with current offsets: {}", this, currentOffsets);
382: Randall Hauch, debug, IfStmt, log.debug("{} Skipping offset commit, task opted-out by returning no offsets from preCommit", this);
401: Ewen Cheslack-Postava, warn, IfStmt, log.warn("{} Ignoring invalid task provided offset {}/{} -- partition not assigned, assignment={}", this, partition, taskProvidedOffset, consumer.assignment());
397: Ewen Cheslack-Postava, warn, IfStmt, log.warn("{} Ignoring invalid task provided offset {}/{} -- not yet consumed, taskOffset={} currentOffset={}", this, partition, taskProvidedOffset, taskOffset, currentOffset);
407: Shikhar Bhushan, debug, IfStmt, log.debug("{} Skipping offset commit, no change since last commit", this);
465: Randall Hauch, trace, ForeachStmt, log.trace("{} Consuming and converting message in topic '{}' partition {} at offset {} and timestamp {}", this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp());
478: Randall Hauch, trace, ForeachStmt, log.trace("{} Applying transformations to record in topic '{}' partition {} at offset {} and timestamp {} with key {} and value {}", this, msg.topic(), msg.partition(), msg.offset(), timestamp, keyAndSchema.value(), valueAndSchema.value());
488: Randall Hauch, trace, IfStmt, log.trace("{} Transformations returned null, so dropping record in topic '{}' partition {} at offset {} and timestamp {} with key {} and value {}", this, msg.topic(), msg.partition(), msg.offset(), timestamp, keyAndSchema.value(), valueAndSchema.value());
537: Ewen Cheslack-Postava, error, CatchClause, log.error("{} RetriableException from SinkTask:", this, e);
544: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Task threw an uncaught and unrecoverable exception. Task is being killed and will not " + "recover until manually restarted.", this, t);
522: Randall Hauch, trace, TryStmt, log.trace("{} Delivering batch of {} messages to task", this, messageBatch.size());
564: Randall Hauch, warn, IfStmt, log.warn("{} Cannot rewind {} to null offset", this, tp);
559: Randall Hauch, trace, IfStmt, log.trace("{} Rewind {} to offset {}", this, tp, offset);
604: Randall Hauch, debug, MethodDeclaration, log.debug("{} Partitions assigned {}", WorkerSinkTask.this, partitions);
611: Randall Hauch, debug, ForeachStmt, log.debug("{} Assigned topic partition {} with offset {}", this, tp, pos);
645: Randall Hauch, debug, MethodDeclaration, log.debug("{} Partitions revoked", WorkerSinkTask.this);
53: Randall Hauch, debug, MethodDeclaration, log.debug("{} Setting offsets for topic partitions {}", this, offsets);
59: Randall Hauch, debug, MethodDeclaration, log.debug("{} Setting offset for topic partition {} to {}", this, tp, offset);
77: Randall Hauch, debug, MethodDeclaration, log.debug("{} Setting timeout to {} ms", this, timeoutMs);
108: Randall Hauch, debug, IfStmt, log.debug("{} Pausing partitions {}. Connector is not paused.", this, partitions);
105: Randall Hauch, debug, IfStmt, log.debug("{} Connector is paused, so not pausing consumer's partitions {}", this, partitions);
126: Randall Hauch, debug, IfStmt, log.debug("{} Resuming partitions: {}", this, partitions);
123: Randall Hauch, debug, IfStmt, log.debug("{} Connector is paused, so not resuming consumer's partitions {}", this, partitions);
139: Randall Hauch, debug, MethodDeclaration, log.debug("{} Requesting commit", this);
133: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Task failed initialization and will not be started.", this, t);
166: Ewen Cheslack-Postava, info, TryStmt, log.info("{} Source task finished initialization and start", this);
185: oleg, trace, IfStmt, log.trace("{} Nothing to send to Kafka. Polling source for additional records", this);
194: Ewen Cheslack-Postava, debug, WhileStmt, log.debug("{} About to send " + toSend.size() + " records to Kafka", this);
232: Ewen Cheslack-Postava, trace, ForeachStmt, log.trace("{} Appending record with key {}, value {}", this, record.key(), record.value());
276: Ewen Cheslack-Postava, warn, CatchClause, log.warn("{} Failed to send {}, backing off before retrying:", this, producerRecord, e);
264: Ewen Cheslack-Postava, trace, IfStmt, log.trace("{} Wrote record successfully: topic {} partition {} offset {}", this, recordMetadata.topic(), recordMetadata.partition(), recordMetadata.offset());
262: Ewen Cheslack-Postava, debug, IfStmt, log.debug("{} Failed record: {}", this, preTransformRecord);
308: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Exception thrown while calling task.commitRecord()", this, t);
319: Ewen Cheslack-Postava, error, IfStmt, log.error("{} CRITICAL Saw callback for record that was not present in the outstanding message set: {}", this, record);
329: Stephane Maarek, info, MethodDeclaration, log.info("{} Committing offsets", this);
351: Ewen Cheslack-Postava, error, IfStmt, log.error("{} Failed to flush, timed out while waiting for producer to flush outstanding {} messages", this, outstandingMessages.size());
376: Ewen Cheslack-Postava, debug, IfStmt, log.debug("{} Finished offset commitOffsets successfully in {} ms", this, durationMillis);
391: Ewen Cheslack-Postava, trace, IfStmt, log.trace("{} Finished flushing offsets to storage", this);
389: Ewen Cheslack-Postava, error, IfStmt, log.error("{} Failed to flush offsets to storage: ", this, error);
409: Ewen Cheslack-Postava, warn, CatchClause, log.warn("{} Flush of offsets interrupted, cancelling", this);
414: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Flush of offsets threw an unexpected exception: ", this, e);
419: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Timed out waiting to flush offsets to storage", this);
428: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("{} Finished commitOffsets successfully in {} ms", this, durationMillis);
440: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Exception thrown while calling task.commit()", this, t);
151: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Task threw an uncaught and unrecoverable exception during shutdown", this, t);
172: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Task threw an uncaught and unrecoverable exception", this, t);
173: Ewen Cheslack-Postava, error, CatchClause, log.error("{} Task is being killed and will not recover until manually restarted", this);
226: Ewen Cheslack-Postava, error, CatchClause, log.error("Uncaught exception in herder work thread, exiting: ", t);
211: Ewen Cheslack-Postava, info, TryStmt, log.info("Herder starting");
215: Ewen Cheslack-Postava, info, TryStmt, log.info("Herder started");
223: Ewen Cheslack-Postava, info, TryStmt, log.info("Herder stopped");
339: Jason Gustafson, info, ForeachStmt, log.info("Handling connector-only config update by {} connector {}", remains ? "restarting" : "stopping", connectorName);
352: Jason Gustafson, debug, IfStmt, log.debug("Received target state change for unknown connector: {}", connector);
371: Konstantine Karantasis, info, SynchronizedStmt, log.info("Stopping connectors and tasks that are still assigned to this worker.");
397: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Herder stopping");
417: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Herder stopped");
422: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Submitting connector listing request");
441: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Submitting connector info request {}", connName);
473: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Submitting connector config read request {}", connName);
491: Jason Gustafson, trace, MethodDeclaration, log.trace("Handling connector config request {}", connName);
500: Jason Gustafson, trace, IfStmt, log.trace("Removing connector config {} {}", connName, configState.connectors());
531: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Submitting connector config write request {}", connName);
540: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Handling connector config request {}", connName);
552: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Submitting connector config {} {} {}", connName, allowReplace, configState.connectors());
570: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Submitting connector task reconfiguration request {}", connName);
584: Gwen Shapira, error, IfStmt, log.error("Unexpected error during task reconfiguration: ", error);
585: Gwen Shapira, error, IfStmt, log.error("Task reconfiguration for {} failed unexpectedly, this connector will not be properly reconfigured unless manually triggered.", connName);
594: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Submitting get task configuration request {}", connName);
622: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Submitting put task configuration request {}", connName);
769: Ewen Cheslack-Postava, warn, IfStmt, log.warn("Catching up to assignment's config offset.");
765: Ewen Cheslack-Postava, warn, IfStmt, log.warn("Join group completed, but assignment failed. We were up to date, so just retrying.");
762: Ewen Cheslack-Postava, warn, IfStmt, log.warn("Join group completed, but assignment failed and we lagging. Reading to end of config and retrying.");
759: Ewen Cheslack-Postava, warn, IfStmt, log.warn("Join group completed, but assignment failed and we are the leader. Reading to end of config and retrying.");
793: Ewen Cheslack-Postava, info, IfStmt, log.info("Current config state offset {} does not match group assignment {}. Forcing rebalance.", configState.offset(), assignment.offset());
814: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Current config state offset {} is behind group assignment {}, reading to end of config log", configState.offset(), assignment.offset());
818: Ewen Cheslack-Postava, info, TryStmt, log.info("Finished reading to end of log and updated config snapshot, new config log offset: {}", configState.offset());
844: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Starting connectors and tasks using config offset {}", assignment.offset());
854: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Finished starting connectors and tasks");
858: Jason Gustafson, info, MethodDeclaration, log.info("Starting task {}", taskId);
875: Konstantine Karantasis, error, CatchClause, log.error("Couldn't instantiate task {} because it has an invalid task configuration. This task will not execute until reconfigured.", taskId, t);
897: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Starting connector {}", connectorName);
919: Konstantine Karantasis, error, CatchClause, log.error("Couldn't instantiate connector " + connectorName + " because it has an invalid connector " + "configuration. This connector will not execute until reconfigured.", t);
935: Konstantine Karantasis, error, CatchClause, log.error("Failed to shut down connector " + connectorName, t);
951: Ewen Cheslack-Postava, error, IfStmt, log.error("Failed to reconfigure connector's tasks, retrying after backoff:", error);
962: Ewen Cheslack-Postava, error, MethodDeclaration, log.error("Unexpected error during connector task reconfiguration: ", error);
963: Ewen Cheslack-Postava, error, MethodDeclaration, log.error("Task reconfiguration for {} failed unexpectedly, this connector will not be properly reconfigured unless manually triggered.", connName);
977: Jason Gustafson, info, IfStmt, log.info("Skipping reconfiguration of connector {} since it is not running", connName);
1000: Ewen Cheslack-Postava, debug, IfStmt, log.debug("Change in task configurations, writing updated task configurations");
994: Ewen Cheslack-Postava, debug, IfStmt, log.debug("Change in connector task count from {} to {}, writing updated task configurations", currentNumTasks, taskProps.size());
1022: Ewen Cheslack-Postava, error, CatchClause, log.error("Request to leader to reconfigure connector tasks failed", e);
1069: Jason Gustafson, info, MethodDeclaration, log.info("Connector {} config removed", connector);
1082: Jason Gustafson, info, MethodDeclaration, log.info("Connector {} config updated", connector);
1097: Jason Gustafson, info, MethodDeclaration, log.info("Tasks {} configs updated", tasks);
1111: Jason Gustafson, info, MethodDeclaration, log.info("Connector {} target state change", connector);
1177: Jason Gustafson, debug, IfStmt, log.debug("Cleaning status information for connector {}", connector);
1218: Jason Gustafson, info, MethodDeclaration, log.info("Rebalance started");
1250: Jason Gustafson, info, IfStmt, log.info("Wasn't unable to resume work after last rebalance, can skip stopping connectors and tasks");
1248: Jason Gustafson, info, IfStmt, log.info("Finished stopping tasks in preparation for rebalance");
159: Ewen Cheslack-Postava, debug, MethodDeclaration, log.debug("Performing task assignment");
187: Ewen Cheslack-Postava, debug, MethodDeclaration, log.debug("Max config offset root: {}, local snapshot config offsets root: {}", maxOffset, configSnapshot.offset());
200: Ewen Cheslack-Postava, info, IfStmt, log.info("Was selected to perform assignments, but do not have latest config found in sync request. " + "Returning an empty configuration to trigger re-sync.");
225: Ewen Cheslack-Postava, trace, ForeachStmt, log.trace("Assigning connector {} to {}", connectorId, connectorAssignedTo);
236: Ewen Cheslack-Postava, trace, ForeachStmt, log.trace("Assigning task {} to {}", taskId, taskAssignedTo);
269: Ewen Cheslack-Postava, debug, ForeachStmt, log.debug("Assignment: {} -> {}", member, assignment);
272: Ewen Cheslack-Postava, debug, MethodDeclaration, log.debug("Finished assignment");
279: Ewen Cheslack-Postava, debug, MethodDeclaration, log.debug("Revoking previous assignment {}", assignmentSnapshot);
138: Ewen Cheslack-Postava, debug, TryStmt, log.debug("Connect group member created");
198: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Stopping the Connect group member.");
208: Ewen Cheslack-Postava, debug, BlockStmt, log.debug("The Connect group member has stopped.");
107: Konstantine Karantasis, debug, MethodDeclaration, log.debug("Getting plugin class loader for connector: '{}'", connectorClassOrAlias);
113: Konstantine Karantasis, error, IfStmt, log.error("Plugin class loader for connector: '{}' was not found. Returning: {}", connectorClassOrAlias, this);
146: Konstantine Karantasis, info, IfStmt, log.info("Added plugin '{}'", pluginClassName);
184: Konstantine Karantasis, error, CatchClause, log.error("Invalid path in plugin path: {}. Ignoring.", path, e);
186: Konstantine Karantasis, error, CatchClause, log.error("Could not get listing for plugin path: {}. Ignoring.", path, e);
188: Konstantine Karantasis, error, CatchClause, log.error("Could not instantiate plugins in: {}. Ignoring: {}", path, e);
194: Konstantine Karantasis, info, MethodDeclaration, log.info("Loading plugin from: {}", pluginLocation);
201: Konstantine Karantasis, debug, IfStmt, log.debug("Loading plugin urls: {}", Arrays.toString(urls));
217: Konstantine Karantasis, info, MethodDeclaration, log.info("Registered loader: {}", loader);
257: Konstantine Karantasis, debug, CatchClause, log.debug("Ignoring java.sql.Driver classes listed in resources but not" + " present in class loader's classpath: ", t);
251: Konstantine Karantasis, debug, WhileStmt, log.debug("Registered java.sql.Driver: {} to java.sql.DriverManager", driver);
326: Konstantine Karantasis, trace, IfStmt, log.trace("Retrieving loaded class '{}' from '{}'", fullName, pluginLoader);
352: Konstantine Karantasis, info, IfStmt, log.info("Added aliases '{}' and '{}' to plugin '{}'", simple, pruned, plugin.className());
349: Konstantine Karantasis, info, IfStmt, log.info("Added alias '{}' to plugin '{}'", simple, plugin.className());
378: Robert Yokota, warn, IfStmt, log.warn("could not create Vfs.Dir from url. ignoring the exception and continuing", e);
100: Konstantine Karantasis, trace, CatchClause, log.trace("Class '{}' not found. Delegating to parent", name);
237: Randall Hauch, debug, MethodDeclaration, log.debug("Configuring the {} converter with configuration:{}{}", isKeyConverter ? "key" : "value", System.lineSeparator(), converterConfig);
294: Randall Hauch, debug, MethodDeclaration, log.debug("Configuring the header converter with configuration:{}{}", System.lineSeparator(), converterConfig);
287: Konstantine Karantasis, warn, IfStmt, log.warn("Plugin path contains both java archives and class files. Returning only the" + " archives");
73: Jakub Scholz, error, CatchClause, log.error("Failed to start RestClient: ", e);
107: Jakub Scholz, error, CatchClause, log.error("IO error forwarding REST request: ", e);
114: Jakub Scholz, error, CatchClause, log.error("Failed to stop HTTP client", e);
79: Jakub Scholz, trace, TryStmt, log.trace("Sending {} with input {} to {}", method, serializedBody, url);
92: Konstantine Karantasis, debug, TryStmt, log.debug("Request's response code: {}", responseCode);
111: Jakub Scholz, info, IfStmt, log.info("Added connector for " + listener);
155: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Starting REST server");
207: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("REST server listening at " + jettyServer.getURI() + ", advertising URL " + advertisedUrl());
211: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Stopping REST server");
222: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("REST server stopped");
248: Jakub Scholz, info, MethodDeclaration, log.info("Advertised URI: {}", builder.build());
39: Ewen Cheslack-Postava, debug, MethodDeclaration, log.debug("Uncaught exception in REST call to /{}", uriInfo.getPath(), exception);
61: Ewen Cheslack-Postava, error, IfStmt, log.error("Uncaught exception in REST call to /{}", uriInfo.getPath(), exception);
263: Jason Gustafson, debug, IfStmt, log.debug("Forwarding request {} {} {}", forwardUrl, method, body);
72: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Herder starting");
74: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Herder started");
79: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Herder stopping");
89: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Herder stopped");
196: Ewen Cheslack-Postava, error, IfStmt, log.error("Task that requested reconfiguration does not exist: {}", connName);
287: Jason Gustafson, info, IfStmt, log.info("Skipping reconfiguration of connector {} since it is not running", connName);
59: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Starting FileOffsetBackingStore with file {}", file);
67: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Stopped FileOffsetBackingStore");
244: Jason Gustafson, info, MethodDeclaration, log.info("Starting KafkaConfigBackingStore");
249: Jason Gustafson, info, MethodDeclaration, log.info("Started KafkaConfigBackingStore");
254: Jason Gustafson, info, MethodDeclaration, log.info("Closing KafkaConfigBackingStore");
256: Jason Gustafson, info, MethodDeclaration, log.info("Closed KafkaConfigBackingStore");
294: Jason Gustafson, debug, MethodDeclaration, log.debug("Writing connector configuration {} for connector {} configuration", properties, connector);
307: Jason Gustafson, debug, MethodDeclaration, log.debug("Removing connector configuration for connector {}", connector);
313: Jason Gustafson, error, CatchClause, log.error("Failed to remove connector configuration from Kafka: ", e);
328: Ewen Cheslack-Postava, error, CatchClause, log.error("Failed to write connector configuration to Kafka: ", e);
349: Ewen Cheslack-Postava, error, CatchClause, log.error("Failed to write root configuration to Kafka: ", e);
361: Liquan Pei, debug, ForeachStmt, log.debug("Writing configuration for task " + index + " configuration: " + taskConfig);
384: Ewen Cheslack-Postava, error, CatchClause, log.error("Failed to write root configuration to Kafka: ", e);
378: Liquan Pei, debug, TryStmt, log.debug("Writing commit for connector " + connector + " with " + taskCount + " tasks.");
403: Jason Gustafson, debug, MethodDeclaration, log.debug("Writing target state {} for connector {}", state, connector);
435: Randall Hauch, debug, MethodDeclaration, log.debug("Creating admin client to manage Connect internal config topic");
449: Jason Gustafson, error, IfStmt, log.error("Unexpected in consumer callback for KafkaConfigBackingStore: ", error);
457: Ewen Cheslack-Postava, error, CatchClause, log.error("Failed to convert config data to Kafka Connect format: ", e);
630: Ewen Cheslack-Postava, error, IfStmt, log.error("Discarding config update record with invalid key: " + record.key());
594: Gwen Shapira, error, IfStmt, log.error("Ignoring connector tasks configuration commit for connector " + connectorName + " because it is in the wrong format: " + value.value());
546: Ewen Cheslack-Postava, error, IfStmt, log.error("Ignoring task configuration because " + record.key() + " couldn't be parsed as a task config key");
550: Gwen Shapira, error, IfStmt, log.error("Ignoring task configuration for task " + taskId + " because it is in the wrong format: " + value.value());
556: Gwen Shapira, error, IfStmt, log.error("Invalid data for task config (" + taskId + "): properties filed should be a Map but is " + newTaskConfig.getClass());
565: Gwen Shapira, debug, SynchronizedStmt, log.debug("Storing new config for task " + taskId + " this will wait for a commit message before the new config will take effect. New config: " + newTaskConfig);
518: Ewen Cheslack-Postava, error, IfStmt, log.error("Found connector configuration (" + record.key() + ") in wrong format: " + value.value().getClass());
523: Jason Gustafson, error, IfStmt, log.error("Invalid data for connector config ({}): properties field should be a Map but is {}", connectorName, newConnectorConfig == null ? null : newConnectorConfig.getClass());
527: Jason Gustafson, debug, IfStmt, log.debug("Updating configuration for connector " + connectorName + " configuration: " + newConnectorConfig);
512: Gwen Shapira, info, IfStmt, log.info("Removed connector " + connectorName + " due to null configuration. This is usually intentional and does not indicate an issue.");
480: Jason Gustafson, error, IfStmt, log.error("Found target state ({}) in wrong format: {}", record.key(), value.value().getClass());
485: Jason Gustafson, error, IfStmt, log.error("Invalid data for target state for connector ({}): 'state' field should be a Map but is {}", connectorName, targetState == null ? null : targetState.getClass());
495: Jason Gustafson, error, CatchClause, log.error("Invalid target state for connector ({}): {}", connectorName, targetState);
492: Jason Gustafson, debug, TryStmt, log.debug("Setting target state for connector {} to {}", connectorName, targetState);
470: Jason Gustafson, debug, IfStmt, log.debug("Removed target state for connector {} due to null value in topic.", connectorName);
97: Randall Hauch, debug, MethodDeclaration, log.debug("Creating admin client to manage Connect internal offset topic");
108: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Starting KafkaOffsetBackingStore");
110: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Finished reading offsets topic and starting KafkaOffsetBackingStore");
115: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Stopping KafkaOffsetBackingStore");
117: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Stopped KafkaOffsetBackingStore");
160: Randall Hauch, debug, MethodDeclaration, log.debug("Creating admin client to manage Connect internal status topic");
248: Benedict Jin, error, IfStmt, log.error("Failed to write status update", exception);
334: Jason Gustafson, error, CatchClause, log.error("Failed to deserialize connector status", e);
322: Jason Gustafson, error, IfStmt, log.error("Invalid connector status type {}", schemaAndValue.value().getClass());
354: Jason Gustafson, error, CatchClause, log.error("Failed to deserialize task status", e);
343: Jason Gustafson, error, IfStmt, log.error("Invalid connector status type {}", schemaAndValue.value().getClass());
382: Jason Gustafson, warn, CatchClause, log.warn("Invalid task status key {}", key);
390: Jason Gustafson, warn, IfStmt, log.warn("Discarding record with invalid connector status key {}", key);
395: Jason Gustafson, trace, IfStmt, log.trace("Removing status for connector {}", connector);
405: Jason Gustafson, trace, SynchronizedStmt, log.trace("Received connector {} status update {}", connector, status);
414: Jason Gustafson, warn, IfStmt, log.warn("Discarding record with invalid task status key {}", key);
419: Jason Gustafson, trace, IfStmt, log.trace("Removing task status for {}", id);
426: Jason Gustafson, warn, IfStmt, log.warn("Failed to parse task status with key {}", key);
431: Jason Gustafson, trace, SynchronizedStmt, log.trace("Received task {} status update {}", id, status);
445: Jason Gustafson, warn, IfStmt, log.warn("Discarding record with invalid key {}", key);
70: Ewen Cheslack-Postava, error, CatchClause, log.error("CRITICAL: Failed to serialize partition key when getting offsets for task with " + "namespace {}. No value for this data will be returned, which may break the " + "task or cause it to skip some data.", namespace, t);
81: Ewen Cheslack-Postava, error, CatchClause, log.error("Failed to fetch offsets from namespace {}: ", namespace, e);
102: Ewen Cheslack-Postava, error, CatchClause, log.error("CRITICAL: Failed to deserialize offset data when getting offsets for task with" + " namespace {}. No value for this data will be returned, which may break the " + "task or cause it to skip some data. This could either be due to an error in " + "the connector implementation or incompatible schema.", namespace, t);
91: Ewen Cheslack-Postava, error, IfStmt, log.error("Should be able to map {} back to a requested partition-offset key, backing " + "store may have returned invalid data", rawEntry.getKey());
108: Ewen Cheslack-Postava, error, IfStmt, log.error("Invalid call to OffsetStorageWriter flush() while already flushing, the " + "framework should not allow this");
159: oleg, error, CatchClause, log.error("Cause of serialization failure:", t);
165: oleg, debug, SynchronizedStmt, log.debug("Submitting {} entries to backing store. The offsets are: {}", offsetsSerialized.size(), toFlush);
76: Ewen Cheslack-Postava, debug, IfStmt, log.debug("Started MockConnector with failure delay of {} ms", delayMs);
81: Ewen Cheslack-Postava, debug, MethodDeclaration, log.debug("Triggering connector failure");
95: Ewen Cheslack-Postava, debug, MethodDeclaration, log.debug("Creating single task for MockConnector");
54: Ewen Cheslack-Postava, debug, IfStmt, log.debug("Started MockSinkTask at {} with failure scheduled in {} ms", startTimeMs, failureDelayMs);
64: Ewen Cheslack-Postava, debug, IfStmt, log.debug("Triggering sink task failure");
53: Ewen Cheslack-Postava, debug, IfStmt, log.debug("Started MockSourceTask at {} with failure scheduled in {} ms", startTimeMs, failureDelayMs);
62: Ewen Cheslack-Postava, debug, IfStmt, log.debug("Triggering source task failure");
114: Liquan Pei, info, MethodDeclaration, log.info("Started SchemaSourceTask {}-{} producing to topic {} resuming from seqno {}", name, id, topic, startingSeqno);
90: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Started VerifiableSourceTask {}-{} producing to topic {} resuming from seqno {}", name, id, topic, startingSeqno);
43: Randall Hauch, info, MethodDeclaration, log.info("Creating Kafka admin client");
50: Ewen Cheslack-Postava, debug, MethodDeclaration, log.debug("Looking up Kafka cluster ID");
54: Ewen Cheslack-Postava, info, IfStmt, log.info("Kafka cluster version is too old to return cluster ID");
57: Randall Hauch, debug, TryStmt, log.debug("Fetching Kafka cluster ID");
59: Ewen Cheslack-Postava, info, TryStmt, log.info("Kafka cluster ID: {}", kafkaClusterId);
124: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Starting KafkaBasedLog with topic " + topic);
153: Jason Gustafson, info, MethodDeclaration, log.info("Finished reading KafkaBasedLog for topic " + topic);
155: Jason Gustafson, info, MethodDeclaration, log.info("Started KafkaBasedLog for topic " + topic);
159: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Stopping KafkaBasedLog for topic " + topic);
176: Ewen Cheslack-Postava, error, CatchClause, log.error("Failed to stop KafkaBasedLog producer", e);
182: Ewen Cheslack-Postava, error, CatchClause, log.error("Failed to stop KafkaBasedLog consumer", e);
185: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Stopped KafkaBasedLog for topic " + topic);
202: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Starting read to end log for topic {}", topic);
263: Ewen Cheslack-Postava, Error, CatchClause, log.error("Error polling: " + e);
268: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Reading to end of offset log");
272: Balint Molnar, trace, MethodDeclaration, log.trace("Reading to end of log offsets {}", endOffsets);
334: Konstantine Karantasis, error, CatchClause, log.error("Unexpected exception in {}", this, t);
297: Konstantine Karantasis, trace, TryStmt, log.trace("{} started execution", this);
309: Ewen Cheslack-Postava, trace, TryStmt, log.trace("Finished read to end log for topic {}", topic);
83: Ewen Cheslack-Postava, error, CatchClause, log.error("Thread {} exiting with uncaught exception: ", getName(), e);
118: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Starting graceful shutdown of thread {}", getName());
140: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Forcing shutdown of thread {}", getName());
234: Randall Hauch, debug, IfStmt, log.debug("Found existing topic '{}' on the brokers at {}", topic, bootstrapServers);
238: Gavrie Philipson, debug, IfStmt, log.debug("Unable to create topic(s) '{}' since the brokers at {} do not support the CreateTopics API.", " Falling back to assume topic(s) exist or will be auto-created by the broker.", topicNameList, bootstrapServers);
244: Gavrie Philipson, debug, IfStmt, log.debug("Not authorized to create topic(s) '{}'." + " Falling back to assume topic(s) exist or will be auto-created by the broker.", topicNameList, bootstrapServers);
229: Randall Hauch, info, TryStmt, log.info("Created topic {} on brokers at {}", topicsByName.get(topic), bootstrapServers);
114: Konstantine Karantasis, error, MethodDeclaration, mockLog.error(EasyMock.anyString());
164: Ismael Juma, trace, MethodDeclaration, mockLog.trace(EasyMock.anyString(), EasyMock.<Object>anyObject());
238: Ashish Singh, debug, MethodDeclaration, LogLog.debug("Kafka producer connected to " + brokerList);
239: Ashish Singh, debug, MethodDeclaration, LogLog.debug("Logging for topic: " + topic);
249: Ashish Singh, debug, MethodDeclaration, LogLog.debug("[" + new Date(event.getTimeStamp()) + "]" + message);
73: Ashish Singh, error, ForStmt, logger.error(getMessage(i));
234: umesh chaudhary, debug, IfStmt, log.debug("Cannot transit to {} within {}ms", targetState, waitMs);
262: umesh chaudhary, info, IfStmt, log.info("State transition from {} to {}", oldState, newState);
422: umesh chaudhary, warn, IfStmt, log.warn("All stream threads have died. The instance will be in error state and should be closed.");
469: umesh chaudhary, warn, IfStmt, log.warn("Global thread has died. The instance will be in error state and should be closed.");
709: Guozhang Wang, warn, IfStmt, log.warn("Negative cache size passed in. Reverting to cache size of 0 bytes.");
806: umesh chaudhary, debug, MethodDeclaration, log.debug("Starting Streams client");
829: umesh chaudhary, info, IfStmt, log.info("Started Streams client");
858: umesh chaudhary, debug, MethodDeclaration, log.debug("Stopping Streams client with timeoutMillis = {} ms.", timeUnit.toMillis(timeout));
919: umesh chaudhary, info, IfStmt, log.info("Streams client cannot stop completely within the timeout");
916: umesh chaudhary, info, IfStmt, log.info("Streams client stopped completely");
761: ppatierno, debug, IfStmt, log.debug("Using {} default value of {} as exactly once is enabled.", COMMIT_INTERVAL_MS_CONFIG, EOS_DEFAULT_COMMIT_INTERVAL_MS);
814: Mariam John, warn, IfStmt, log.warn(String.format(nonConfigurableConfigMessage, "producer", config, eosMessage, clientProvidedProps.get(config), PRODUCER_EOS_OVERRIDES.get(config)));
808: Mariam John, warn, IfStmt, log.warn(String.format(nonConfigurableConfigMessage, "consumer", config, eosMessage, clientProvidedProps.get(config), CONSUMER_EOS_OVERRIDES.get(config)));
802: Mariam John, warn, IfStmt, log.warn(String.format(nonConfigurableConfigMessage, "consumer", config, "", clientProvidedProps.get(config), CONSUMER_DEFAULT_OVERRIDES.get(config)));
39: Eno Thereska, warn, MethodDeclaration, log.warn("Exception caught during Deserialization, " + "taskId: {}, topic: {}, partition: {}, offset: {}", context.taskId(), record.topic(), record.partition(), record.offset(), exception);
39: Matthias J. Sax, error, MethodDeclaration, log.error("Exception caught during Deserialization, " + "taskId: {}, topic: {}, partition: {}, offset: {}", context.taskId(), record.topic(), record.partition(), record.offset(), exception);
74: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key or value. key=[{}] value=[{}] topic=[{}] partition=[{}] offset=[{}]", key, value, context().topic(), context().partition(), context().offset());
78: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key or value. key=[{}] value=[{}] topic=[{}] partition=[{}] offset=[{}]", key, value, context().topic(), context().partition(), context().offset());
64: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key or value. key=[{}] value=[{}] topic=[{}] partition=[{}] offset=[{}]", key, value, context().topic(), context().partition(), context().offset());
72: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key or value. key=[{}] value=[{}] topic=[{}] partition=[{}] offset=[{}]", key, value, context().topic(), context().partition(), context().offset());
91: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]", value, context().topic(), context().partition(), context().offset());
85: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]", value, context().topic(), context().partition(), context().offset());
80: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]", value, context().topic(), context().partition(), context().offset());
86: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key. change=[{}] topic=[{}] partition=[{}] offset=[{}]", change, context().topic(), context().partition(), context().offset());
79: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key. change=[{}] topic=[{}] partition=[{}] offset=[{}]", change, context().topic(), context().partition(), context().offset());
78: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key. change=[{}] topic=[{}] partition=[{}] offset=[{}]", change, context().topic(), context().partition(), context().offset());
78: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key. change=[{}] topic=[{}] partition=[{}] offset=[{}]", change, context().topic(), context().partition(), context().offset());
67: John Roesler, warn, IfStmt, LOG.warn("Skipping record due to null key. topic=[{}] partition=[{}] offset=[{}]", context().topic(), context().partition(), context().offset());
59: Bill Bejeck, warn, IfStmt, LOG.warn("Updating node {} with predecessor name {}", node, previousNode.processorNodeName());
63: Bill Bejeck, debug, MethodDeclaration, LOG.debug("Adding node {}", node);
86: Bill Bejeck, warn, IfStmt, log.warn("Skipping creating tasks for the topic group {} since topic {}'s metadata is not available yet;" + " no tasks for this topic group will be assigned to any client.\n" + " Make sure all supplied topics in the topology are created before starting" + " as this could lead to unexpected results", topics, topic);
72: Matthias J. Sax, error, MethodDeclaration, log.error(message);
66: Matthias J. Sax, warn, MethodDeclaration, log.warn("Input record {} will be dropped because it has an invalid (negative) timestamp.", record);
868: Matthias J. Sax, Info, MethodDeclaration, final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroupsWithNewTopicsInfo = internalTopologyBuilder.topicGroups();
872: Matthias J. Sax, Info, ForeachStmt, final InternalTopologyBuilder.TopicsInfo newTopicsInfo = entry.getValue();
68: Guozhang Wang, error, CatchClause, log.error("Failed to write offset checkpoint file to {} while re-initializing {}: {}", checkpoint, stateStores, fatalException);
89: Matthias J. Sax, error, CatchClause, log.error("Failed to reinitialize store {}.", storeName, fatalException);
96: Matthias J. Sax, error, CatchClause, log.error("Failed to reinitialize store {}.", storeName, fatalException);
182: umesh chaudhary, trace, IfStmt, log.trace("Updating store offset limits {} for changelog {}", offset, partition);
220: umesh chaudhary, trace, MethodDeclaration, log.trace("Initializing state stores");
226: umesh chaudhary, trace, ForeachStmt, log.trace("Initializing store {}", store.name());
243: umesh chaudhary, trace, MethodDeclaration, log.trace("Closing state manager");
51: Matthias J. Sax, debug, IfStmt, log.debug("Committed active task {} per user request in", task.id());
98: Guozhang Wang, info, CatchClause, log.info("Failed to process stream task {} since it got migrated to another thread already. " + "Closing it as zombie before triggering a new rebalance.", task.id());
107: Matthias J. Sax, error, CatchClause, log.error("Failed to process stream task {} due to the following error:", task.id(), e);
130: Guozhang Wang, info, CatchClause, log.info("Failed to punctuate stream task {} since it got migrated to another thread already. " + "Closing it as zombie before triggering a new rebalance.", task.id());
139: Matthias J. Sax, error, CatchClause, log.error("Failed to punctuate stream task {} due to the following error:", task.id(), e);
83: Damian Guy, debug, IfStmt, log.debug("Initializing {}s {}", taskTypeName, created.keySet());
97: Guozhang Wang, trace, CatchClause, log.trace("Could not create {} {} due to {}; will retry", taskTypeName, entry.getKey(), e.toString());
89: Guozhang Wang, debug, IfStmt, log.debug("Transitioning {} {} to restoring", taskTypeName, entry.getKey());
106: Guozhang Wang, trace, MethodDeclaration, log.trace("{} changelog partitions that have completed restoring so far: {}", taskTypeName, restored);
122: Guozhang Wang, trace, IfStmt, log.trace("{} {} cannot resume processing yet since some of its changelog partitions have not completed restoring: {}", taskTypeName, task.id(), outstandingPartitions);
114: Guozhang Wang, trace, IfStmt, log.trace("{} {} completed restoration as all its changelog partitions {} have been applied to restore state", taskTypeName, task.id(), task.changelogPartitions());
146: umesh chaudhary, trace, MethodDeclaration, log.trace("Suspending running {} {}", taskTypeName, runningTaskIds());
148: umesh chaudhary, trace, MethodDeclaration, log.trace("Close restoring {} {}", taskTypeName, restoring.keySet());
150: umesh chaudhary, trace, MethodDeclaration, log.trace("Close created {} {}", taskTypeName, created.keySet());
168: umesh chaudhary, error, CatchClause, log.error("Failed to close {}, {}", taskTypeName, task.id(), e);
186: Guozhang Wang, info, CatchClause, log.info("Failed to suspend {} {} since it got migrated to another thread already. " + "Closing it as zombie and move on.", taskTypeName, task.id());
191: umesh chaudhary, error, CatchClause, log.error("Suspending {} {} failed due to the following error:", taskTypeName, task.id(), e);
196: umesh chaudhary, error, CatchClause, log.error("After suspending failed, closing the same {} {} failed again due to the following error:", taskTypeName, task.id(), f);
207: Guozhang Wang, warn, CatchClause, log.warn("Failed to close zombie {} {} due to {}; ignore and proceed.", taskTypeName, task.id(), e.toString());
223: umesh chaudhary, trace, IfStmt, log.trace("found suspended {} {}", taskTypeName, taskId);
244: Damian Guy, warn, IfStmt, log.warn("couldn't resume task {} assigned partitions {}, task partitions {}", taskId, partitions, task.partitions());
241: umesh chaudhary, trace, IfStmt, log.trace("resuming suspended {} {}", taskTypeName, task.id());
264: umesh chaudhary, debug, MethodDeclaration, log.debug("transitioning {} {} to running", taskTypeName, task.id());
364: Guozhang Wang, info, CatchClause, log.info("Failed to commit {} {} since it got migrated to another thread already. " + "Closing it as zombie before triggering a new rebalance.", taskTypeName, task.id());
373: umesh chaudhary, error, CatchClause, log.error("Failed to {} {} {} due to the following error:", action.name(), taskTypeName, task.id(), t);
394: umesh chaudhary, debug, IfStmt, log.debug("Closing suspended and not re-assigned {} {}", taskTypeName, suspendedTask.id());
398: umesh chaudhary, error, CatchClause, log.error("Failed to remove suspended {} {} due to the following error:", taskTypeName, suspendedTask.id(), e);
412: Guozhang Wang, info, CatchClause, log.info("Failed to close {} {} since it got migrated to another thread already. " + "Closing it as zombie and move on.", taskTypeName, task.id());
416: umesh chaudhary, error, CatchClause, log.error("Failed while closing {} {} due to the following error:", task.getClass().getSimpleName(), task.id(), t);
439: Matthias J. Sax, info, MethodDeclaration, log.info("Try to close {} {} unclean.", task.getClass().getSimpleName(), task.id());
443: Matthias J. Sax, error, CatchClause, log.error("Failed while closing {} {} due to the following error:", task.getClass().getSimpleName(), task.id(), fatalException);
113: ppatierno, error, CatchClause, log.error("Failed to unlock the global state directory", e);
169: ppatierno, info, MethodDeclaration, log.info("Restoring state for global store {}", store.name());
179: Matthias J. Sax, error, IfStmt, log.error("Failed to get end offsets for topic partitions of global store {} after {} retry attempts. " + "You can increase the number of retries via configuration parameter `retries`.", store.name(), retries, retryableException);
188: Matthias J. Sax, debug, CatchClause, log.debug("Failed to get end offsets for partitions {}, backing off for {} ms to retry (attempt {} of {})", topicPartitions, retryBackoffMs, attempts, retries, retryableException);
223: Matthias J. Sax, error, IfStmt, log.error("Failed to get partitions for topic {} after {} retry attempts due to timeout. " + "The broker may be transiently unavailable at the moment. " + "You can increase the number of retries via configuration parameter `retries`.", sourceTopic, retries, retryableException);
234: Matthias J. Sax, debug, CatchClause, log.debug("Failed to get partitions for topic {} due to timeout. The broker may be transiently unavailable at the moment. " + "Backing off for {} ms to retry (attempt {} of {})", sourceTopic, retryBackoffMs, attempts, retries, retryableException);
296: Matthias J. Sax, warn, CatchClause, log.warn("Restoring GlobalStore {} failed due to: {}. Deleting global store to recreate from scratch.", storeName, recoverableException.toString());
312: Matthias J. Sax, debug, MethodDeclaration, log.debug("Flushing all global globalStores registered in the state manager");
315: Damian Guy, trace, TryStmt, log.trace("Flushing global store={}", store.name());
332: Damian Guy, debug, ForeachStmt, log.debug("Closing global storage engine {}", entry.getKey());
336: Damian Guy, error, CatchClause, log.error("Failed to close global state store {}", entry.getKey(), e);
361: Guozhang Wang, warn, CatchClause, log.warn("Failed to write offset checkpoint file to {} for global stores: {}", checkpoint, e);
165: umesh chaudhary, info, IfStmt, log.info("State transition from {} to {}", oldState, newState);
162: umesh chaudhary, error, IfStmt, log.error("Unexpected state transition from {} to {}", oldState, newState);
260: Matthias J. Sax, error, CatchClause, log.error("Updating global state failed. You can restart KafkaStreams to recover from this error.", recoverableException);
285: John Roesler, info, CatchClause, log.info("Shutting down from initialization");
291: John Roesler, info, CatchClause, log.info("Shutdown complete");
304: umesh chaudhary, Error, IfStmt, log.warn("Error happened during initialization of the global state store; this thread has shutdown");
321: umesh chaudhary, info, TryStmt, log.info("Shutting down");
326: umesh chaudhary, error, CatchClause, log.error("Failed to close state maintainer due to the following error:", e);
333: umesh chaudhary, info, TryStmt, log.info("Shutdown complete");
380: Matthias J. Sax, error, CatchClause, log.error(errorMsg, fatalException);
67: Matthias J. Sax, debug, ConstructorDeclaration, log.debug("Configs:" + Utils.NL, "\t{} = {}" + Utils.NL, "\t{} = {}" + Utils.NL, "\t{} = {}", AdminClientConfig.RETRIES_CONFIG, retries, StreamsConfig.REPLICATION_FACTOR_CONFIG, replicationFactor, StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG, windowChangeLogAdditionalRetention);
98: Guozhang Wang, debug, ForeachStmt, log.debug("Going to create topic {} with {} partitions and config {}.", internalTopicConfig.name(), internalTopicConfig.numberOfPartitions(), topicConfig);
133: Guozhang Wang, info, IfStmt, log.info("Topic {} exist already: {}", topicName, couldNotCreateTopic.toString());
129: Matthias J. Sax, debug, IfStmt, log.debug("Could not get number of partitions for topic {} due to timeout. " + "Will try again (remaining retries {}).", topicName, remainingRetries - 1);
142: Matthias J. Sax, ERROR, CatchClause, log.error(INTERRUPTED_ERROR_MESSAGE, fatalException);
164: Matthias J. Sax, Error, IfStmt, log.error(timeoutAndRetryError);
191: Matthias J. Sax, ERROR, CatchClause, log.error(INTERRUPTED_ERROR_MESSAGE, fatalException);
201: Guozhang Wang, error, IfStmt, log.debug(error, topicFuture.getKey(), cause.toString());
197: Matthias J. Sax, debug, IfStmt, log.debug("Could not get number of partitions for topic {} due to timeout. " + "Will try again (remaining retries {}).", topicFuture.getKey(), remainingRetries - 1);
231: Matthias J. Sax, error, IfStmt, log.error(errorMsg);
1105: Matthias J. Sax, debug, ForeachStmt, log.debug("nodeToSourceTopics {}", nodeToSourceTopics);
1275: Guozhang Wang, debug, MethodDeclaration, log.debug("{}updating builder with {} topic(s) with possible matching regex subscription(s)", logPrefix, subscriptionUpdates);
1878: Guozhang Wang, debug, MethodDeclaration, log.debug("{}found {} topics possibly matching regex", topics, logPrefix);
96: umesh chaudhary, debug, ConstructorDeclaration, log.debug("Created state store manager for task {} with the acquired state dir lock", taskId);
113: Matthias J. Sax, debug, MethodDeclaration, log.debug("Registering state store {} to its state manager", storeName);
137: Matthias J. Sax, trace, IfStmt, log.trace("Restoring state store {} from changelog topic {}", storeName, topic);
133: Matthias J. Sax, trace, IfStmt, log.trace("Preparing standby replica of persistent state store {} with changelog topic {}", storeName, topic);
221: umesh chaudhary, trace, MethodDeclaration, log.trace("Updating store offset limit for partition {} to {}", partition, limit);
240: umesh chaudhary, debug, IfStmt, log.debug("Flushing all stores registered in the state manager");
242: Matthias J. Sax, trace, ForeachStmt, log.trace("Flushing store {}", store.name());
249: Matthias J. Sax, error, CatchClause, log.error("Failed to flush state store {}: ", store.name(), e);
270: umesh chaudhary, debug, IfStmt, log.debug("Closing its state manager and all the registered state stores");
272: Matthias J. Sax, debug, ForeachStmt, log.debug("Closing storage engine {}", store.name());
279: Matthias J. Sax, error, CatchClause, log.error("Failed to close state store {}: ", store.name(), e);
318: Guozhang Wang, trace, MethodDeclaration, log.trace("Writing checkpoint: {}", checkpointableOffsets);
322: Guozhang Wang, warn, CatchClause, log.warn("Failed to write offset checkpoint file to {}: {}", checkpoint, e);
332: umesh chaudhary, debug, MethodDeclaration, log.debug("Register global stores {}", stateStores);
126: Guozhang Wang, error, MethodDeclaration, log.error(errorLogMessage, key, value, timestamp, topic, exception.toString());
201: Matthias J. Sax, error, CatchClause, log.error("Timeout exception caught when sending record to topic {}. " + "This might happen if the producer cannot send data to the Kafka cluster and thus, " + "its internal buffer fills up. " + "You can increase producer parameter `max.block.ms` to increase this timeout.", topic);
188: John Roesler, Error, IfStmt, log.warn("Error sending records (key=[{}] value=[{}] timestamp=[{}]) to topic=[{}] and partition=[{}]; " + "The exception handler chose to CONTINUE processing in spite of this error.", key, value, timestamp, topic, partition, exception);
169: Guozhang Wang, warn, IfStmt, log.warn(LOG_MESSAGE, key, value, timestamp, topic, exception.toString());
230: umesh chaudhary, debug, MethodDeclaration, log.debug("Flushing producer");
237: umesh chaudhary, debug, MethodDeclaration, log.debug("Closing producer");
72: John Roesler, error, CatchClause, log.error("Deserialization error callback failed after deserialization error for record {}", rawRecord, deserializationException);
86: John Roesler, warn, IfStmt, log.warn("Skipping record due to deserialization error. topic=[{}] partition=[{}] offset=[{}]", rawRecord.topic(), rawRecord.partition(), rawRecord.offset(), deserializationException);
113: Damian Guy, trace, ForeachStmt, log.trace("Source node {} extracted timestamp {} for record {}", source.name(), timestamp, record);
117: John Roesler, warn, IfStmt, log.warn("Skipping record due to negative extracted timestamp. topic=[{}] partition=[{}] offset=[{}] extractedTimestamp=[{}] extractor=[{}]", record.topic(), record.partition(), record.offset(), timestamp, timestampExtractor.getClass().getCanonicalName());
67: Bill Bejeck, trace, MethodDeclaration, log.trace("Initializing state stores");
87: umesh chaudhary, debug, MethodDeclaration, log.debug("Resuming");
100: umesh chaudhary, trace, MethodDeclaration, log.trace("Committing");
114: umesh chaudhary, debug, MethodDeclaration, log.debug("Suspending");
138: umesh chaudhary, debug, MethodDeclaration, log.debug("Closing");
166: umesh chaudhary, trace, MethodDeclaration, log.trace("Updating standby replicas of its state store for partition [{}]", partition);
133: Damian Guy, trace, IfStmt, log.trace("{} Found cached state dir lock for task {}", logPrefix(), taskId);
163: Damian Guy, debug, IfStmt, log.debug("{} Acquired state dir lock for task {}", logPrefix(), taskId);
170: Damian Guy, trace, IfStmt, log.trace("{} Found cached state dir lock for the global task", logPrefix());
192: Damian Guy, debug, MethodDeclaration, log.debug("{} Acquired global state dir lock", logPrefix());
206: Damian Guy, debug, MethodDeclaration, log.debug("{} Released global state dir lock", logPrefix());
217: Damian Guy, debug, IfStmt, log.debug("{} Released state dir lock for task {}", logPrefix(), taskId);
236: Matthias J. Sax, error, CatchClause, log.error("{} Failed to delete global state directory due to an unexpected exception", logPrefix(), e);
293: Matthias J. Sax, error, IfStmt, log.error("{} Failed to get the state directory lock.", logPrefix(), e);
297: Matthias J. Sax, error, CatchClause, log.error("{} Failed to delete the state directory.", logPrefix(), e);
305: Matthias J. Sax, error, CatchClause, log.error("{} Failed to release the state directory lock.", logPrefix());
281: Matthias J. Sax, info, IfStmt, log.info("{} Deleting state directory {} for task {} as user calling cleanup.", logPrefix(), dirName, id);
273: Matthias J. Sax, info, IfStmt, log.info("{} Deleting obsolete state directory {} for task {} as {}ms has elapsed (cleanup delay is {}ms).", logPrefix(), dirName, id, now - lastModifiedMs, cleanupDelayMs);
91: Matthias J. Sax, warn, CatchClause, log.warn("Restoring StreamTasks failed. Deleting StreamTasks stores to recreate from scratch.", recoverableException);
95: Matthias J. Sax, info, ForeachStmt, log.info("Reinitializing StreamTask {}", task);
131: umesh chaudhary, debug, CatchClause, log.debug("Could not fetch end offset for {}; will fall back to partition by partition fetching", initializable);
155: umesh chaudhary, info, IfStmt, log.info("End offset cannot be found form the returned metadata; removing this partition from the current run loop");
167: umesh chaudhary, debug, MethodDeclaration, log.debug("Start restoring state stores from changelog topics {}", initialized.keySet());
203: umesh chaudhary, debug, MethodDeclaration, log.debug("Restoring partition {} from offset {} to endOffset {}", partition, startingOffset, endOffset);
212: Guozhang Wang, trace, MethodDeclaration, log.trace("The set of restoration completed partitions so far: {}", completed);
220: umesh chaudhary, debug, CatchClause, log.debug("Could not fetch topic metadata within the timeout, will retry in the next run loop");
269: umesh chaudhary, debug, IfStmt, log.debug("Completed restoring state from changelog {} with {} records ranging from offset {} to {}", topicPartition, restorer.restoredNumRecords(), restorer.startingOffset(), restorer.restoredOffset());
206: Matthias J. Sax, info, SwitchStmt, log.info("Downgrading metadata version from {} to 1 for upgrade from 0.10.0.x.", SubscriptionInfo.LATEST_SUPPORTED_VERSION);
214: Matthias J. Sax, info, SwitchStmt, log.info("Downgrading metadata version from {} to 2 for upgrade from " + upgradeFrom + ".x.", SubscriptionInfo.LATEST_SUPPORTED_VERSION);
225: Matthias J. Sax, fatal, IfStmt, log.error(fatalException.getMessage(), fatalException);
231: Matthias J. Sax, fatal, IfStmt, log.error(fatalException.getMessage(), fatalException);
344: umesh chaudhary, debug, MethodDeclaration, log.debug("Constructed client metadata {} from the member subscriptions.", clientsMetadata);
433: umesh chaudhary, debug, MethodDeclaration, log.debug("Created repartition topics {} from the parsed topology.", allRepartitionTopicPartitions.values());
454: umesh chaudhary, warn, IfStmt, log.warn("Partition {} is assigned to more than one tasks: {}", partition, partitionsForTask);
481: umesh chaudhary, warn, IfStmt, log.warn("No partitions found for topic {}", topic);
473: Bill Bejeck, warn, IfStmt, log.warn("Partition {} is not assigned to any tasks: {}" + " Possible causes of a partition not getting assigned" + " is that another topic defined in the topology has not been" + " created when starting your streams application," + " resulting in no tasks created for this topology at all.", partition, partitionsForTask);
504: umesh chaudhary, debug, IfStmt, log.debug("No tasks found for topic group {}", topicGroupId);
511: umesh chaudhary, debug, MethodDeclaration, log.debug("Created state changelog topics {} from the parsed topology.", changelogTopicMetadata.values());
521: Guozhang Wang, debug, MethodDeclaration, log.debug("Assigning tasks {} to clients {} with number of replicas {}", partitionsForTask.keySet(), states, numStandbyReplicas);
527: umesh chaudhary, info, MethodDeclaration, log.info("Assigned tasks to clients as {}.", states);
723: umesh chaudhary, debug, MethodDeclaration, log.debug("Starting to validate internal topics in partition assignor.");
747: Guozhang Wang, debug, MethodDeclaration, log.debug("Completed validating internal topics in partition assignor.");
224: Bill Bejeck, trace, MethodDeclaration, log.trace("Initializing state stores");
261: umesh chaudhary, debug, MethodDeclaration, log.debug("Resuming");
285: umesh chaudhary, trace, TryStmt, log.trace("Start processing one record [{}]", record);
290: umesh chaudhary, trace, TryStmt, log.trace("Completed processing one record [{}]", record);
331: umesh chaudhary, trace, IfStmt, log.trace("Punctuating processor {} with timestamp {} and punctuation type {}", node.name(), timestamp, type);
372: umesh chaudhary, debug, MethodDeclaration, log.debug("Committing");
394: umesh chaudhary, trace, MethodDeclaration, log.trace("Flushing state and producer");
410: Matthias J. Sax, trace, IfStmt, log.trace("Committing offsets");
454: umesh chaudhary, trace, MethodDeclaration, log.trace("Initializing processor nodes of the topology");
479: umesh chaudhary, debug, MethodDeclaration, log.debug("Suspending");
504: umesh chaudhary, trace, MethodDeclaration, log.trace("Closing processor topology");
541: umesh chaudhary, error, CatchClause, log.error("Could not close state manager due to the following error:", e);
571: umesh chaudhary, error, CatchClause, log.error("Failed to close producer due to the following error:", e);
602: umesh chaudhary, debug, MethodDeclaration, log.debug("Closing");
610: umesh chaudhary, error, CatchClause, log.error("Could not close task due to the following error:", e);
630: umesh chaudhary, trace, IfStmt, log.trace("Added records into the buffered queue of partition {}, new queue size is {}", partition, newQueueSize);
208: umesh chaudhary, info, IfStmt, log.info("State transition from {} to {}", oldState, newState);
205: umesh chaudhary, error, IfStmt, log.error("Unexpected state transition from {} to {}", oldState, newState);
255: umesh chaudhary, debug, MethodDeclaration, log.debug("at state {}: partitions {} assigned at the end of consumer rebalance.\n" + "\tcurrent suspended active tasks: {}\n" + "\tcurrent suspended standby tasks: {}\n", streamThread.state, assignment, taskManager.suspendedActiveTaskIds(), taskManager.suspendedStandbyTaskIds());
270: John Roesler, Error, CatchClause, log.error("Error caught during partition assignment, " + "will abort the current process and re-throw at the end of rebalance: {}", t);
277: umesh chaudhary, info, TryStmt, log.info("partition assignment took {} ms.\n" + "\tcurrent active tasks: {}\n" + "\tcurrent standby tasks: {}\n" + "\tprevious active tasks: {}\n", time.milliseconds() - start, taskManager.activeTaskIds(), taskManager.standbyTaskIds(), taskManager.prevActiveTaskIds());
290: umesh chaudhary, debug, MethodDeclaration, log.debug("at state {}: partitions {} revoked at the beginning of consumer rebalance.\n" + "\tcurrent assigned active tasks: {}\n" + "\tcurrent assigned standby tasks: {}\n", streamThread.state, assignment, taskManager.activeTaskIds(), taskManager.standbyTaskIds());
304: John Roesler, Error, CatchClause, log.error("Error caught during partition revocation, " + "will abort the current process and re-throw at the end of rebalance: {}", t);
313: umesh chaudhary, info, TryStmt, log.info("partition revocation took {} ms.\n" + "\tsuspended active tasks: {}\n" + "\tsuspended standby tasks: {}", time.milliseconds() - start, taskManager.suspendedActiveTaskIds(), taskManager.suspendedStandbyTaskIds());
368: umesh chaudhary, trace, IfStmt, log.trace("Created task {} with assigned partitions {}", taskId, partitions);
437: umesh chaudhary, info, IfStmt, log.info("Creating producer client for task {}", id);
451: umesh chaudhary, error, CatchClause, log.error("Failed to close producer due to the following error:", e);
494: John Roesler, trace, IfStmt, log.trace("Skipped standby task {} with assigned partitions {} " + "since it does not have any state stores to materialize", taskId, partitions);
594: umesh chaudhary, info, MethodDeclaration, log.info("Creating restore consumer client");
603: umesh chaudhary, info, IfStmt, log.info("Creating shared producer client");
646: Guozhang Wang, info, MethodDeclaration, log.info("Creating consumer client");
711: umesh chaudhary, info, MethodDeclaration, log.info("Starting");
713: Rohan Desai, info, IfStmt, log.info("StreamThread already shutdown. Not running");
751: Guozhang Wang, warn, CatchClause, log.warn("Detected task {} that got migrated to another thread. " + "This implies that this thread missed a rebalance and dropped out of the consumer group. " + "Will try to rejoin the consumer group. Below is the detailed description of the task:\n{}", ignoreAndRejoinGroup.migratedTask().id(), ignoreAndRejoinGroup.migratedTask().toString(">"));
885: umesh chaudhary, info, IfStmt, log.info(logMessage, topic, resetPolicy);
901: Guozhang Wang, info, IfStmt, log.info("Stream task {} is already closed, probably because it got unexpectedly migrated to another thread already. " + "Notifying the thread to trigger a new rebalance immediately.", task.id());
985: umesh chaudhary, debug, IfStmt, log.debug("processing latency {} < commit time {} for {} records. Adjusting up recordsProcessedBeforeCommit={}", processLatency, commitTime, totalProcessed, recordsProcessedBeforeCommit);
980: umesh chaudhary, debug, IfStmt, log.debug("processing latency {} > commit time {} for {} records. Adjusting down recordsProcessedBeforeCommit={}", processLatency, commitTime, totalProcessed, recordsProcessedBeforeCommit);
1001: umesh chaudhary, trace, IfStmt, log.trace("Committing all active tasks {} and standby tasks {} since {}ms has elapsed (commit interval is {}ms)", taskManager.activeTaskIds(), taskManager.standbyTaskIds(), now - lastCommitMs, commitTimeMs);
1013: coscale_kdegroot, debug, IfStmt, log.debug("Committed all active tasks {} and standby tasks {} in {}ms", taskManager.activeTaskIds(), taskManager.standbyTaskIds(), timerStartedMs - now);
1036: Guozhang Wang, info, IfStmt, log.info("Standby task {} is already closed, probably because it got unexpectedly migrated to another thread already. " + "Notifying the thread to trigger a new rebalance immediately.", task.id());
1052: umesh chaudhary, debug, IfStmt, log.debug("Updated standby tasks {} in {}ms", taskManager.standbyTaskIds(), time.milliseconds() - now);
1082: Matthias J. Sax, warn, CatchClause, log.warn("Updating StandbyTasks failed. Deleting StandbyTasks stores to recreate from scratch.", recoverableException);
1088: Guozhang Wang, info, IfStmt, log.info("Standby task {} is already closed, probably because it got unexpectedly migrated to another thread already. " + "Notifying the thread to trigger a new rebalance immediately.", task.id());
1093: Matthias J. Sax, info, ForeachStmt, log.info("Reinitializing StandbyTask {}", task);
1069: Guozhang Wang, info, IfStmt, log.info("Standby task {} is already closed, probably because it got unexpectedly migrated to another thread already. " + "Notifying the thread to trigger a new rebalance immediately.", task.id());
1121: umesh chaudhary, Info, MethodDeclaration, log.info("Informed to shut down");
1137: umesh chaudhary, info, MethodDeclaration, log.info("Shutting down");
1142: Matthias J. Sax, error, CatchClause, log.error("Failed to close task manager due to the following error:", e);
1147: umesh chaudhary, error, CatchClause, log.error("Failed to close consumer due to the following error:", e);
1152: umesh chaudhary, error, CatchClause, log.error("Failed to close restore consumer due to the following error:", e);
1157: umesh chaudhary, info, MethodDeclaration, log.info("Shutdown complete");
110: Kamal C, trace, MethodDeclaration, log.trace("Pausing partitions: {}", assignment);
120: Guozhang Wang, debug, MethodDeclaration, log.debug("Adding assigned tasks as active: {}", assignedActiveTasks);
135: umesh chaudhary, warn, IfStmt, log.warn("Task {} owned partitions {} are not contained in the assignment {}", taskId, partitions, assignment);
131: Matthias J. Sax, error, CatchClause, log.error("Failed to resume an active task {} due to the following error:", taskId, e);
158: umesh chaudhary, debug, MethodDeclaration, log.debug("Adding assigned standby tasks {}", assignedStandbyTasks);
238: umesh chaudhary, debug, MethodDeclaration, log.debug("Suspending all active tasks {} and standby tasks {}", active.runningTaskIds(), standby.runningTaskIds());
256: umesh chaudhary, debug, MethodDeclaration, log.debug("Shutting down all active tasks {}, standby tasks {}, suspended tasks {}, and suspended standby tasks {}", active.runningTaskIds(), standby.runningTaskIds(), active.previousTaskIds(), standby.previousTaskIds());
324: Kamal C, trace, IfStmt, log.trace("Resuming partitions {}", assignment);
435: Guozhang Wang, debug, IfStmt, log.debug("Previous delete-records request has failed: {}. Try sending the new request now", deleteRecordsResult.lowWatermarks());
444: Guozhang Wang, trace, IfStmt, log.trace("Sent delete-records request: {}", recordsToDelete);
234: Matthias J. Sax, fatal, SwitchStmt, log.error(fatalException.getMessage(), fatalException);
61: Damian Guy, warn, IfStmt, log.warn("Unable to assign {} of {} standby tasks for task [{}]. " + "There is not enough available capacity. You should " + "increase the number of threads and/or application instances " + "to maintain the requested number of standby replicas.", numStandbyReplicas - i, numStandbyReplicas, taskId);
255: Matthias J. Sax, info, SwitchStmt, log.info("Unable to decode subscription data: used version: {}; latest supported version: {}", usedVersion, LATEST_SUPPORTED_VERSION);
301: Guozhang Wang, trace, MethodDeclaration, log.trace("Defining InMemory Store name={} capacity={} logged={}", name, capacity, logged);
365: Guozhang Wang, trace, MethodDeclaration, log.trace("Defining RocksDb Store name={} numSegments={} logged={}", name, numSegments, logged);
106: Xavier Laut, trace, IfStmt, log.trace("Named cache {} stats on flush: #hits={}, #misses={}, #overwrites={}, #flushes={}", name, hits(), misses(), overwrites(), flushes());
124: Damian Guy, ERROR, MethodDeclaration, options.setInfoLogLevel(InfoLogLevel.ERROR_LEVEL);
200: Colin P. Mccabe, Error, CatchClause, log.error("Error destroying {}", segment, e);
221: Damian Guy, warn, CatchClause, log.warn("Unable to parse segmentName {} to a date. This segment will be skipped", segmentName);
130: umesh chaudhary, trace, IfStmt, log.trace("Cache stats on flush: #puts={}, #gets={}, #evicts={}, #flushes={}", puts(), gets(), evicts(), flushes());
248: umesh chaudhary, trace, IfStmt, log.trace("Evicted {} entries from cache {}", numEvicted, namespace);
123: Guozhang Wang, Info, TryStmt, final Collection<DescribeLogDirsResponse.LogDirInfo> logDirInfo = adminClient.describeLogDirs(Collections.singleton(0)).values().get(0).get().values();
126: Guozhang Wang, Info, ForeachStmt, final DescribeLogDirsResponse.ReplicaInfo replicaInfo = partitionInfo.replicaInfos.get(new TopicPartition(REPARTITION_TOPIC, 0));
157: Filipe Agapito, warn, CatchClause, log.warn("Unable to read '{}{}{}'. Using default inputValues list", "resources", File.separator, fileName);
85: Eno Thereska, debug, MethodDeclaration, log.debug("Initiating embedded Kafka cluster startup");
86: Eno Thereska, debug, MethodDeclaration, log.debug("Starting a ZooKeeper instance");
88: Eno Thereska, debug, MethodDeclaration, log.debug("ZooKeeper instance is running at {}", zKConnectString());
107: Guozhang Wang, debug, ForStmt, log.debug("Starting a Kafka instance on port {} ...", brokerConfig.get(KafkaConfig$.MODULE$.PortProp()));
110: Eno Thereska, debug, ForStmt, log.debug("Kafka instance is running at {}, connected to ZooKeeper at {}", brokers[i].brokerList(), brokers[i].zookeeperConnect());
73: Eno Thereska, debug, ConstructorDeclaration, log.debug("Starting embedded Kafka broker (with log.dirs={} and ZK ensemble at {}) ...", logDir, zookeeperConnect());
76: Eno Thereska, debug, ConstructorDeclaration, log.debug("Startup of embedded Kafka broker at {} completed (with ZK ensemble at {}) ...", brokerList(), zookeeperConnect());
127: Eno Thereska, debug, MethodDeclaration, log.debug("Shutting down embedded Kafka broker at {} (with ZK ensemble at {}) ...", brokerList(), zookeeperConnect());
131: Eno Thereska, debug, MethodDeclaration, log.debug("Removing logs.dir at {} ...", logDir);
135: Eno Thereska, debug, MethodDeclaration, log.debug("Shutdown of embedded Kafka broker at {} completed (with ZK ensemble at {}) ...", brokerList(), zookeeperConnect());
171: Eno Thereska, debug, MethodDeclaration, log.debug("Creating topic { name: {}, partitions: {}, replication: {}, config: {} }", topic, partitions, replication, topicConfig);
185: bbejeck, debug, MethodDeclaration, log.debug("Deleting topic { name: {} }", topic);
334: Matthias J. Sax, Info, MethodDeclaration, final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();
336: Matthias J. Sax, Info, MethodDeclaration, final Map<Integer, InternalTopologyBuilder.TopicsInfo> expectedTopicGroups = new HashMap<>();
337: Matthias J. Sax, Info, MethodDeclaration, expectedTopicGroups.put(0, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet("topic-1", "X-topic-1x", "topic-2"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.<String, InternalTopicConfig>emptyMap()));
338: Matthias J. Sax, Info, MethodDeclaration, expectedTopicGroups.put(1, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet("topic-3", "topic-4"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.<String, InternalTopicConfig>emptyMap()));
339: Matthias J. Sax, Info, MethodDeclaration, expectedTopicGroups.put(2, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet("topic-5"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.<String, InternalTopicConfig>emptyMap()));
372: Matthias J. Sax, Info, MethodDeclaration, final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();
374: Matthias J. Sax, Info, MethodDeclaration, final Map<Integer, InternalTopologyBuilder.TopicsInfo> expectedTopicGroups = new HashMap<>();
378: Matthias J. Sax, Info, MethodDeclaration, expectedTopicGroups.put(0, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet("topic-1", "topic-1x", "topic-2"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.singletonMap(store1, (InternalTopicConfig) new UnwindowedChangelogTopicConfig(store1, Collections.<String, String>emptyMap()))));
382: Matthias J. Sax, Info, MethodDeclaration, expectedTopicGroups.put(1, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet("topic-3", "topic-4"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.singletonMap(store2, (InternalTopicConfig) new UnwindowedChangelogTopicConfig(store2, Collections.<String, String>emptyMap()))));
386: Matthias J. Sax, Info, MethodDeclaration, expectedTopicGroups.put(2, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet("topic-5"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.singletonMap(store3, (InternalTopicConfig) new UnwindowedChangelogTopicConfig(store3, Collections.<String, String>emptyMap()))));
520: Matthias J. Sax, Info, MethodDeclaration, final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();
521: Matthias J. Sax, Info, MethodDeclaration, final InternalTopologyBuilder.TopicsInfo topicsInfo = topicGroups.values().iterator().next();
538: Matthias J. Sax, Info, MethodDeclaration, final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();
539: Matthias J. Sax, Info, MethodDeclaration, final InternalTopologyBuilder.TopicsInfo topicsInfo = topicGroups.values().iterator().next();
554: Matthias J. Sax, Info, MethodDeclaration, final InternalTopologyBuilder.TopicsInfo topicsInfo = builder.topicGroups().values().iterator().next();
636: Matthias J. Sax, Info, MethodDeclaration, final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();
530: Damian Guy, Info, MethodDeclaration, Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();
259: Colin P. Mccabe, info, WhileStmt, log.info("Saw only {} cluster nodes.  Waiting to see {}.", nodes.size(), testConfig.numClusterNodes);
304: Colin P. Mccabe, info, WhileStmt, log.info("Did not see newtopic.  Retrying listTopics...");
395: Vahid Hashemian, info, MethodDeclaration, log.info("offsetsForTime = {}", offsetsForTime.result);
450: Vahid Hashemian, debug, TryStmt, log.debug("Found first message...");
469: Vahid Hashemian, debug, CatchClause, log.debug("Got RecordTooLargeException", e);
474: Vahid Hashemian, debug, TryStmt, log.debug("Closing consumer.");
476: Colin P. Mccabe, info, MethodDeclaration, log.info("Closed consumer.");
501: Colin P. Mccabe, info, CatchClause, log.info("Got UnsupportedVersionException when attempting to use feature {}", featureName);
499: Colin P. Mccabe, info, TryStmt, log.info("Successfully used feature {}", featureName);
129: Ewen Cheslack-Postava, info, MethodDeclaration, log.info("Configured PushHttpMetricsReporter for {} to report every {} seconds", url, period);
136: Ewen Cheslack-Postava, debug, ForeachStmt, log.debug("Adding metric {}", metric.metricName());
145: Ewen Cheslack-Postava, debug, SynchronizedStmt, log.debug("Updating metric {}", metric.metricName());
153: Ewen Cheslack-Postava, debug, SynchronizedStmt, log.debug("Removing metric {}", metric.metricName());
184: Ewen Cheslack-Postava, trace, MethodDeclaration, log.trace("Reporting {} metrics to {}", samples.size(), url);
216: Ewen Cheslack-Postava, Error, CatchClause, log.error("Error reporting metrics", e);
213: Ewen Cheslack-Postava, info, IfStmt, log.info("Finished reporting metrics with response code {}", responseCode);
211: Ewen Cheslack-Postava, error, IfStmt, log.error("PushHttpMetricsReporter does not currently support redirects, saw {}", responseCode);
209: Ewen Cheslack-Postava, Error, IfStmt, log.error("Error reporting metrics, {}: {}", responseCode, msg);
262: Ashish Singh, info, MethodDeclaration, logger.info(msg);
147: Colin P. Mccabe, info, MethodDeclaration, log.info("Starting agent process.");
153: Colin P. Mccabe, warn, MethodDeclaration, log.warn("Running agent shutdown hook.");
158: Colin P. Mccabe, error, CatchClause, log.error("Got exception while running agent shutdown hook.", e);
349: Colin Patrick McCabe, info, IfStmt, log.info("{}: Error creating worker {} for task {} with spec {}", nodeName, workerId, taskId, spec, e);
346: Colin Patrick McCabe, info, IfStmt, log.info("{}: request conflict while creating worker {} for task {} with spec {}.", nodeName, workerId, taskId, spec);
315: Colin P. Mccabe, info, IfStmt, log.info("{}: Ignoring request to create worker {}, because there is already " + "a worker with that id.", nodeName, workerId);
328: Colin Patrick McCabe, info, IfStmt, log.info("{}: Worker {} is halting with error {}", nodeName, worker, errorString);
326: Colin Patrick McCabe, info, IfStmt, log.info("{}: Worker {} is halting.", nodeName, worker);
339: Colin Patrick McCabe, info, CatchClause, log.info("{}: Worker {} start() exception", nodeName, worker, e);
392: Colin Patrick McCabe, info, CatchClause, log.info("{}: unable to create worker {} for task {}, with spec {}", nodeName, workerId, taskId, spec, e);
389: Colin Patrick McCabe, info, TryStmt, log.info("{}: Created worker {} with spec {}", nodeName, worker, spec);
413: Colin P. Mccabe, info, SwitchStmt, log.info("{}: Worker {} was cancelled while it was starting up.  " + "Transitioning to STOPPING.", nodeName, worker);
418: Colin P. Mccabe, info, SwitchStmt, log.info("{}: Worker {} is now RUNNING.  Scheduled to stop in {} ms.", nodeName, worker, worker.spec.durationMs());
457: Colin P. Mccabe, info, IfStmt, log.info("{}: Worker {} {} during startup.  Transitioning to CANCELLING.", nodeName, worker, verb);
453: Colin P. Mccabe, info, IfStmt, log.info("{}: Worker {} {} during startup.  Transitioning to DONE.", nodeName, worker, verb);
463: Colin P. Mccabe, info, SwitchStmt, log.info("{}: Cancelling worker {} {}.  ", nodeName, worker, verb);
467: Colin P. Mccabe, info, SwitchStmt, log.info("{}: Running worker {} {}.  Transitioning to STOPPING.", nodeName, worker, verb);
472: Colin Patrick McCabe, info, SwitchStmt, log.info("{}: Stopping worker {} {}.", nodeName, worker, verb);
475: Colin P. Mccabe, info, SwitchStmt, log.info("{}: Can't halt worker {} because it is already DONE.", nodeName, worker);
507: Colin Patrick McCabe, info, IfStmt, log.info("{}: completed worker {} with error {}", nodeName, worker, worker.error);
503: Colin Patrick McCabe, info, IfStmt, log.info("{}: destroying worker {} with error {}", nodeName, worker, worker.error);
538: Colin Patrick McCabe, info, IfStmt, log.info("{}: Can't stop worker {} because there is no worker with that ID.", nodeName, workerId);
547: Colin P. Mccabe, info, SwitchStmt, log.info("{}: Cancelling worker {} during its startup process.", nodeName, worker);
552: Colin P. Mccabe, info, SwitchStmt, log.info("{}: Can't stop worker {}, because it is already being " + "cancelled.", nodeName, worker);
556: Colin Patrick McCabe, info, SwitchStmt, log.info("{}: Stopping running worker {}.", nodeName, worker);
560: Colin P. Mccabe, info, SwitchStmt, log.info("{}: Can't stop worker {}, because it is already " + "stopping.", nodeName, worker);
569: Colin Patrick McCabe, debug, IfStmt, log.debug("{}: Can't stop worker {}, because it is already done.", nodeName, worker);
565: Colin Patrick McCabe, info, IfStmt, log.info("{}: destroying worker {} with error {}", nodeName, worker, worker.error);
595: Colin P. Mccabe, error, CatchClause, log.error("{}: worker.stop() exception", nodeName, exception);
635: Colin Patrick McCabe, info, MethodDeclaration, log.info("{}: Shutting down WorkerManager.", nodeName);
649: Colin Patrick McCabe, info, CatchClause, log.info("{}: Caught exception while shutting down WorkerManager", nodeName, e);
638: Colin Patrick McCabe, info, TryStmt, log.info("{}: Waiting for shutdownManager quiescence...", nodeName);
642: Colin Patrick McCabe, info, TryStmt, log.info("{}: Waiting for workerCleanupExecutor to terminate...", nodeName);
644: Colin Patrick McCabe, info, TryStmt, log.info("{}: Waiting for stateChangeExecutor to terminate...", nodeName);
646: Colin Patrick McCabe, info, TryStmt, log.info("{}: Shutting down shutdownExecutor.", nodeName);
662: Colin Patrick McCabe, info, MethodDeclaration, log.info("{}: Destroying all workers.", nodeName);
677: Colin Patrick McCabe, error, CatchClause, log.error("Failed to stop worker {}", workerId, e);
55: Colin P. Mccabe, info, CatchClause, log.info("RUN: {}. ERROR: [{}]", Utils.join(command, " "), e.getMessage());
52: Colin P. Mccabe, info, TryStmt, log.info("RUN: {}. RESULT: [{}]", Utils.join(command, " "), result);
64: Colin P. Mccabe, warn, MethodDeclaration, log.warn("{} caught an exception: ", what, exception);
132: Anna Povzner, warn, CatchClause, log.warn("Failed to create or verify topics {}", topics, e);
158: Anna Povzner, warn, CatchClause, log.warn("Failed to get topic partitions matching {}", topicRegex, e);
171: Anna Povzner, warn, IfStmt, log.warn("Request to create topics has an empty topic list.");
178: Anna Povzner, warn, IfStmt, log.warn("Topic(s) {} already exist.", topicsExists);
206: Anna Povzner, info, WhileStmt, log.info("Attempting to create {} topics (try {})...", topicsToCreate.size(), ++tries);
235: Anna Povzner, warn, IfStmt, log.warn("Failed to create {}", topicName, e.getCause());
232: Anna Povzner, info, IfStmt, log.info("Topic {} already exists.", topicName);
228: Anna Povzner, warn, IfStmt, log.warn("Attempt to create topic `{}` failed: {}", topicName, e.getCause().getMessage());
224: Anna Povzner, debug, TryStmt, log.debug("Successfully created {}.", topicName);
246: Anna Povzner, warn, IfStmt, log.warn(str);
278: Anna Povzner, warn, IfStmt, log.warn(str);
155: Colin P. Mccabe, info, MethodDeclaration, log.info("Starting coordinator process.");
162: Colin P. Mccabe, warn, MethodDeclaration, log.warn("Running coordinator shutdown hook.");
167: Colin P. Mccabe, error, CatchClause, log.error("Got exception while running coordinator shutdown hook.", e);
101: Colin Patrick McCabe, error, CatchClause, log.error("{}: error creating worker {}.", node.name(), this, e);
109: Colin Patrick McCabe, error, CatchClause, log.error("{}: error stopping worker {}.", node.name(), this, e);
244: Colin P. Mccabe, error, CatchClause, log.error("{}: Unhandled exception in NodeHeartbeatRunnable", node.name(), e);
194: Colin P. Mccabe, error, CatchClause, log.error("{}: failed to get agent status: ConnectException {}", node.name(), e.getMessage());
197: Colin P. Mccabe, error, CatchClause, log.error("{}: failed to get agent status", node.name(), e);
203: Colin Patrick McCabe, trace, IfStmt, log.trace("{}: got heartbeat status {}", node.name(), agentStatus);
237: Colin Patrick McCabe, info, IfStmt, log.info("{}: worker state changed from {} to {}", node.name(), worker.state, state);
235: Colin Patrick McCabe, debug, IfStmt, log.debug("{}: worker state is still {}", node.name(), worker.state);
278: Colin Patrick McCabe, error, IfStmt, log.error("{}: there is already a worker {} with ID {}.", node.name(), worker, workerId);
283: Colin Patrick McCabe, info, MethodDeclaration, log.info("{}: scheduling worker {} to start.", node.name(), worker);
313: Colin Patrick McCabe, error, IfStmt, log.error("{}: unable to locate worker to stop with ID {}.", node.name(), workerId);
317: Colin Patrick McCabe, error, IfStmt, log.error("{}: Worker {} is already scheduled to stop.", node.name(), worker);
321: Colin Patrick McCabe, info, MethodDeclaration, log.info("{}: scheduling worker {} to stop.", node.name(), worker);
351: Colin Patrick McCabe, error, IfStmt, log.error("{}: unable to locate worker to destroy with ID {}.", node.name(), workerId);
365: Colin P. Mccabe, error, CatchClause, log.error("{}: Failed to send shutdown request", node.name(), e);
140: Colin P. Mccabe, info, ConstructorDeclaration, log.info("Created TaskManager for agent(s) on: {}", Utils.join(nodeManagers.keySet(), ", "));
306: Colin Patrick McCabe, info, CatchClause, log.info("createTask(id={}, spec={}) error", id, spec, e);
334: Colin Patrick McCabe, info, IfStmt, log.info("Task {} already exists with spec {}", id, spec);
345: Colin P. Mccabe, info, IfStmt, log.info("Failed to create a new task {} with spec {}: {}", id, spec, failure);
357: Colin P. Mccabe, info, MethodDeclaration, log.info("Created a new task {} with spec {}, scheduled to start {} ms from now.", id, spec, delayMs);
377: Colin P. Mccabe, info, IfStmt, log.info("Can't start task {}, because it is already in state {}.", task.id, task.state);
385: Colin P. Mccabe, error, CatchClause, log.error("Unable to find nodes for task {}", task.id, e);
391: Colin P. Mccabe, info, MethodDeclaration, log.info("Running task {} on node(s): {}", task.id, Utils.join(nodeNames, ", "));
413: Colin Patrick McCabe, info, CatchClause, log.info("stopTask(id={}) error", id, e);
435: Colin P. Mccabe, info, IfStmt, log.info("Can't cancel non-existent task {}.", id);
444: Colin P. Mccabe, info, SwitchStmt, log.info("Stopped pending task {}.", id);
461: Colin Patrick McCabe, info, IfStmt, log.info("Cancelling task {} with worker(s) {}", id, Utils.mkString(activeWorkerIds, "", "", " = ", ", "));
453: Colin Patrick McCabe, info, IfStmt, log.info("Task {} is now complete with error: {}", id, task.error);
451: Colin Patrick McCabe, info, IfStmt, log.info("Task {} is now complete with no errors.", id);
467: Colin P. Mccabe, info, SwitchStmt, log.info("Can't cancel task {} because it is already stopping.", id);
470: Colin P. Mccabe, info, SwitchStmt, log.info("Can't cancel task {} because it is already done.", id);
481: Colin Patrick McCabe, info, CatchClause, log.info("destroyTask(id={}) error", id, e);
503: Colin Patrick McCabe, info, IfStmt, log.info("Can't destroy task {}: no such task found.", id);
506: Colin Patrick McCabe, info, MethodDeclaration, log.info("Destroying task {}.", id);
561: Colin Patrick McCabe, Error, CatchClause, log.error("Error updating worker state for {} on {}.  Stopping worker.", workerId, nodeName, e);
554: Colin Patrick McCabe, debug, TryStmt, log.debug("Task {}: Updating worker state for {} on {} from {} to {}.", task.id, workerId, nodeName, prevState, nextState);
581: Colin Patrick McCabe, warn, IfStmt, log.warn("{}: Worker {} finished with error '{}' and status '{}'", nodeName, task.id, state.error(), JsonUtil.toJsonString(state.status()));
578: Colin Patrick McCabe, info, IfStmt, log.info("{}: Worker {} finished with status '{}'", nodeName, task.id, JsonUtil.toJsonString(state.status()));
593: Colin Patrick McCabe, info, IfStmt, log.info("{}: task {} stopped with error {}.  Stopping worker(s): {}", nodeName, task.id, task.error, Utils.mkString(activeWorkerIds, "{", "}", ": ", ", "));
589: Colin Patrick McCabe, info, IfStmt, log.info("{}: Task {} is now complete on {} with error: {}", nodeName, task.id, Utils.join(task.workerIds.keySet(), ", "), task.error.isEmpty() ? "(none)" : task.error);
656: Colin P. Mccabe, info, MethodDeclaration, log.info("Shutting down TaskManager{}.", stopAgents ? " and agents" : "");
49: Colin P. Mccabe, info, MethodDeclaration, log.info("Activating {} {}: {}.", spec.getClass().getSimpleName(), id, spec);
58: Colin P. Mccabe, info, MethodDeclaration, log.info("Deactivating {} {}: {}.", spec.getClass().getSimpleName(), id, spec);
52: Colin P. Mccabe, info, MethodDeclaration, log.info("Activating NetworkPartitionFault {}.", id);
61: Colin P. Mccabe, info, MethodDeclaration, log.info("Deactivating NetworkPartitionFault {}.", id);
50: Colin P. Mccabe, info, MethodDeclaration, log.info("Activating ProcessStopFault {}.", id);
58: Colin P. Mccabe, info, MethodDeclaration, log.info("Deactivating ProcessStopFault {}.", id);
74: Colin P. Mccabe, error, CatchClause, log.error("Failed to parse process ID from line {}", e);
82: Colin P. Mccabe, info, IfStmt, log.info("{}: sending {} to {} pid(s) {}", id, signalName, javaProcessName, Utils.join(pids, ", "));
79: Colin P. Mccabe, error, IfStmt, log.error("{}: no processes containing {} found to send {} to.", id, javaProcessName, signalName);
89: Colin P. Mccabe, info, MethodDeclaration, log.info("Starting REST server");
94: Colin P. Mccabe, info, ForeachStmt, log.info("Registered resource {}", resource);
123: Colin P. Mccabe, info, MethodDeclaration, log.info("REST server listening at " + jettyServer.getURI());
144: Colin P. Mccabe, error, CatchClause, log.error("Unable to stop REST server", e);
139: Colin P. Mccabe, info, TryStmt, log.info("Stopping REST server");
142: Colin P. Mccabe, info, TryStmt, log.info("REST server stopped");
196: Colin Patrick McCabe, debug, TryStmt, logger.debug("Sending {} with input {} to {}", method, serializedBody, url);
284: Colin Patrick McCabe, info, CatchClause, logger.info("{} {}: error: {}", method, url, e.getMessage());
38: Colin P. Mccabe, info, IfStmt, log.info("Uncaught exception in REST call: {}", e.getMessage());
36: Colin P. Mccabe, debug, IfStmt, log.debug("Uncaught exception in REST call: ", e);
40: Colin P. Mccabe, info, MethodDeclaration, log.info("{}: Activating NoOpTask.", id);
47: Colin P. Mccabe, info, MethodDeclaration, log.info("{}: Deactivating NoOpTask.", id);
76: Anna Povzner, info, MethodDeclaration, log.info("{}: Activating ConsumeBenchWorker with {}", id, spec);
94: Colin Patrick McCabe, info, TryStmt, log.info("Will consume from {}", partitions);
167: Anna Povzner, info, TryStmt, log.info("Consumed total number of messages={}, bytes={} in {} ms.  status: {}", messagesConsumed, bytesConsumed, curTimeMs - startTimeMs, statusData);
205: Anna Povzner, info, MethodDeclaration, log.info("Status={}", JsonUtil.toJsonString(statusData));
283: Anna Povzner, info, MethodDeclaration, log.info("{}: Deactivating ConsumeBenchWorker.", id);
82: Colin Patrick McCabe, info, MethodDeclaration, log.info("{}: Activating ProduceBenchWorker with {}", id, spec);
142: Colin P. Mccabe, error, IfStmt, log.error("SendRecordsCallback: error", exception);
231: Colin P. Mccabe, info, TryStmt, log.info("Sent {} total record(s) in {} ms.  status: {}", histogram.summarize().numSamples(), curTimeMs - startTimeMs, statusData);
327: Colin P. Mccabe, info, MethodDeclaration, log.info("{}: Deactivating ProduceBenchWorker.", id);
111: Colin P. Mccabe, info, MethodDeclaration, log.info("{}: Activating RoundTripWorker.", id);
227: Colin P. Mccabe, debug, MethodDeclaration, log.debug("{}: Starting RoundTripWorker#ProducerRunnable.", id);
266: Colin P. Mccabe, info, TryStmt, log.info("{}: ProducerRunnable is exiting.  messagesSent={}; uniqueMessagesSent={}; " + "ackedSends={}.", id, messagesSent, uniqueMessagesSent, spec.maxMessages() - unackedSends.getCount());
256: Colin P. Mccabe, info, IfStmt, log.info("{}: Got exception when sending message {}: {}", id, messageIndex, exception.getMessage());
306: Colin P. Mccabe, info, MethodDeclaration, log.info("{}: consumer waiting for {} message(s), starting with: {}", id, numToReceive, Utils.join(list, ", "));
334: Colin P. Mccabe, debug, MethodDeclaration, log.debug("{}: Starting RoundTripWorker#ConsumerRunnable.", id);
372: Colin P. Mccabe, info, TryStmt, log.info("{}: ConsumerRunnable is exiting.  Invoked poll {} time(s).  " + "messagesReceived = {}; uniqueMessagesReceived = {}.", id, pollInvoked, messagesReceived, uniqueMessagesReceived);
364: Colin P. Mccabe, debug, CatchClause, log.debug("{}: Consumer got WakeupException", id, e);
366: Colin P. Mccabe, debug, CatchClause, log.debug("{}: Consumer got TimeoutException", id, e);
348: Colin P. Mccabe, info, IfStmt, log.info("{}: Consumer received the full count of {} unique messages.  " + "Waiting for all sends to be acked...", id, spec.maxMessages());
351: Colin P. Mccabe, info, IfStmt, log.info("{}: all sends have been acked.", id);
424: Colin P. Mccabe, info, MethodDeclaration, log.info("{}: Deactivating RoundTripWorkloadWorker.", id);
53: Colin P. Mccabe, debug, MethodDeclaration, log.debug("RAN {}: {}", curNode, Utils.join(command, " "));
151: Colin P. Mccabe, info, CatchClause, log.info("Unable to get coordinator tasks", e);
165: Colin P. Mccabe, info, IfStmt, log.info("EXPECTED TASKS: {}", JsonUtil.toJsonString(expected));
166: Colin P. Mccabe, info, IfStmt, log.info("ACTUAL TASKS  : {}", JsonUtil.toJsonString(tasks.tasks()));
167: Colin P. Mccabe, error, IfStmt, log.info(errorString);
184: Colin P. Mccabe, info, CatchClause, log.info("Unable to get agent status", e);
202: Colin P. Mccabe, info, IfStmt, log.info("EXPECTED WORKERS: {}", JsonUtil.toJsonString(expected));
203: Colin P. Mccabe, info, IfStmt, log.info("ACTUAL WORKERS  : {}", JsonUtil.toJsonString(status.workers()));
204: Colin P. Mccabe, error, IfStmt, log.info(errorString);
135: Colin P. Mccabe, info, MethodDeclaration, log.info("Creating MiniTrogdorCluster with agents: {} and coordinator: {}", Utils.join(agentNames, ", "), coordinatorName);
191: Colin P. Mccabe, error, CatchClause, log.error("Unable to initialize {}", nodeName, e);
261: Colin P. Mccabe, info, MethodDeclaration, log.info("Closing MiniTrogdorCluster.");
329: Colin P. Mccabe, debug, IfStmt, log.debug("Got expected lines for {}", nodeName);
333: Colin P. Mccabe, info, IfStmt, log.info("Failed to find the expected lines for {}.  First " + "missing line on index {}: {}", nodeName, matchIdx, expectedLines.get(matchIdx));
343: Colin P. Mccabe, trace, IfStmt, log.trace("Expected:\n'{}', Got:\n'{}'", expectedLine, actualLine);
